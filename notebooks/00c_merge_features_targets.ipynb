{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJYcEj9SeSN9zHgr4YnZ2K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Rpm20QokCv2p","executionInfo":{"status":"ok","timestamp":1756575287953,"user_tz":-60,"elapsed":1145,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"8291fc4d-69ba-4fe3-b2ae-4199ad9b5673"},"outputs":[{"output_type":"stream","name":"stdout","text":["Notebook 3 config OK.\n","Loaded prices: (3765, 8) | tidy sentiment: (11295, 23) | wide pivot: (3765, 62)\n","AAPL: wrote final_inputs/AAPL_input.csv rows=731 cols=77\n","AMZN: wrote final_inputs/AMZN_input.csv rows=731 cols=77\n","MSFT: wrote final_inputs/MSFT_input.csv rows=731 cols=77\n","TSLA: wrote final_inputs/TSLA_input.csv rows=731 cols=77\n","AMD: wrote final_inputs/AMD_input.csv rows=731 cols=77\n","\n","All final datasets constructed.\n","Ticker Status      Start        End  Rows  DuplicateDates  TotalNaNs MissingCore Unexpected\n","  AAPL     ok 2021-02-03 2023-12-28   731               0          0                       \n","  AMZN     ok 2021-02-03 2023-12-28   731               0          0                       \n","  MSFT     ok 2021-02-03 2023-12-28   731               0          0                       \n","  TSLA     ok 2021-02-03 2023-12-28   731               0          0                       \n","   AMD     ok 2021-02-03 2023-12-28   731               0          0                       \n","\n","Saved: /content/sanity_reports/notebook03_final_inputs_summary.csv\n","\n","AAPL first non-zero Twitter rows in final_inputs:\n","      date  Tw_count_items  Tw_mean_s\n","2021-09-30             4.0   0.068671\n","2021-10-01             5.0  -0.149365\n","2021-10-04             1.0   0.014596\n","2021-10-05             2.0  -0.828093\n","2021-10-06             1.0  -0.000429\n","\n","AMZN first non-zero Twitter rows in final_inputs:\n","      date  Tw_count_items  Tw_mean_s\n","2021-10-01             4.0  -0.363160\n","2021-10-04             6.0  -0.158318\n","2021-10-05             2.0   0.059983\n","2021-10-06             2.0  -0.517008\n","2021-10-08             1.0   0.031474\n","\n","MSFT first non-zero Twitter rows in final_inputs:\n","      date  Tw_count_items  Tw_mean_s\n","2021-10-01             2.0  -0.465869\n","2021-10-04             1.0   0.117473\n","2021-10-08             1.0   0.031474\n","2021-10-12             1.0  -0.829969\n","2021-10-14             2.0  -0.340263\n","\n","TSLA first non-zero Twitter rows in final_inputs:\n","      date  Tw_count_items  Tw_mean_s\n","2021-09-30            72.0  -0.010670\n","2021-10-01            84.0   0.042776\n","2021-10-04           255.0   0.132666\n","2021-10-05            71.0   0.080334\n","2021-10-06            57.0   0.029970\n","\n","AMD first non-zero Twitter rows in final_inputs:\n","      date  Tw_count_items  Tw_mean_s\n","2021-09-30             2.0   0.097912\n","2021-10-01             1.0   0.147540\n","2021-10-04             2.0   0.092942\n","2021-10-05             2.0   0.482731\n","2021-10-07             2.0   0.066079\n","Sentiment tidy empty: False | wide empty: False\n","Note: tidy is per (ticker,date,source); pivot merges sources per (ticker,date).\n","AAPL: wrote splits_AAPL.csv | Train=482, Val=103, Test=146\n","AMZN: wrote splits_AMZN.csv | Train=482, Val=103, Test=146\n","MSFT: wrote splits_MSFT.csv | Train=482, Val=103, Test=146\n","TSLA: wrote splits_TSLA.csv | Train=482, Val=103, Test=146\n","AMD: wrote splits_AMD.csv | Train=482, Val=103, Test=146\n","\n","NB3 split summary\n","ticker  train_min  train_max  train_n    val_min    val_max  val_n   test_min   test_max  test_n\n","  AAPL 2021-02-03 2022-12-30      482 2023-01-03 2023-05-31    103 2023-06-01 2023-12-28     146\n","  AMZN 2021-02-03 2022-12-30      482 2023-01-03 2023-05-31    103 2023-06-01 2023-12-28     146\n","  MSFT 2021-02-03 2022-12-30      482 2023-01-03 2023-05-31    103 2023-06-01 2023-12-28     146\n","  TSLA 2021-02-03 2022-12-30      482 2023-01-03 2023-05-31    103 2023-06-01 2023-12-28     146\n","   AMD 2021-02-03 2022-12-30      482 2023-01-03 2023-05-31    103 2023-06-01 2023-12-28     146\n","\n","All splits are compliant.\n","Wrote appendix_file_hashes.csv\n","Created: /content/final_inputs.zip\n","Created: /content/splits.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_dd9c21d9-62b5-4c8f-bc21-25bb63d413a5\", \"final_inputs.zip\", 609135)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_113e343b-42a4-4246-8486-a07e4bd2befb\", \"splits.zip\", 8840)"]},"metadata":{}}],"source":["# =========================\n","# Notebook 3 â€” Final dataset construction (ROOT-ONLY, EMR-aligned)\n","# =========================\n","import warnings\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","CONFIG = {\n","    \"DATE_START\": \"2021-01-01\",\n","    \"DATE_END\": \"2023-12-31\",\n","    \"TICKERS\": [\"AAPL\", \"AMZN\", \"MSFT\", \"TSLA\", \"AMD\"],\n","\n","    # Prices (calendar source of truth)\n","    \"PRICE_TIDY\": \"historical_stock_yfinance_full_2021_2023_tidy.csv\",\n","\n","    # NB2 daily sentiment inputs (authoritative)\n","    \"TW_DAILY\": \"twitter_daily.csv\",\n","    \"RD_DAILY\": \"reddit_daily.csv\",\n","    \"NW_DAILY\": \"sp500_daily.csv\",\n","\n","    # Directories\n","    \"INT_DIR\": \"intermediates\",     # fallback only\n","    \"OUT_DIR\": \"final_inputs\",\n","\n","    # Feature name prefixes\n","    \"ALLOWED_PREFIXES\": (\"Tw_\", \"Rd_\", \"Nw_SP500_\"),\n","}\n","\n","DATE_START_TS = pd.to_datetime(CONFIG[\"DATE_START\"])\n","DATE_END_TS   = pd.to_datetime(CONFIG[\"DATE_END\"])\n","TICKERS = CONFIG[\"TICKERS\"]\n","\n","OUT_DIR = Path(CONFIG[\"OUT_DIR\"]); OUT_DIR.mkdir(parents=True, exist_ok=True)\n","print(\"Notebook 3 config OK.\")\n","\n","# =========================\n","# Pre-flight + tidy loaders\n","# =========================\n","def find_daily(name: str) -> Path | None:\n","    p_root = Path(name)\n","    p_int  = Path(CONFIG[\"INT_DIR\"]) / name\n","    if p_root.exists(): return p_root\n","    if p_int.exists():  return p_int\n","    return None\n","\n","def load_prices_tidy() -> pd.DataFrame:\n","    p = find_daily(CONFIG[\"PRICE_TIDY\"]) or Path(CONFIG[\"PRICE_TIDY\"])\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing tidy price file: {CONFIG['PRICE_TIDY']}\")\n","    px = pd.read_csv(p, parse_dates=[\"Date\"])\n","    if \"Ticker\" in px.columns:\n","        px = px.rename(columns={\"Date\":\"date\",\"Ticker\":\"ticker\"})\n","    else:\n","        px = px.rename(columns={\"Date\":\"date\"})\n","        if \"ticker\" not in px.columns and \"Symbol\" in px.columns:\n","            px = px.rename(columns={\"Symbol\":\"ticker\"})\n","    px = px[px[\"ticker\"].isin(TICKERS)]\n","    px = px[(px[\"date\"] >= DATE_START_TS) & (px[\"date\"] <= DATE_END_TS)]\n","    return px.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","\n","SRC_FILES = [(\"Twitter\", CONFIG[\"TW_DAILY\"]),\n","             (\"Reddit\",  CONFIG[\"RD_DAILY\"]),\n","             (\"SP500\",   CONFIG[\"NW_DAILY\"])]\n","\n","def load_sentiment_long(tickers) -> pd.DataFrame:\n","    \"\"\"Load NB2 daily files. If they lack 'source', synthesise it from filename and\n","    convert wide prefixed cols to tidy rows for a clean pivot later.\"\"\"\n","    frames = []\n","    for src_name, fname in SRC_FILES:\n","        p = find_daily(fname)\n","        if p is None:\n","            print(f\"Missing daily file: {fname} (skipping)\")\n","            continue\n","        df = pd.read_csv(p, parse_dates=[\"date\"])\n","        df = df[df[\"ticker\"].isin(tickers)].copy()\n","\n","        # Preferred: tidy already has a 'source' column\n","        if \"source\" in df.columns:\n","            df[\"source\"] = df[\"source\"].replace({\"News\":\"SP500\"})  # normalise naming\n","            frames.append(df)\n","            continue\n","\n","        # Fallback: wide with a known prefix -> synthesise tidy rows\n","        pref = {\"Twitter\":\"Tw_\",\"Reddit\":\"Rd_\",\"SP500\":\"Nw_SP500_\"}[src_name]\n","        feat_cols = [c for c in df.columns if c.startswith(pref)]\n","        if not feat_cols:\n","            print(f\"{fname}: no {pref}* columns found; skipping\")\n","            continue\n","        tmp = df[[\"date\",\"ticker\"] + feat_cols].copy()\n","        tmp.columns = [\"date\",\"ticker\"] + [c.replace(pref,\"\") for c in feat_cols]\n","        tmp.insert(2, \"source\", src_name)\n","        frames.append(tmp)\n","\n","    if not frames:\n","        return pd.DataFrame(columns=[\"date\",\"ticker\",\"source\"])\n","\n","    long_df = pd.concat(frames, ignore_index=True)\n","    long_df = long_df.sort_values([\"ticker\",\"date\",\"source\"]).reset_index(drop=True)\n","    return long_df\n","\n","def pivot_sentiment(long_df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Pivot tidy sentiment to a single wide frame keyed by (date, ticker).\"\"\"\n","    if long_df.empty:\n","        return pd.DataFrame(columns=[\"date\",\"ticker\"])\n","    feats = [c for c in long_df.columns if c not in [\"date\",\"ticker\",\"source\"]]\n","    # NB2 already has one row per (date,ticker,source); mean is idempotent\n","    gv = long_df.groupby([\"date\",\"ticker\",\"source\"], as_index=False)[feats].mean()\n","    piv = gv.pivot(index=[\"date\",\"ticker\"], columns=\"source\", values=feats)\n","    pref_map = {\"Twitter\":\"Tw_\",\"Reddit\":\"Rd_\",\"SP500\":\"Nw_SP500_\"}\n","    piv.columns = [f\"{pref_map[src]}{feat}\" for feat, src in piv.columns]\n","    piv = piv.reset_index().sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    return piv\n","\n","prices = load_prices_tidy()\n","all_sent_long = load_sentiment_long(TICKERS)\n","all_sent_wide = pivot_sentiment(all_sent_long)\n","print(\"Loaded prices:\", prices.shape, \"| tidy sentiment:\", all_sent_long.shape, \"| wide pivot:\", all_sent_wide.shape)\n","\n","# =========================\n","# Technical indicators (RSI-14, MACD 12-26-9, returns, vols, SMAs)\n","# =========================\n","def _rsi_wilder(close: pd.Series, period: int = 14) -> pd.Series:\n","    delta = close.diff()\n","    up, down = delta.clip(lower=0), -delta.clip(upper=0)\n","    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n","    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n","    rs = roll_up / roll_down.replace(0, np.nan)\n","    rsi = 100 - (100 / (1 + rs))\n","    return rsi.fillna(0.0)\n","\n","def _macd(close: pd.Series, fast=12, slow=26, signal=9) -> tuple[pd.Series,pd.Series]:\n","    ema_fast = close.ewm(span=fast, adjust=False).mean()\n","    ema_slow = close.ewm(span=slow, adjust=False).mean()\n","    macd = ema_fast - ema_slow\n","    sig  = macd.ewm(span=signal, adjust=False).mean()\n","    return macd, sig\n","\n","def compute_technicals(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.sort_values(\"date\").reset_index(drop=True).copy()\n","    df[\"Return_1d\"] = df[\"Close\"].pct_change()\n","    df[\"LogRet_1d\"] = np.log(df[\"Close\"]).diff()\n","    df[\"Vol_7\"]  = df[\"LogRet_1d\"].rolling(7,  min_periods=7).std()\n","    df[\"Vol_21\"] = df[\"LogRet_1d\"].rolling(21, min_periods=21).std()\n","    df[\"SMA_7\"]  = df[\"Close\"].rolling(7,  min_periods=7).mean()\n","    df[\"SMA_21\"] = df[\"Close\"].rolling(21, min_periods=21).mean()\n","    df[\"RSI_14\"] = _rsi_wilder(df[\"Close\"], 14)\n","    macd, sig = _macd(df[\"Close\"], 12, 26, 9)\n","    df[\"MACD\"] = macd\n","    df[\"MACD_Signal\"] = sig\n","    return df\n","\n","# =========================\n","# Build final per-ticker (single merge from NB2 daily)\n","# =========================\n","def build_final_for_ticker(ticker: str,\n","                           prices_df: pd.DataFrame,\n","                           sent_wide_df: pd.DataFrame) -> pd.DataFrame:\n","    p = prices_df[prices_df[\"ticker\"] == ticker].copy()\n","    p = p[(p[\"date\"] >= DATE_START_TS) & (p[\"date\"] <= DATE_END_TS)]\n","    p = compute_technicals(p)\n","\n","    s = sent_wide_df[sent_wide_df[\"ticker\"] == ticker].drop(columns=[\"ticker\"], errors=\"ignore\")\n","    df = p.merge(s, on=\"date\", how=\"left\")\n","\n","    # Fill sentiment NaNs neutrally (NB2 already calendar-fills; this is defensive)\n","    sent_cols = [c for c in df.columns if c.startswith(CONFIG[\"ALLOWED_PREFIXES\"])]\n","    if sent_cols:\n","        df[sent_cols] = df[sent_cols].fillna(0.0)\n","\n","    # REQUIRED FIX: recompute *_zero_day strictly from *_count_items\n","    for pref in [\"Tw_\", \"Rd_\", \"Nw_SP500_\"]:\n","        cnt = f\"{pref}count_items\"\n","        zro = f\"{pref}zero_day\"\n","        if cnt in df.columns:\n","            df[zro] = (df[cnt] == 0).astype(float)\n","\n","    # Target is next trading day's Close (leakage-safe)\n","    df[\"Target\"] = df[\"Close\"].shift(-1)\n","\n","    # Drop rows where any numeric feature or Target is NaN (warm-up trimming)\n","    num_cols = df.select_dtypes(include=[np.number]).columns\n","    df = df.dropna(subset=[\"Target\"]).dropna(subset=num_cols)\n","\n","    # Column ordering\n","    core = [\"date\",\"ticker\",\n","            \"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\n","            \"Return_1d\",\"LogRet_1d\",\"Vol_7\",\"Vol_21\",\"SMA_7\",\"SMA_21\",\"RSI_14\",\"MACD\",\"MACD_Signal\"]\n","    others = [c for c in df.columns if c not in core + [\"Target\"]]\n","    ordered = core + sorted([c for c in others if c.startswith(CONFIG[\"ALLOWED_PREFIXES\"])]) + [\"Target\"]\n","    df = df[[c for c in ordered if c in df.columns]]\n","\n","    out_path = OUT_DIR / f\"{ticker}_input.csv\"\n","    df.to_csv(out_path, index=False)\n","    print(f\"{ticker}: wrote {out_path} rows={len(df)} cols={len(df.columns)}\")\n","    return df\n","\n","# =========================\n","# Run for all tickers\n","# =========================\n","built = {}\n","for t in TICKERS:\n","    built[t] = build_final_for_ticker(t, prices, all_sent_wide)\n","print(\"\\nAll final datasets constructed.\")\n","\n","# =========================\n","# Sanity: schema, leakage guard, NaNs, coverage, prefixes\n","# =========================\n","rows = []\n","need_core = {\"date\",\"ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\n","             \"Return_1d\",\"LogRet_1d\",\"Vol_7\",\"Vol_21\",\"SMA_7\",\"SMA_21\",\"RSI_14\",\"MACD\",\"MACD_Signal\",\"Target\"}\n","\n","for t in TICKERS:\n","    p = OUT_DIR / f\"{t}_input.csv\"\n","    if not p.exists():\n","        rows.append([t,\"missing\",None,None,0,0,0,\"\",\"\"])\n","        continue\n","    df = pd.read_csv(p, parse_dates=[\"date\"])\n","\n","    dup = int(df.duplicated([\"date\"]).sum())\n","    sorted_ok = bool(df[\"date\"].is_monotonic_increasing)\n","    close_next = df[\"Close\"].shift(-1)\n","    mis = int(((df[\"Target\"] - close_next).abs() > 1e-9).sum())\n","    if mis not in (0,1): print(f\"WARNING {t}: target mismatch rows = {mis}\")\n","\n","    total_nas = int(df.select_dtypes(include=[np.number]).isna().sum().sum())\n","    start = df[\"date\"].min().date() if not df.empty else None\n","    end   = df[\"date\"].max().date() if not df.empty else None\n","\n","    unexpected = [c for c in df.columns\n","                  if not any(c.startswith(a) for a in CONFIG[\"ALLOWED_PREFIXES\"])\n","                  and c not in need_core]\n","    missing_core = sorted(list(need_core - set(df.columns)))\n","\n","    ok = (dup == 0) and sorted_ok and (total_nas == 0) and (len(missing_core) == 0)\n","    rows.append([t, \"ok\" if ok else \"check\", start, end, len(df), dup, total_nas,\n","                 \";\".join(missing_core), \";\".join(unexpected)])\n","\n","rep = pd.DataFrame(rows, columns=[\"Ticker\",\"Status\",\"Start\",\"End\",\"Rows\",\"DuplicateDates\",\"TotalNaNs\",\"MissingCore\",\"Unexpected\"])\n","print(rep.to_string(index=False))\n","\n","SANITY = Path(\"sanity_reports\"); SANITY.mkdir(parents=True, exist_ok=True)\n","rep.to_csv(SANITY / \"notebook03_final_inputs_summary.csv\", index=False)\n","print(\"\\nSaved:\", (SANITY / \"notebook03_final_inputs_summary.csv\").resolve())\n","\n","# =========================\n","# Quick visibility: first non-zero Twitter rows per ticker (if present)\n","# =========================\n","for t in TICKERS:\n","    p = OUT_DIR / f\"{t}_input.csv\"\n","    if not p.exists():\n","        continue\n","    df = pd.read_csv(p, parse_dates=[\"date\"])\n","    twc = \"Tw_count_items\"\n","    if twc in df.columns:\n","        nz = df[df[twc] > 0].head(5)\n","        print(f\"\\n{t} first non-zero Twitter rows in final_inputs:\")\n","        print(nz[[\"date\", twc, \"Tw_mean_s\"]].to_string(index=False) if not nz.empty else \"  (none)\")\n","\n","# =========================\n","# Cross-source key alignment (debug only)\n","# =========================\n","def _keyset(df):\n","    return set(zip(df[\"ticker\"], df[\"date\"].dt.strftime(\"%Y-%m-%d\"))) if not df.empty else set()\n","\n","k_long = _keyset(all_sent_long)  # set collapse is fine for debug\n","k_wide = _keyset(all_sent_wide)\n","print(\"Sentiment tidy empty:\", all_sent_long.empty, \"| wide empty:\", all_sent_wide.empty)\n","print(\"Note: tidy is per (ticker,date,source); pivot merges sources per (ticker,date).\")\n","\n","# =========================\n","# NB3 Splits: Build from inputs + Audit (self-contained)\n","# =========================\n","from pathlib import Path\n","\n","# ---- Settings ----\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","\n","# Locations\n","try:\n","    inputs_dir = Path(OUT_DIR)\n","except NameError:\n","    inputs_dir = Path(\"final_inputs\") if Path(\"final_inputs\").exists() else Path(\".\")\n","\n","splits_out_dir = Path(\".\")  # keep as '.' unless ARIMA loader updated\n","\n","DATE_COL, SET_COL = \"date\", \"Set\"\n","\n","# Chronological split windows (authoritative)\n","TRAIN_START, TRAIN_END = \"2021-02-02\",\"2022-12-31\"\n","VAL_START,   VAL_END   = \"2023-01-01\",\"2023-05-31\"\n","TEST_START,  TEST_END  = \"2023-06-01\",\"2023-12-28\"\n","EXPECTED_TEST_LEN = 146\n","\n","def build_nb3_splits(df: pd.DataFrame) -> pd.DataFrame:\n","    d = df[[DATE_COL]].copy()\n","    d[SET_COL] = None\n","    d.loc[(d[DATE_COL] >= TRAIN_START) & (d[DATE_COL] <= TRAIN_END), SET_COL] = \"Train\"\n","    d.loc[(d[DATE_COL] >= VAL_START)   & (d[DATE_COL] <= VAL_END),   SET_COL] = \"Val\"\n","    d.loc[(d[DATE_COL] >= TEST_START)  & (d[DATE_COL] <= TEST_END),  SET_COL] = \"Test\"\n","    d = d.dropna(subset=[SET_COL]).sort_values(DATE_COL).reset_index(drop=True)\n","    return d\n","\n","def audit_nb3(df_input: pd.DataFrame, df_split: pd.DataFrame, ticker: str) -> dict:\n","    if df_split.duplicated(DATE_COL).any():\n","        raise ValueError(f\"{ticker}: duplicate dates in splits file\")\n","    if not set(df_split[DATE_COL]).issubset(set(df_input[DATE_COL])):\n","        raise ValueError(f\"{ticker}: splits contain dates not present in input\")\n","\n","    g = df_split.groupby(SET_COL)[DATE_COL].agg(['min','max','count'])\n","    tr = g.loc[\"Train\"] if \"Train\" in g.index else None\n","    va = g.loc[\"Val\"]   if \"Val\"   in g.index else None\n","    te = g.loc[\"Test\"]  if \"Test\"  in g.index else None\n","    if tr is None or va is None or te is None:\n","        raise ValueError(f\"{ticker}: Train/Val/Test not all present\")\n","\n","    te_min, te_max, te_n = str(te['min'].date()), str(te['max'].date()), int(te['count'])\n","    if te_min != TEST_START:  raise AssertionError(f\"{ticker}: Test start {te_min} != {TEST_START}\")\n","    if te_max != TEST_END:    raise AssertionError(f\"{ticker}: Test end {te_max} != {TEST_END}\")\n","    if te_n != EXPECTED_TEST_LEN: raise AssertionError(f\"{ticker}: Test rows {te_n} != {EXPECTED_TEST_LEN}\")\n","\n","    tr_min, tr_max = str(tr['min'].date()), str(tr['max'].date())\n","    va_min, va_max = str(va['min'].date()), str(va['max'].date())\n","    if not (TRAIN_START <= tr_min <= TRAIN_END and TRAIN_START <= tr_max <= TRAIN_END):\n","        raise AssertionError(f\"{ticker}: Train span {tr_min}..{tr_max} not within {TRAIN_START}..{TRAIN_END}\")\n","    if not (VAL_START   <= va_min <= VAL_END   and VAL_START   <= va_max <= VAL_END):\n","        raise AssertionError(f\"{ticker}: Val span {va_min}..{va_max} not within {VAL_START}..{VAL_END}\")\n","\n","    return {\n","        \"ticker\": ticker,\n","        \"train_min\": tr_min, \"train_max\": tr_max, \"train_n\": int(tr['count']),\n","        \"val_min\": va_min,   \"val_max\": va_max,   \"val_n\":   int(va['count']),\n","        \"test_min\": te_min,  \"test_max\": te_max,  \"test_n\":  te_n\n","    }\n","\n","summary_rows = []\n","for t in TICKERS:\n","    fi = inputs_dir / f\"{t}_input.csv\"\n","    assert fi.exists(), f\"Missing input: {fi}\"\n","    df = pd.read_csv(fi, parse_dates=[DATE_COL]).sort_values(DATE_COL)\n","    assert {DATE_COL, \"Close\"}.issubset(df.columns), f\"{fi} must contain 'date' and 'Close'\"\n","\n","    splits = build_nb3_splits(df)\n","\n","    out_path = splits_out_dir / f\"splits_{t}.csv\"\n","    if out_path.exists():\n","        out_path.rename(out_path.with_suffix(out_path.suffix + \".bak\"))\n","\n","    splits.to_csv(out_path, index=False)\n","    audit_row = audit_nb3(df, splits, t)\n","    summary_rows.append(audit_row)\n","    print(f\"{t}: wrote {out_path} | Train={audit_row['train_n']}, Val={audit_row['val_n']}, Test={audit_row['test_n']}\")\n","\n","summary = pd.DataFrame(summary_rows)\n","summary.to_csv(\"appendix_split_checks.csv\", index=False)\n","print(\"\\nNB3 split summary\")\n","print(summary.to_string(index=False))\n","print(\"\\nAll splits are compliant.\")\n","\n","# =========================\n","# Optional integrity hashes for inputs and splits (appendix_file_hashes.csv)\n","# =========================\n","import os, hashlib\n","\n","def sha256_of(path: Path) -> str:\n","    h = hashlib.sha256()\n","    with open(path, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","rows, missing = [], []\n","for t in TICKERS:\n","    for p in [Path(OUT_DIR) / f\"{t}_input.csv\", Path(f\"splits_{t}.csv\")]:\n","        if p.exists():\n","            rows.append({\"file\": str(p), \"bytes\": p.stat().st_size, \"sha256\": sha256_of(p)})\n","        else:\n","            missing.append(str(p))\n","            rows.append({\"file\": str(p), \"bytes\": None, \"sha256\": None})\n","\n","pd.DataFrame(rows).to_csv(\"appendix_file_hashes.csv\", index=False)\n","print(\"Wrote appendix_file_hashes.csv\")\n","if missing:\n","    print(\"Missing files (hashes not computed):\")\n","    for m in missing:\n","        print(\" -\", m)\n","\n","# =========================\n","# Zip NB3 inputs and splits for export\n","# =========================\n","import zipfile\n","\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","inputs_dir = Path(\"final_inputs\") if Path(\"final_inputs\").exists() else Path(\".\")\n","splits_dir = Path(\".\")  # splits saved in CWD above\n","\n","fin_zip = Path(\"final_inputs.zip\")\n","with zipfile.ZipFile(fin_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n","    for t in TICKERS:\n","        p = inputs_dir / f\"{t}_input.csv\"\n","        if p.exists():\n","            zf.write(p, arcname=Path(\"final_inputs\") / p.name)\n","        else:\n","            print(f\"[warn] missing input: {p}\")\n","\n","spl_zip = Path(\"splits.zip\")\n","with zipfile.ZipFile(spl_zip, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n","    candidates = sorted(Path(\".\").glob(\"splits_*.csv\"))\n","    if not candidates:\n","        print(\"[warn] no splits_*.csv found\")\n","    for p in candidates:\n","        zf.write(p, arcname=Path(\"splits\") / p.name)\n","\n","print(\"Created:\", fin_zip.resolve())\n","print(\"Created:\", spl_zip.resolve())\n","\n","# Download in Colab (best-effort)\n","try:\n","    from google.colab import files as colab_files\n","    colab_files.download(str(fin_zip))\n","    colab_files.download(str(spl_zip))\n","except Exception:\n","    try:\n","        from IPython.display import FileLink, display\n","        display(FileLink(str(fin_zip)))\n","        display(FileLink(str(spl_zip)))\n","        print(\"Click the links above to download the zips.\")\n","    except Exception:\n","        pass"]}]}