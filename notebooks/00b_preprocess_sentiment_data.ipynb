{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCOqaD56oVq/mVMFX+/V/4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Sz2EsJsMx9aB","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1756576532890,"user_tz":-60,"elapsed":51779,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"57a86951-cdfc-44f1-a3ab-c6fb22122538"},"outputs":[{"output_type":"stream","name":"stdout","text":["Notebook 2 config OK (root-only).\n","All required root files are present.\n","twitter_scored: columns -> ['date', 'ticker', 'text', 'author', 's_finbert', 'p_pos', 'p_neu', 'p_neg', 's_deberta', 's_distil', 'model_finbert', 'model_deberta', 'model_distil']\n","reddit_scored: columns -> ['date', 'ticker', 'text', 'author', 's_finbert', 'p_pos', 'p_neu', 'p_neg', 's_deberta', 's_distil', 'model_finbert', 'model_deberta', 'model_distil']\n","sp500_scored: columns -> ['date', 'ticker', 'text', 'author', 's_finbert', 'p_pos', 'p_neu', 'p_neg', 's_deberta', 's_distil', 'model_finbert', 'model_deberta', 'model_distil']\n","Trading calendar: 2021-01-04 -> 2023-12-29\n","Written: /content/twitter_daily.csv size= 577463 bytes\n","Written: /content/reddit_daily.csv size= 397789 bytes\n","Written: /content/sp500_daily.csv size= 394580 bytes\n","Twitter coverage:\n"," ticker        min        max  count\n","  AAPL 2021-01-04 2023-12-29    753\n","   AMD 2021-01-04 2023-12-29    753\n","  AMZN 2021-01-04 2023-12-29    753\n","  MSFT 2021-01-04 2023-12-29    753\n","  TSLA 2021-01-04 2023-12-29    753\n","Reddit coverage:\n"," ticker        min        max  count\n","  AAPL 2021-01-04 2023-12-29    753\n","   AMD 2021-01-04 2023-12-29    753\n","  AMZN 2021-01-04 2023-12-29    753\n","  MSFT 2021-01-04 2023-12-29    753\n","  TSLA 2021-01-04 2023-12-29    753\n","SP500 coverage:\n"," ticker        min        max  count\n","  AAPL 2021-01-04 2023-12-29    753\n","   AMD 2021-01-04 2023-12-29    753\n","  AMZN 2021-01-04 2023-12-29    753\n","  MSFT 2021-01-04 2023-12-29    753\n","  TSLA 2021-01-04 2023-12-29    753\n","Twitter: aligned to trading calendar.\n","Reddit: aligned to trading calendar.\n","News: aligned to trading calendar.\n","Cross-source key alignment: OK\n","\n","Daily feature files look OK.\n","\n","Twitter first non-zero days:\n","ticker       date  Tw_count_items  Tw_mean_s\n","  AAPL 2021-09-30             4.0   0.068671\n","   AMD 2021-09-30             2.0   0.097912\n","  AMZN 2021-10-01             4.0  -0.363160\n","  MSFT 2021-10-01             2.0  -0.465869\n","  TSLA 2021-09-30            72.0  -0.010670\n","\n","Reddit first non-zero days:\n","ticker       date  Rd_count_items  Rd_mean_s\n","  AAPL 2021-01-28             1.0  -0.201328\n","   AMD 2021-01-29             8.0   0.080100\n","  AMZN 2021-01-29             2.0  -0.303902\n","  MSFT 2021-02-01             1.0   0.051691\n","  TSLA 2021-01-29            10.0  -0.033845\n","\n","News first non-zero days:\n","ticker       date  Nw_SP500_count_items  Nw_SP500_mean_s\n","  AAPL 2021-01-04                 133.0         0.040905\n","   AMD 2021-01-04                   6.0         0.095324\n","  AMZN 2021-01-04                  28.0        -0.052619\n","  MSFT 2021-01-04                  18.0         0.025997\n","  TSLA 2021-01-04                 103.0         0.088171\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_627bd7c3-adfd-4e9f-8f10-b6c312b940ff\", \"twitter_daily.csv\", 577463)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b261797b-8c44-4f39-ad33-4989bdc2d6a2\", \"reddit_daily.csv\", 397789)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_18832449-9e76-40d5-92eb-b6c74e40f53e\", \"sp500_daily.csv\", 394580)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[PASS] twitter_daily.csv\n","[PASS] reddit_daily.csv\n","[PASS] sp500_daily.csv\n","[PASS] CROSS-SOURCE\n","  Cross-source key alignment: PASS\n","\n","Validation JSON -> daily_validation_report.json\n","\n","VIABILITY: PASS ✅\n"]}],"source":["# =========================\n","# Notebook 2: Daily feature aggregation (ROOT-ONLY)\n","# =========================\n","import warnings\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -------------------------\n","# Configuration (root files only)\n","# -------------------------\n","CONFIG = {\n","    \"DATE_START\": \"2021-01-01\",\n","    \"DATE_END\": \"2023-12-31\",\n","    \"TICKERS\": [\"AAPL\", \"AMZN\", \"MSFT\", \"TSLA\", \"AMD\"],\n","    \"PX_TIDY\": \"historical_stock_yfinance_full_2021_2023_tidy.csv\",\n","    \"CLIP_STRATEGY\": \"expanding\",   # or \"static\"\n","    \"ACTIVITY_K\": 3,\n","    \"SRC_TW_SCORED\": \"twitter_scored.csv\",\n","    \"SRC_RD_SCORED\": \"reddit_scored.csv\",\n","    \"SRC_SP_SCORED\": \"sp500_scored.csv\",\n","    \"OUT_TW_DAILY\": \"twitter_daily.csv\",\n","    \"OUT_RD_DAILY\": \"reddit_daily.csv\",\n","    \"OUT_SP_DAILY\": \"sp500_daily.csv\",\n","}\n","\n","DATE_START_TS = pd.to_datetime(CONFIG[\"DATE_START\"])\n","DATE_END_TS   = pd.to_datetime(CONFIG[\"DATE_END\"])\n","TICKERS = CONFIG[\"TICKERS\"]\n","\n","print(\"Notebook 2 config OK (root-only).\")\n","\n","# =========================\n","# Pre-flight: root files must exist (root-only)\n","# =========================\n","need = [\n","    CONFIG[\"SRC_TW_SCORED\"],\n","    CONFIG[\"SRC_RD_SCORED\"],\n","    CONFIG[\"SRC_SP_SCORED\"],\n","    CONFIG[\"PX_TIDY\"],\n","]\n","\n","missing = [p for p in need if not Path(p).exists()]\n","if missing:\n","    raise FileNotFoundError(f\"Missing required root files: {missing}\")\n","\n","print(\"All required root files are present.\")\n","\n","def quick_summary(path, tag):\n","    try:\n","        df = pd.read_csv(path, nrows=5)\n","        print(f\"{tag}: columns ->\", list(df.columns))\n","    except Exception as e:\n","        print(f\"{tag}: unable to read head ->\", e)\n","\n","quick_summary(CONFIG[\"SRC_TW_SCORED\"], \"twitter_scored\")\n","quick_summary(CONFIG[\"SRC_RD_SCORED\"], \"reddit_scored\")\n","quick_summary(CONFIG[\"SRC_SP_SCORED\"], \"sp500_scored\")\n","\n","# =========================\n","# Trading-day index from price tidy file (root)\n","# =========================\n","def build_trading_day_index(px_path, tickers, start_ts, end_ts):\n","    px = pd.read_csv(px_path, parse_dates=[\"Date\"])\n","    px = px.rename(columns={\"Date\":\"date\", \"Ticker\":\"ticker\"}) if \"Ticker\" in px.columns else px.rename(columns={\"Date\":\"date\"})\n","    if \"ticker\" not in px.columns:\n","        if \"Symbol\" in px.columns:\n","            px = px.rename(columns={\"Symbol\":\"ticker\"})\n","        else:\n","            raise KeyError(\"Price file must include a ticker column (Ticker or Symbol).\")\n","    px = px.loc[px[\"ticker\"].isin(tickers)].copy()\n","    px = px[(px[\"date\"] >= start_ts) & (px[\"date\"] <= end_ts)]\n","    cal = (\n","        px[[\"date\",\"ticker\"]]\n","        .drop_duplicates()\n","        .sort_values([\"ticker\",\"date\"])\n","        .reset_index(drop=True)\n","    )\n","    return cal\n","\n","TRADING_CAL = build_trading_day_index(\n","    CONFIG[\"PX_TIDY\"], TICKERS, DATE_START_TS, DATE_END_TS\n",")\n","TRADING_DAYS_INDEX = TRADING_CAL[\"date\"]\n","print(\"Trading calendar:\", TRADING_DAYS_INDEX.min().date(), \"->\", TRADING_DAYS_INDEX.max().date())\n","\n","# =========================\n","# Robust aggregation helpers (v2, root-only)\n","# =========================\n","def winsor_series(x: pd.Series, lower_q=0.01, upper_q=0.99):\n","    lo, hi = x.quantile(lower_q), x.quantile(upper_q)\n","    return x.clip(lower=lo, upper=hi)\n","\n","def expanding_winsor(x: pd.Series, lower_q=0.01, upper_q=0.99):\n","    out = []\n","    arr = x.values\n","    for i in range(len(arr)):\n","        hist = pd.Series(arr[:i])  # exclude current for no look-ahead\n","        if len(hist) < 10:\n","            out.append(arr[i])\n","            continue\n","        lo, hi = hist.quantile(lower_q), hist.quantile(upper_q)\n","        v = arr[i]\n","        v = lo if v < lo else v\n","        v = hi if v > hi else v\n","        out.append(v)\n","    return pd.Series(out, index=x.index)\n","\n","def select_score_column(df):\n","    for c in [\"s_finbert\",\"s_deberta\",\"score\"]:\n","        if c in df.columns:\n","            return c\n","    raise KeyError(\"No sentiment score column found. Expected s_finbert or s_deberta.\")\n","\n","def aggregate_one(df_scored: pd.DataFrame, src_prefix: str, clip=\"expanding\", activity_k=3):\n","    sc_col = select_score_column(df_scored)\n","    scored = df_scored.copy()\n","    scored[\"date\"] = pd.to_datetime(scored[\"date\"])\n","    scored = scored.sort_values([\"ticker\",\"date\"])\n","\n","    # winsorise score with no look-ahead (expanding) or static\n","    if clip == \"expanding\":\n","        scored[\"_score_w\"] = scored.groupby(\"ticker\", group_keys=False)[sc_col].apply(expanding_winsor)\n","    elif clip == \"static\":\n","        scored[\"_score_w\"] = scored.groupby(\"ticker\", group_keys=False)[sc_col].apply(winsor_series)\n","    else:\n","        raise ValueError(\"clip must be 'expanding' or 'static'\")\n","\n","    # base aggregation on winsorised score\n","    g = scored.groupby([\"ticker\",\"date\"], as_index=False)\n","    ag_score = g[\"_score_w\"].agg(\n","        mean_s = \"mean\",\n","        median_s = \"median\",\n","        std_s = \"std\",\n","        q10_s = lambda s: s.quantile(0.10),\n","        q90_s = lambda s: s.quantile(0.90),\n","    )\n","    ag_count = g.size().rename(columns={\"size\":\"count_items\"})\n","\n","    ag = ag_score.merge(ag_count, on=[\"ticker\",\"date\"], how=\"left\")\n","\n","    # unique authors if present\n","    if \"author\" in scored.columns:\n","        ua = scored.groupby([\"ticker\",\"date\"])[\"author\"].nunique().reset_index(name=\"unique_authors\")\n","        ag = ag.merge(ua, on=[\"ticker\",\"date\"], how=\"left\")\n","    else:\n","        ag[\"unique_authors\"] = 0.0\n","\n","    # shares (prob >= 0.5) and daily means of FinBERT probabilities if present\n","    for pcol in [\"p_pos\",\"p_neu\",\"p_neg\"]:\n","        if pcol in scored.columns:\n","            # share >= 0.5\n","            sh = scored.groupby([\"ticker\",\"date\"])[pcol].apply(lambda s: (s >= 0.5).mean()).reset_index(name=f\"{pcol}_share\")\n","            ag = ag.merge(sh, on=[\"ticker\",\"date\"], how=\"left\")\n","        else:\n","            ag[f\"{pcol}_share\"] = 0.0\n","\n","    if set([\"p_pos\",\"p_neu\",\"p_neg\"]).issubset(scored.columns):\n","        pm = scored.groupby([\"ticker\",\"date\"])[[\"p_pos\",\"p_neu\",\"p_neg\"]].mean().reset_index()\n","        pm = pm.rename(columns={\n","            \"p_pos\":\"mean_p_pos\",\n","            \"p_neu\":\"mean_p_neu\",\n","            \"p_neg\":\"mean_p_neg\"\n","        })\n","        ag = ag.merge(pm, on=[\"ticker\",\"date\"], how=\"left\")\n","    else:\n","        ag[\"mean_p_pos\"] = 0.0\n","        ag[\"mean_p_neu\"] = 0.0\n","        ag[\"mean_p_neg\"] = 0.0\n","\n","    # expanding z, EWMAs on mean_s (computed only on active days, then aligned back)\n","    z_rows = []\n","    for t, tdf in ag.groupby(\"ticker\"):\n","        tdf = tdf.sort_values(\"date\").reset_index(drop=True)\n","        mu = tdf[\"mean_s\"].expanding().mean().shift(1)\n","        sd = tdf[\"mean_s\"].expanding().std().shift(1)\n","        zmean = (tdf[\"mean_s\"] - mu) / sd\n","        tdf[\"z_mean_s\"] = zmean.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n","        tdf[\"ewma3_s\"] = tdf[\"mean_s\"].ewm(span=3, adjust=False).mean()\n","        tdf[\"ewma7_s\"] = tdf[\"mean_s\"].ewm(span=7, adjust=False).mean()\n","        z_rows.append(tdf)\n","    ag = pd.concat(z_rows, ignore_index=True)\n","\n","    # activity flags on active rows (zero_day handled AFTER calendar merge)\n","    act_rows = []\n","    for t, tdf in ag.groupby(\"ticker\"):\n","        tdf = tdf.sort_values(\"date\")\n","        r90 = tdf[\"count_items\"].rolling(60, min_periods=20).quantile(0.9).shift(1)\n","        rmed = tdf[\"count_items\"].rolling(21, min_periods=10).median().shift(1)\n","        tdf[\"burst_day\"] = (tdf[\"count_items\"] > r90.fillna(0.0)).astype(float)\n","        tdf[\"active_k\"]  = (tdf[\"count_items\"] >= activity_k).astype(float)\n","        tdf[\"active_rl\"] = (tdf[\"count_items\"] > rmed.fillna(0.0)).astype(float)\n","        act_rows.append(tdf)\n","    ag = pd.concat(act_rows, ignore_index=True)\n","\n","    # calendar merge (left join), THEN compute zero_day correctly\n","    ag = TRADING_CAL.merge(ag, on=[\"ticker\",\"date\"], how=\"left\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    ag[\"zero_day\"] = ((ag[\"count_items\"].isna()) | (ag[\"count_items\"] == 0)).astype(float)\n","\n","    # fill NaNs for neutral/no-item days\n","    ag = ag.fillna({\n","        \"mean_s\":0.0,\"median_s\":0.0,\"std_s\":0.0,\"q10_s\":0.0,\"q90_s\":0.0,\n","        \"count_items\":0.0,\"unique_authors\":0.0,\n","        \"p_pos_share\":0.0,\"p_neg_share\":0.0,\"p_neu_share\":0.0,\n","        \"mean_p_pos\":0.0,\"mean_p_neu\":0.0,\"mean_p_neg\":0.0,\n","        \"z_mean_s\":0.0,\"ewma3_s\":0.0,\"ewma7_s\":0.0,\n","        \"burst_day\":0.0,\"active_k\":0.0,\"active_rl\":0.0\n","    })\n","\n","    # prefix columns\n","    pref = src_prefix\n","    rename_map = {c: f\"{pref}{c}\" for c in ag.columns if c not in [\"ticker\",\"date\"]}\n","    out = ag.rename(columns=rename_map)\n","    return out\n","\n","# =========================\n","# Main: build and save daily CSVs (write to ROOT)\n","# =========================\n","tw_scored = pd.read_csv(CONFIG[\"SRC_TW_SCORED\"], parse_dates=[\"date\"])\n","rd_scored = pd.read_csv(CONFIG[\"SRC_RD_SCORED\"], parse_dates=[\"date\"])\n","sp_scored = pd.read_csv(CONFIG[\"SRC_SP_SCORED\"], parse_dates=[\"date\"])\n","\n","tw = aggregate_one(tw_scored, \"Tw_\", clip=CONFIG[\"CLIP_STRATEGY\"], activity_k=CONFIG[\"ACTIVITY_K\"])\n","rd = aggregate_one(rd_scored, \"Rd_\", clip=CONFIG[\"CLIP_STRATEGY\"], activity_k=CONFIG[\"ACTIVITY_K\"])\n","nw = aggregate_one(sp_scored, \"Nw_SP500_\", clip=CONFIG[\"CLIP_STRATEGY\"], activity_k=CONFIG[\"ACTIVITY_K\"])\n","\n","tw = tw.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","rd = rd.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","nw = nw.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","\n","tw.to_csv(CONFIG[\"OUT_TW_DAILY\"], index=False)\n","rd.to_csv(CONFIG[\"OUT_RD_DAILY\"], index=False)\n","nw.to_csv(CONFIG[\"OUT_SP_DAILY\"], index=False)\n","\n","for f in [CONFIG[\"OUT_TW_DAILY\"], CONFIG[\"OUT_RD_DAILY\"], CONFIG[\"OUT_SP_DAILY\"]]:\n","    p = Path(f)\n","    print(\"Written:\", p.resolve(), \"size=\", p.stat().st_size, \"bytes\")\n","\n","# =========================\n","# Post-run verification\n","# =========================\n","ALLOWED = {\n","    \"Tw_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","            \"count_items\",\"unique_authors\",\n","            \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","            \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","            \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\n","            \"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","    \"Rd_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","            \"count_items\",\"unique_authors\",\n","            \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","            \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","            \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\n","            \"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","    \"Nw_SP500_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","                  \"count_items\",\"unique_authors\",\n","                  \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","                  \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","                  \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\n","                  \"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","}\n","\n","def check_df(df, pref):\n","    cols = [c for c in df.columns if c not in [\"ticker\",\"date\"]]\n","    bad = [c for c in cols if c.replace(pref,\"\") not in ALLOWED[pref]]\n","    if bad:\n","        raise AssertionError(f\"Unexpected columns for {pref}: {bad}\")\n","    g = df.groupby(\"ticker\")[\"date\"].agg([\"min\",\"max\",\"count\"]).reset_index()\n","    return g\n","\n","tw_v = pd.read_csv(CONFIG[\"OUT_TW_DAILY\"], parse_dates=[\"date\"])\n","rd_v = pd.read_csv(CONFIG[\"OUT_RD_DAILY\"], parse_dates=[\"date\"])\n","nw_v = pd.read_csv(CONFIG[\"OUT_SP_DAILY\"], parse_dates=[\"date\"])\n","\n","print(\"Twitter coverage:\\n\", check_df(tw_v, \"Tw_\").to_string(index=False))\n","print(\"Reddit coverage:\\n\", check_df(rd_v, \"Rd_\").to_string(index=False))\n","print(\"SP500 coverage:\\n\", check_df(nw_v, \"Nw_SP500_\").to_string(index=False))\n","\n","def align_check(df_name, df):\n","    merged = TRADING_CAL.merge(df, on=[\"ticker\",\"date\"], how=\"left\")\n","    col = \"Tw_mean_s\" if df_name==\"Twitter\" else (\"Rd_mean_s\" if df_name==\"Reddit\" else \"Nw_SP500_mean_s\")\n","    miss = merged[col].isna().sum()\n","    if miss > 0:\n","        raise AssertionError(f\"{df_name}: missing days vs calendar = {miss}\")\n","    else:\n","        print(f\"{df_name}: aligned to trading calendar.\")\n","\n","align_check(\"Twitter\", tw_v)\n","align_check(\"Reddit\", rd_v)\n","align_check(\"News\", nw_v)\n","\n","# Cross-source key alignment\n","def keyset(df):\n","    return set(zip(df[\"ticker\"], df[\"date\"].dt.strftime(\"%Y-%m-%d\")))\n","keys_tw = keyset(tw_v); keys_rd = keyset(rd_v); keys_nw = keyset(nw_v)\n","if not (keys_tw == keys_rd == keys_nw):\n","    print(\"WARNING: cross-source key mismatch.\")\n","else:\n","    print(\"Cross-source key alignment: OK\")\n","\n","if tw_v.empty or rd_v.empty or nw_v.empty:\n","    print(\"\\nOne or more daily files are empty. Check inputs and date windows in Notebook 1.\")\n","else:\n","    print(\"\\nDaily feature files look OK.\")\n","\n","# =========================\n","# Optional diagnostics: first non-zero days per ticker\n","# =========================\n","def first_nonzero(df, prefix, name):\n","    c = f\"{prefix}count_items\"\n","    z = df[df[c] > 0].sort_values([\"ticker\",\"date\"]).groupby(\"ticker\").head(1)\n","    print(f\"\\n{name} first non-zero days:\")\n","    if z.empty:\n","        print(\"None\")\n","    else:\n","        print(z[[\"ticker\",\"date\", c, f\"{prefix}mean_s\"]].to_string(index=False))\n","\n","first_nonzero(tw_v, \"Tw_\", \"Twitter\")\n","first_nonzero(rd_v, \"Rd_\", \"Reddit\")\n","first_nonzero(nw_v, \"Nw_SP500_\", \"News\")\n","\n","# =========================\n","# Single-file downloads (Colab)\n","# =========================\n","try:\n","    from google.colab import files\n","    for f in [CONFIG[\"OUT_TW_DAILY\"], CONFIG[\"OUT_RD_DAILY\"], CONFIG[\"OUT_SP_DAILY\"]]:\n","        if Path(f).exists():\n","            files.download(f)\n","except Exception as e:\n","    print(\"Direct download not available in this environment:\", e)\n","\n","# =========================\n","# Daily CSV Validation Pack (strict, EMR-aligned)\n","# =========================\n","import json, math\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","\n","# ---- Config (adjust only if your filenames differ)\n","CFG = {\n","    \"TW\": \"twitter_daily.csv\",\n","    \"RD\": \"reddit_daily.csv\",\n","    \"NW\": \"sp500_daily.csv\",\n","    \"PX_TIDY\": \"historical_stock_yfinance_full_2021_2023_tidy.csv\",\n","    \"ACTIVITY_K\": 3,\n","}\n","ALLOWED = {\n","    \"Tw_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","            \"count_items\",\"unique_authors\",\n","            \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","            \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","            \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","    \"Rd_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","            \"count_items\",\"unique_authors\",\n","            \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","            \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","            \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","    \"Nw_SP500_\": [\"mean_s\",\"median_s\",\"std_s\",\"q10_s\",\"q90_s\",\n","                  \"count_items\",\"unique_authors\",\n","                  \"p_pos_share\",\"p_neg_share\",\"p_neu_share\",\n","                  \"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\",\n","                  \"z_mean_s\",\"ewma3_s\",\"ewma7_s\",\"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"],\n","}\n","\n","# ---- Helpers\n","def _read_strict(path):\n","    if not Path(path).exists():\n","        return None, [f\"Missing file: {path}\"]\n","    try:\n","        df = pd.read_csv(path, parse_dates=[\"date\"])\n","    except Exception as e:\n","        return None, [f\"Unreadable CSV {path}: {e}\"]\n","    # Drop accidental index columns\n","    for junk in [c for c in df.columns if c.lower().startswith(\"unnamed:\")]:\n","        df = df.drop(columns=junk)\n","    return df, []\n","\n","def _build_calendar(px_path):\n","    if not Path(px_path).exists():\n","        return None, [f\"Missing tidy price file: {px_path}\"]\n","    px = pd.read_csv(px_path, parse_dates=[\"Date\"])\n","    if \"Ticker\" in px.columns:\n","        px = px.rename(columns={\"Date\":\"date\",\"Ticker\":\"ticker\"})\n","    else:\n","        px = px.rename(columns={\"Date\":\"date\"})\n","        if \"ticker\" not in px.columns:\n","            if \"Symbol\" in px.columns:\n","                px = px.rename(columns={\"Symbol\":\"ticker\"})\n","            else:\n","                return None, [\"Tidy price file has no ticker column (Ticker/Symbol).\"]\n","    cal = px[[\"date\",\"ticker\"]].drop_duplicates().sort_values([\"ticker\",\"date\"])\n","    return cal, []\n","\n","def _smoke(df, pref, allowed):\n","    issues = []\n","    base = {\"date\",\"ticker\"}\n","    if not base.issubset(df.columns):\n","        issues.append(f\"{pref} missing base cols {base - set(df.columns)}\")\n","    cols = [c for c in df.columns if c not in base]\n","    bad = [c for c in cols if c.replace(pref,\"\") not in allowed]\n","    if bad:\n","        issues.append(f\"{pref} unexpected cols: {bad}\")\n","    dups = df.duplicated([\"ticker\",\"date\"]).sum()\n","    if dups:\n","        issues.append(f\"{pref} duplicate (ticker,date) rows: {dups}\")\n","    if not np.issubdtype(df[\"date\"].dtype, np.datetime64):\n","        issues.append(f\"{pref} date not datetime64\")\n","    # integer-like checks for counts\n","    for c in [f\"{pref}count_items\", f\"{pref}unique_authors\"]:\n","        if c in df.columns:\n","            nonint = (~(df[c].fillna(0) % 1 == 0)).sum()\n","            if nonint:\n","                issues.append(f\"{c} not integer-like rows: {nonint}\")\n","            neg = (df[c] < 0).sum()\n","            if neg:\n","                issues.append(f\"{c} negative rows: {neg}\")\n","    return issues\n","\n","def _range_checks(df, pref):\n","    issues, infos = [], []\n","    EPS = 1e-10\n","    c = lambda s: f\"{pref}{s}\"\n","    # shares in [0,1]\n","    for s in [\"p_pos_share\",\"p_neg_share\",\"p_neu_share\"]:\n","        if c(s) in df.columns:\n","            x = df[c(s)]\n","            bad = ((x < -EPS) | (x > 1+EPS)).sum()\n","            if bad: issues.append(f\"{c(s)} out of [0,1] by >{EPS}: {bad}\")\n","    # daily mean probabilities in [0,1]\n","    for s in [\"mean_p_pos\",\"mean_p_neu\",\"mean_p_neg\"]:\n","        if c(s) in df.columns:\n","            x = df[c(s)]\n","            bad = ((x < -EPS) | (x > 1+EPS)).sum()\n","            if bad: issues.append(f\"{c(s)} out of [0,1] by >{EPS}: {bad}\")\n","    # std >= 0\n","    if (df[c(\"std_s\")] < -EPS).any():\n","        issues.append(f\"{c('std_s')} negative by >{EPS}\")\n","    # q10 <= q90\n","    if (df[c(\"q10_s\")] > df[c(\"q90_s\")] + EPS).any():\n","        issues.append(f\"{c('q10_s')} > {c('q90_s')} by >{EPS}\")\n","    # flags in {0,1}\n","    for s in [\"zero_day\",\"burst_day\",\"active_k\",\"active_rl\"]:\n","        if c(s) in df.columns:\n","            x = df[c(s)]\n","            bad = (~x.isin([0,1])).sum()\n","            if bad: issues.append(f\"{c(s)} not binary: {bad}\")\n","    # median between q10 and q90 on active days (must hold)\n","    msk = df[c(\"count_items\")] > 0\n","    if ((df.loc[msk, c(\"median_s\")] < df.loc[msk, c(\"q10_s\")] - EPS) |\n","        (df.loc[msk, c(\"median_s\")] > df.loc[msk, c(\"q90_s\")] + EPS)).any():\n","        issues.append(f\"{c('median_s')} outside [q10,q90] on active days\")\n","    # mean outside band = info only\n","    skew_rows = df.loc[msk & (\n","        (df[c(\"mean_s\")] < df[c(\"q10_s\")] - EPS) |\n","        (df[c(\"mean_s\")] > df[c(\"q90_s\")] + EPS)\n","    ), [\"ticker\",\"date\",c(\"count_items\"),c(\"q10_s\"),c(\"mean_s\"),c(\"q90_s\")]]\n","    if not skew_rows.empty:\n","        infos.append((\"mean_outside_q10_q90\", skew_rows.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)))\n","    return issues, infos\n","\n","def _audit_time_guard_active_only(df, pref, activity_k):\n","    # Recompute exactly how the notebook defines z/flags (active-only references)\n","    issues = []\n","    c = lambda s: f\"{pref}{s}\"\n","    for t, tdf in df.groupby(\"ticker\"):\n","        tdf = tdf.sort_values(\"date\").reset_index(drop=True)\n","        active = tdf[tdf[c(\"count_items\")] > 0].copy()\n","\n","        mu  = active[c(\"mean_s\")].expanding().mean().shift(1)\n","        sd  = active[c(\"mean_s\")].expanding().std().shift(1)\n","        z   = ((active[c(\"mean_s\")] - mu) / sd).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n","        z_full = tdf[[\"date\"]].merge(active[[\"date\"]].assign(_z=z.values), on=\"date\", how=\"left\")[\"_z\"].fillna(0.0)\n","        if not np.allclose(z_full.values, tdf[c(\"z_mean_s\")].values, atol=1e-8, rtol=1e-5):\n","            issues.append(f\"{pref} z_mean_s mismatch {t}\")\n","\n","        r90  = active[c(\"count_items\")].rolling(60, min_periods=20).quantile(0.9).shift(1).fillna(0.0)\n","        rmed = active[c(\"count_items\")].rolling(21, min_periods=10).median().shift(1).fillna(0.0)\n","        burst = (active[c(\"count_items\")] > r90).astype(float)\n","        arl   = (active[c(\"count_items\")] > rmed).astype(float)\n","\n","        burst_full = tdf[[\"date\"]].merge(active[[\"date\"]].assign(_b=burst.values), on=\"date\", how=\"left\")[\"_b\"].fillna(0.0)\n","        arl_full   = tdf[[\"date\"]].merge(active[[\"date\"]].assign(_a=arl.values),   on=\"date\", how=\"left\")[\"_a\"].fillna(0.0)\n","\n","        if (burst_full != tdf[c(\"burst_day\")]).any(): issues.append(f\"{pref} burst_day mismatch {t}\")\n","        if (arl_full   != tdf[c(\"active_rl\")]).any(): issues.append(f\"{pref} active_rl mismatch {t}\")\n","\n","        ak = (tdf[c(\"count_items\")] >= activity_k).astype(float)\n","        if (ak != tdf[c(\"active_k\")]).any(): issues.append(f\"{pref} active_k mismatch {t}\")\n","    return issues\n","\n","def _coverage(df, pref, cal):\n","    issues = []\n","    g = df.groupby(\"ticker\")[\"date\"].agg([\"min\",\"max\",\"count\"]).reset_index()\n","    if cal is not None:\n","        merged = cal.merge(df[[\"ticker\",\"date\"]], on=[\"ticker\",\"date\"], how=\"left\", indicator=True)\n","        miss = (merged[\"_merge\"] != \"both\").sum()\n","        if miss:\n","            by_t = merged[merged[\"_merge\"] != \"both\"].groupby(\"ticker\").size()\n","            issues.append(f\"{pref} missing trading days vs calendar: total {miss} -> {by_t.to_dict()}\")\n","    for t, tdf in df.groupby(\"ticker\"):\n","        if not tdf[\"date\"].is_monotonic_increasing:\n","            issues.append(f\"{pref} dates not monotonic for {t}\")\n","    return issues, g\n","\n","def validate_daily_file(path, pref, allowed_cols, calendar, activity_k):\n","    rep = {\"file\": path, \"pref\": pref, \"errors\": [], \"infos\": []}\n","    df, errs = _read_strict(path)\n","    if errs:\n","        rep[\"errors\"] += errs; return rep, None\n","    rep[\"errors\"] += _smoke(df, pref, allowed_cols)\n","    rng_errs, rng_info = _range_checks(df, pref)\n","    rep[\"errors\"] += rng_errs\n","    rep[\"infos\"]  += rng_info\n","    cov_errs, cov_table = _coverage(df, pref, calendar)\n","    rep[\"errors\"] += cov_errs\n","    rep[\"errors\"] += _audit_time_guard_active_only(df, pref, activity_k)\n","    return rep, df\n","\n","# ---- Run validations\n","calendar, cal_errs = _build_calendar(CFG[\"PX_TIDY\"])\n","if cal_errs:\n","    print(\"Calendar error:\", cal_errs[0])\n","    calendar = None  # proceed with partial checks\n","\n","reports = []\n","rep_tw, df_tw = validate_daily_file(CFG[\"TW\"], \"Tw_\", ALLOWED[\"Tw_\"], calendar, CFG[\"ACTIVITY_K\"]); reports.append(rep_tw)\n","rep_rd, df_rd = validate_daily_file(CFG[\"RD\"], \"Rd_\", ALLOWED[\"Rd_\"], calendar, CFG[\"ACTIVITY_K\"]); reports.append(rep_rd)\n","rep_nw, df_nw = validate_daily_file(CFG[\"NW\"], \"Nw_SP500_\", ALLOWED[\"Nw_SP500_\"], calendar, CFG[\"ACTIVITY_K\"]); reports.append(rep_nw)\n","\n","# Cross-source key alignment\n","def _keyset(df):\n","    return set(zip(df[\"ticker\"], df[\"date\"].dt.strftime(\"%Y-%m-%d\")))\n","if all(d is not None for d in [df_tw, df_rd, df_nw]):\n","    k_tw, k_rd, k_nw = _keyset(df_tw), _keyset(df_rd), _keyset(df_nw)\n","    if not (k_tw == k_rd == k_nw):\n","        only_tw = len(k_tw - k_rd - k_nw)\n","        only_rd = len(k_rd - k_tw - k_nw)\n","        only_nw = len(k_nw - k_tw - k_rd)\n","        reports.append({\"file\":\"CROSS-SOURCE\",\"pref\":\"*\",\"errors\":[f\"Key mismatch Tw:{only_tw} Rd:{only_rd} Nw:{only_nw}\"],\"infos\":[]})\n","    else:\n","        reports.append({\"file\":\"CROSS-SOURCE\",\"pref\":\"*\",\"errors\":[], \"infos\":[\"Cross-source key alignment: PASS\"]})\n","\n","# ---- Print compact report and write JSON\n","any_errors = False\n","for rep in reports:\n","    tag = rep[\"file\"]\n","    if rep[\"errors\"]:\n","        any_errors = True\n","        print(f\"[FAIL] {tag} ->\")\n","        for e in rep[\"errors\"][:10]:\n","            print(\"   -\", e)\n","        if len(rep[\"errors\"]) > 10:\n","            print(f\"   ... and {len(rep['errors'])-10} more\")\n","    else:\n","        print(f\"[PASS] {tag}\")\n","    for info in rep[\"infos\"]:\n","        if isinstance(info, tuple):\n","            label, frame = info[0], info[1]\n","            print(f\"  INFO {label}:\")\n","            print(frame.to_string(index=False))\n","        else:\n","            print(\" \", info)\n","\n","Path(\"daily_validation_report.json\").write_text(json.dumps(reports, default=str, indent=2))\n","print(\"\\nValidation JSON -> daily_validation_report.json\")\n","print(\"\\nVIABILITY:\", \"PASS ✅\" if not any_errors else \"FAIL ❌\")"]},{"cell_type":"code","source":["# === NB2 tighten-ups: explicit row-count gate + human-readable summary (fixed) ===\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","\n","CFG = {\n","    \"PX_TIDY\": (CONFIG[\"PX_TIDY\"] if \"CONFIG\" in globals() and \"PX_TIDY\" in CONFIG\n","                else \"historical_stock_yfinance_full_2021_2023_tidy.csv\"),\n","    \"TICKERS\": (CONFIG[\"TICKERS\"] if \"CONFIG\" in globals() and \"TICKERS\" in CONFIG\n","                else [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]),\n","    \"OUTS\": [\n","        (CONFIG[\"OUT_TW_DAILY\"] if \"CONFIG\" in globals() and \"OUT_TW_DAILY\" in CONFIG else \"twitter_daily.csv\", \"Tw_\"),\n","        (CONFIG[\"OUT_RD_DAILY\"] if \"CONFIG\" in globals() and \"OUT_RD_DAILY\" in CONFIG else \"reddit_daily.csv\", \"Rd_\"),\n","        (CONFIG[\"OUT_SP_DAILY\"] if \"CONFIG\" in globals() and \"OUT_SP_DAILY\" in CONFIG else \"sp500_daily.csv\", \"Nw_SP500_\"),\n","    ],\n","}\n","\n","def _calendar_from_prices(px_path: str, tickers: list[str]) -> pd.DataFrame:\n","    px = pd.read_csv(px_path, parse_dates=[\"Date\"])\n","    if \"Ticker\" in px.columns:\n","        px = px.rename(columns={\"Date\":\"date\",\"Ticker\":\"ticker\"})\n","    else:\n","        px = px.rename(columns={\"Date\":\"date\"})\n","        if \"ticker\" not in px.columns and \"Symbol\" in px.columns:\n","            px = px.rename(columns={\"Symbol\":\"ticker\"})\n","    px = px[px[\"ticker\"].isin(tickers)].copy()\n","    cal = (px[[\"date\",\"ticker\"]].drop_duplicates()\n","           .sort_values([\"ticker\",\"date\"]).reset_index(drop=True))\n","    return cal\n","\n","CAL = (TRADING_CAL[[\"date\",\"ticker\"]].drop_duplicates()\n","       .sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","       if \"TRADING_CAL\" in globals() else _calendar_from_prices(CFG[\"PX_TIDY\"], CFG[\"TICKERS\"]))\n","\n","expected_rows = int(len(CAL))\n","per_ticker_expected = CAL.groupby(\"ticker\")[\"date\"].nunique()\n","\n","summary_rows, errors = [], []\n","\n","for out_path, pref in CFG[\"OUTS\"]:\n","    p = Path(out_path)\n","    if not p.exists():\n","        errors.append(f\"Missing daily file: {out_path}\")\n","        continue\n","\n","    df = pd.read_csv(p, parse_dates=[\"date\"])\n","\n","    # 1) Explicit row-count hard gate\n","    if len(df) != expected_rows:\n","        raise AssertionError(f\"{out_path}: rows {len(df)} != expected {expected_rows} from trading calendar.\")\n","\n","    # 2) Per-ticker coverage hard gate\n","    per_ticker_got = df.groupby(\"ticker\")[\"date\"].nunique().reindex(per_ticker_expected.index, fill_value=0)\n","    mism = (per_ticker_got - per_ticker_expected)\n","    if (mism != 0).any():\n","        bad = mism[mism != 0].to_dict()\n","        raise AssertionError(f\"{out_path}: per-ticker day-count mismatch vs calendar -> {bad}\")\n","\n","    # Duplicate key hard gate\n","    dups = int(df.duplicated([\"ticker\",\"date\"]).sum())\n","    if dups:\n","        raise AssertionError(f\"{out_path}: duplicate (ticker,date) rows = {dups}\")\n","\n","    # Non-finite numeric guard (FIXED)\n","    arr = df.select_dtypes(include=[np.number]).to_numpy()\n","    nonfinite = int((~np.isfinite(arr)).sum())\n","    if nonfinite:\n","        raise AssertionError(f\"{out_path}: found {nonfinite} non-finite numeric values.\")\n","\n","    # Human-readable summary row\n","    summary_rows.append({\n","        \"file\": out_path,\n","        \"min_date\": df[\"date\"].min().date().isoformat(),\n","        \"max_date\": df[\"date\"].max().date().isoformat(),\n","        \"rows\": int(len(df)),\n","        \"expected_rows\": expected_rows,\n","        \"dupes\": dups,\n","        \"missing_vs_calendar\": 0,\n","        \"tickers\": \",\".join(sorted(df[\"ticker\"].unique()))\n","    })\n","\n","summary_df = pd.DataFrame(summary_rows, columns=[\n","    \"file\",\"min_date\",\"max_date\",\"rows\",\"expected_rows\",\"dupes\",\"missing_vs_calendar\",\"tickers\"\n","])\n","summary_df.to_csv(\"daily_summary_report.csv\", index=False)\n","print(\"NB2 tighten-ups: wrote daily_summary_report.csv\")\n","if errors:\n","    print(\"NB2 tighten-ups warnings:\")\n","    for e in errors:\n","        print(\" -\", e)\n","else:\n","    print(\"NB2 tighten-ups: all daily files pass explicit gates.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9O7lET-HUfK","executionInfo":{"status":"ok","timestamp":1756576643329,"user_tz":-60,"elapsed":137,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"bcdc175d-937e-4598-ccb3-1358171f902b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NB2 tighten-ups: wrote daily_summary_report.csv\n","NB2 tighten-ups: all daily files pass explicit gates.\n"]}]}]}