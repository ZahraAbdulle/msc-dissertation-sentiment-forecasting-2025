{"cells":[{"cell_type":"markdown","source":["# 01. Imports and Configuration\n","**What:** Import libraries; set seeds, paths, hyperparameters, and fixed splits.  \n","**Why:** Deterministic training/evaluation and reproducible packaging.  \n","**Method choices:** Fixed splits (Train 2021-02-03→2022-12-30; Val 2023-01-03→2023-05-31; Test 2023-06-01→2023-12-28, n=146); Target=Close.shift(-1); America/New_York 16:00 cut-off; ES on Validation; deep models use **train-only scaling** with **zero-preservation** for Tw_/Rd_/Nw_SP500_*; cadence = **monthly refit**.\n"],"metadata":{"id":"ViG5FHYDQxuI"}},{"cell_type":"code","source":["import os, sys, json, math, time, hashlib, platform, warnings, random, zipfile\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","# -------------------- config --------------------\n","warnings.filterwarnings(\"ignore\")\n","\n","MODEL_ID = \"LSTM_SE\"\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","DATA_DIR = Path(\"final_inputs\")\n","OUT_ROOT = Path(\"LSTM_SE_FINAL\")\n","MODEL_DIR = OUT_ROOT / MODEL_ID\n","OUT_ROOT.mkdir(parents=True, exist_ok=True)\n","MODEL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","ASSERT_N_TEST = 146\n","\n","# Reproducibility\n","RANDOM_SEED = 1337\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","try:\n","    torch.use_deterministic_algorithms(True)\n","except Exception:\n","    pass\n","\n","# Modelling hyperparameters\n","L = 90\n","BATCH_SIZE = 64\n","EPOCHS = 150\n","EARLY_STOP_PATIENCE = 15\n","LR = 1e-3\n","HIDDEN = 64\n","LAYERS = 2\n","DROPOUT = 0.2\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Metrics thresholds\n","EPSILON_RET = 0.0010  # for DA epsilon-gated classification\n","\n","# External provenance (for code adaptation disclosure)\n","COMMIT_SHA = \"f7e54711205ca29a06577a04047b4a40df32fdec\"\n","\n","# Audit evidence: market clock and target provenance\n","CLOCK_INVARIANTS = {\n","    \"market_timezone\": \"America/New_York\",\n","    \"cutoff_local_time\": \"16:00:00\",\n","    \"no_forward_fill_past_cutoff\": True,\n","    \"sentiment_zero_encoding_on_no_activity_days\": True\n","}\n","TARGET_PROVENANCE = {\n","    \"definition\": \"Target = Close.shift(-1)\",\n","    \"created_after_all_features\": True\n","}\n","\n","# Target scaling policy: standardise y on Train-only for numerically stable training;\n","# ALWAYS inverse-transform to level BEFORE metrics and writing predictions.\n","SCALE_TARGET = True"],"metadata":{"id":"6Na1K8LZQxLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Loading\n","**What:** Read per-ticker panels and split into Train/Val/Test by fixed dates.  \n","**Why:** Ensure leakage-safe, consistent windows and schema.  \n","**Method choices:** Dates sorted; Target column required; splits = exact dates above.\n"],"metadata":{"id":"_W-2pij4Q5g3"}},{"cell_type":"code","source":["# -------------------- IO helpers --------------------\n","def load_frame(ticker: str) -> pd.DataFrame:\n","    p = DATA_DIR / f\"{ticker}_input.csv\"\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing input file: {p}\")\n","    df = pd.read_csv(p)\n","    if \"date\" not in df.columns or \"Target\" not in df.columns:\n","        raise RuntimeError(f\"{p} must include 'date' and 'Target' columns\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    return df\n","\n","def fixed_splits(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n","    tr = df[(df[\"date\"]>=TRAIN_START) & (df[\"date\"]<=TRAIN_END)].copy()\n","    va = df[(df[\"date\"]>=VAL_START) & (df[\"date\"]<=VAL_END)].copy()\n","    te = df[(df[\"date\"]>=TEST_START) & (df[\"date\"]<=TEST_END)].copy()\n","    return {\"train\": tr, \"val\": va, \"test\": te}\n","\n","def infer_features(df: pd.DataFrame) -> List[str]:\n","    drop_cols = {\"date\",\"ticker\",\"Target\"}\n","    return [c for c in df.columns if c not in drop_cols]\n","\n","def sentiment_cols_from(features: List[str]) -> List[str]:\n","    return [c for c in features if c.startswith((\"Tw_\",\"Rd_\",\"Nw_SP500_\"))]"],"metadata":{"id":"uiQb6zjZQ6Ko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing\n","**What:** Train-only scaling: zero-preserving for sentiment; standard z-scaling for other features; optional target standardisation.  \n","**Why:** Stabilise optimisation while preserving zeros for Tw_/Rd_/Nw_SP500_* and avoiding look-ahead.  \n","**Method choices:** Fit scalers on **Train** only; apply to Val/Test; inverse-transform **y** to level before metrics/outputs."],"metadata":{"id":"WttQoHgZQ8cv"}},{"cell_type":"code","source":["# -------------------- scalers --------------------\n","class ZeroPreservingStandardScaler:\n","    def __init__(self, columns: List[str]):\n","        self.columns = list(columns)\n","        self.mu_: Dict[str, float] = {}\n","        self.sd_: Dict[str, float] = {}\n","        self.fitted_ = False\n","    def fit(self, df_train: pd.DataFrame):\n","        for c in self.columns:\n","            x = df_train[c].to_numpy()\n","            nz = x != 0\n","            if nz.any():\n","                mu = x[nz].mean()\n","                sd = x[nz].std(ddof=0)\n","                if sd <= 0: sd = 1.0\n","            else:\n","                mu, sd = 0.0, 1.0\n","            self.mu_[c] = float(mu)\n","            self.sd_[c] = float(sd)\n","        self.fitted_ = True\n","        return self\n","    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n","        assert self.fitted_\n","        z = df.copy()\n","        for c in self.columns:\n","            x = z[c].to_numpy(dtype=float)\n","            nz = x != 0\n","            y = x.copy()\n","            y[nz] = (x[nz] - self.mu_[c]) / self.sd_[c]\n","            z[c] = y\n","        return z\n","\n","class StdScaler:\n","    def __init__(self, columns: List[str]):\n","        self.columns = list(columns)\n","        self.mu_: Dict[str, float] = {}\n","        self.sd_: Dict[str, float] = {}\n","        self.fitted_ = False\n","    def fit(self, df_train: pd.DataFrame):\n","        for c in self.columns:\n","            x = df_train[c].to_numpy(dtype=float)\n","            mu = float(x.mean())\n","            sd = float(x.std(ddof=0))\n","            if sd <= 0: sd = 1.0\n","            self.mu_[c] = mu\n","            self.sd_[c] = sd\n","        self.fitted_ = True\n","        return self\n","    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n","        assert self.fitted_\n","        z = df.copy()\n","        for c in self.columns:\n","            x = z[c].to_numpy(dtype=float)\n","            z[c] = (x - self.mu_[c]) / self.sd_[c]\n","        return z\n","\n","class Identity1D:\n","    def fit(self, y: np.ndarray): return self\n","    def transform(self, y: np.ndarray) -> np.ndarray: return y.astype(np.float32)\n","    def inverse_transform(self, y: np.ndarray) -> np.ndarray: return y.astype(np.float32)\n","\n","class StdScaler1D:\n","    def __init__(self): self.mu = 0.0; self.sd = 1.0\n","    def fit(self, y: np.ndarray):\n","        y = np.asarray(y, dtype=np.float64)\n","        self.mu = float(np.mean(y))\n","        sd = float(np.std(y, ddof=0))\n","        self.sd = sd if sd > 0 else 1.0\n","        return self\n","    def transform(self, y: np.ndarray) -> np.ndarray:\n","        return ((y - self.mu) / self.sd).astype(np.float32)\n","    def inverse_transform(self, yhat: np.ndarray) -> np.ndarray:\n","        return (yhat * self.sd + self.mu).astype(np.float32)"],"metadata":{"id":"WCcRY8XsQ9xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Definition\n","**What:** Stacked LSTM with linear head; sequence window length L=90.  \n","**Why:** Capture temporal dynamics with a lightweight recurrent baseline.  \n","**Method choices:** Hidden=64, Layers=2, Dropout=0.2; MSE loss; AdamW optimizer; deterministic CuDNN settings."],"metadata":{"id":"5d0nBzkkRAeA"}},{"cell_type":"code","source":["# -------------------- datasets / model --------------------\n","class SeqDS(Dataset):\n","    def __init__(self, x: np.ndarray, y: np.ndarray):\n","        self.x = x\n","        self.y = y\n","    def __len__(self): return len(self.y)\n","    def __getitem__(self, idx):\n","        return torch.from_numpy(self.x[idx]).float(), torch.from_numpy(self.y[idx]).float()\n","\n","def make_sequences(panel_x: pd.DataFrame, panel_y: pd.Series, L: int) -> Tuple[np.ndarray,np.ndarray]:\n","    x = panel_x.to_numpy(dtype=np.float32)\n","    y = panel_y.to_numpy(dtype=np.float32)\n","    xs, ys = [], []\n","    for t in range(L, len(panel_x)):\n","        xs.append(x[t-L:t])\n","        ys.append(y[t])\n","    return np.stack(xs), np.array(ys)[:, None]\n","\n","class StackedLSTM(nn.Module):\n","    def __init__(self, in_dim: int, hidden: int, layers: int, dropout: float):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hidden, num_layers=layers,\n","                            batch_first=True, dropout=(dropout if layers > 1 else 0.0))\n","        self.head = nn.Linear(hidden, 1)\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        h = o[:, -1, :]\n","        y = self.head(h)\n","        return y"],"metadata":{"id":"ma2VCxamRBlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","**What:** Train on Train/Val with early stopping; **monthly refit** over Test (refit on each month’s first trading day using history `< t`).  \n","**Why:** Avoid look-ahead.  \n","**Method choices:** Train-only fit for scalers; deterministic DataLoaders (shuffle=False); inverse-transform **y** before metrics/emit."],"metadata":{"id":"fjpoGG45RDDn"}},{"cell_type":"code","source":["# -------------------- core metrics helpers --------------------\n","def rmse(a, b): return float(np.sqrt(np.mean((a-b)**2)))\n","def mae(a, b):  return float(np.mean(np.abs(a-b)))\n","\n","def theil_u2(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n","    y = y_true\n","    ylag = np.r_[np.nan, y[:-1]]\n","    num = np.nansum((y_hat - y)**2)\n","    den = np.nansum((y - ylag)**2)\n","    if den <= 0: return float(\"nan\")\n","    return float(np.sqrt(num/den))\n","\n","def pure_sign_returns(y_true: np.ndarray, y_hat: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","    # Percentage returns from level prices\n","    ylag = np.r_[np.nan, y_true[:-1]]\n","    r_true = (y_true - ylag) / ylag\n","    r_hat  = (y_hat  - ylag) / ylag\n","    r_true[~np.isfinite(r_true)] = 0.0\n","    r_hat[~np.isfinite(r_hat)] = 0.0\n","    pos = np.sign(r_hat)  # position_t = sign(ŷ_{t+1} − y_t)\n","    return r_true, r_hat, pos\n","\n","def dir_accuracy_eps(y_true: np.ndarray, y_hat: np.ndarray, eps: float) -> Tuple[float, float, int]:\n","    r_true, r_hat, _ = pure_sign_returns(y_true, y_hat)\n","    true_class = np.sign(r_true)\n","    pred_class = np.sign(r_hat)\n","    true_class[np.abs(r_true) < eps] = 0\n","    pred_class[np.abs(r_hat)  < eps] = 0\n","    mask = (np.abs(r_true) >= eps)  # coverage is fraction of non-neutral actuals\n","    if mask.sum() == 0:\n","        return float(\"nan\"), 0.0, 0\n","    da = float(np.mean(pred_class[mask] == true_class[mask]))\n","    cov = float(np.mean(mask))\n","    n = int(mask.sum())\n","    return da, cov, n\n","\n","def sharpe_maxdd_turnover(y_true: np.ndarray, y_hat: np.ndarray, bps: float=0.0) -> Tuple[float,float,int]:\n","    r_true, r_hat, pos = pure_sign_returns(y_true, y_hat)\n","    fee = bps/10000.0\n","    changes = np.abs(np.diff(np.r_[0.0, pos]))  # 1 when position changes, else 0\n","    turnover = int(np.sum(changes > 0))\n","    strategy_ret = pos * r_true - fee * changes\n","    mu = float(np.mean(strategy_ret))\n","    sd = float(np.std(strategy_ret, ddof=0))\n","    if sd > 0:\n","        sharpe = float(np.sqrt(252.0) * mu / sd)\n","    else:\n","        sharpe = float(\"nan\")\n","    curve = np.cumprod(1.0 + strategy_ret)\n","    if curve.size:\n","        peak = np.maximum.accumulate(curve)\n","        dd = 1.0 - curve/peak\n","        maxdd = float(np.max(dd))\n","    else:\n","        maxdd = float(\"nan\")\n","    # Internal sanity prints\n","    total_cost_calc = fee * np.sum(changes)\n","    print(f\"[Trading sanity] bps={bps:.1f} mu={mu:.6f} sd={sd:.6f} n_changes={turnover} total_cost={total_cost_calc:.6f}\")\n","    return sharpe, maxdd, turnover\n","\n","# -------------------- training utils --------------------\n","def train_loop(model, dl_tr, dl_va, epochs, patience, device):\n","    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n","    crit = nn.MSELoss()\n","    best = math.inf\n","    wait = 0\n","    hist_tr, hist_va = [], []\n","    best_state = None\n","    for ep in range(1, epochs+1):\n","        model.train()\n","        tr_loss = 0.0\n","        for bx, by in dl_tr:\n","            bx = bx.to(device); by = by.to(device)\n","            opt.zero_grad()\n","            pred = model(bx)\n","            loss = crit(pred, by)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","            tr_loss += loss.item() * len(bx)\n","        tr_loss /= max(1, len(dl_tr.dataset))\n","        model.eval()\n","        with torch.no_grad():\n","            va_loss = 0.0\n","            for bx, by in dl_va:\n","                bx = bx.to(device); by = by.to(device)\n","                pred = model(bx)\n","                loss = crit(pred, by)\n","                va_loss += loss.item() * len(bx)\n","            va_loss /= max(1, len(dl_va.dataset))\n","        hist_tr.append(tr_loss); hist_va.append(va_loss)\n","        if va_loss < best - 1e-9:\n","            best = va_loss\n","            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n","            wait = 0\n","        else:\n","            wait += 1\n","            if wait >= patience:\n","                break\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    return hist_tr, hist_va\n","\n","def predict_one_step(model, x_seq: np.ndarray) -> float:\n","    with torch.no_grad():\n","        t = torch.from_numpy(x_seq[None, ...]).float().to(DEVICE)\n","        y = model(t).cpu().numpy().ravel()[0]\n","    return float(y)\n","\n","# -------------------- month refit boundaries --------------------\n","def first_trading_days(df: pd.DataFrame) -> List[pd.Timestamp]:\n","    te = df[(df[\"date\"]>=TEST_START) & (df[\"date\"]<=TEST_END)].copy()\n","    te[\"ym\"] = te[\"date\"].dt.to_period(\"M\")\n","    return te.groupby(\"ym\")[\"date\"].min().tolist()\n","\n","# -------------------- plotting --------------------\n","def save_training_curves(path: Path, tr: List[float], va: List[float]):\n","    fig = plt.figure(figsize=(6,4))\n","    plt.plot(tr, label=\"train\")\n","    plt.plot(va, label=\"val\")\n","    plt.xlabel(\"epoch\"); plt.ylabel(\"MSE (y on training scale)\")\n","    plt.legend(); plt.tight_layout()\n","    fig.savefig(path, dpi=160)\n","    plt.close(fig)"],"metadata":{"id":"zPATLBRGRDuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n","**What:** Zero-preservation audit, inverse-transform to level, compute metrics and trading diagnostics on Test.  \n","**Why:** Evidence that scaling policy preserves zeros; report comparable metrics on level prices; DAε uses ε=0.0010; trading metrics use pure-sign rule.  \n","**Method choices:** Coverage reported as fraction of non-neutral actuals; turnover counts position changes; Sharpe annualised with √252."],"metadata":{"id":"8QDedgVXRGLv"}},{"cell_type":"code","source":["# -------------------- zero-preservation check --------------------\n","def emit_zero_preservation_check(raw_df: pd.DataFrame,\n","                                 scaled_df: pd.DataFrame,\n","                                 cols: List[str],\n","                                 out_csv: Path,\n","                                 atol: float = 1e-12):\n","    rows = []\n","    total_violations = 0\n","    if not cols:\n","        pd.DataFrame([{\"column\":\"<none>\", \"raw_zero_count\":0, \"preserved_zero_count\":0,\n","                       \"violations\":0, \"ok\": True, \"zeros_after_total\":0}]).to_csv(out_csv, index=False)\n","        return\n","    for c in cols:\n","        raw = raw_df[c].to_numpy()\n","        sca = scaled_df[c].to_numpy()\n","        mask_raw_zero = np.isfinite(raw) & (raw == 0)\n","        n_raw_zeros = int(mask_raw_zero.sum())\n","        n_preserved = int(np.isclose(sca[mask_raw_zero], 0.0, atol=atol, rtol=0).sum())\n","        violations = max(int(n_raw_zeros - n_preserved), 0)\n","        total_violations += violations\n","        rows.append({\n","            \"column\": c,\n","            \"raw_zero_count\": n_raw_zeros,\n","            \"preserved_zero_count\": n_preserved,\n","            \"violations\": violations,\n","            \"ok\": (violations == 0),\n","            \"zeros_after_total\": int(np.isclose(sca, 0.0, atol=atol, rtol=0).sum())\n","        })\n","    pd.DataFrame(rows).to_csv(out_csv, index=False)\n","    if total_violations > 0:\n","        raise RuntimeError(f\"Zero-preservation check failed: {total_violations} violations\")"],"metadata":{"id":"OTCfzF3DRHLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Outputs and Artefacts\n","**What:** Persist predictions (level), metrics, run_config, training curves, zero-preservation checks, manifests, and final ZIP.  \n","**Why:** Meet finalisation checklist and enable reproducibility.  \n","**Method choices:** Per-ticker directories; features parity hash; assert required files and row counts; build bundle with SHA256."],"metadata":{"id":"EPfq3XDcRIpH"}},{"cell_type":"code","source":["# -------------------- provenance / manifests --------------------\n","def write_env_manifest(run_root: Path):\n","    lines = [f\"python: {platform.python_version()}\"]\n","    try:\n","        import numpy, pandas, torch as _torch\n","        lines.append(f\"numpy: {numpy.__version__}\")\n","        lines.append(f\"pandas: {pd.__version__}\")\n","        lines.append(f\"torch: {_torch.__version__}\")\n","        try:\n","            lines.append(f\"cuda: {_torch.version.cuda}\")\n","            lines.append(f\"cudnn: {_torch.backends.cudnn.version()}\")\n","        except Exception:\n","            pass\n","    except Exception:\n","        pass\n","    (run_root / \"env_manifest.txt\").write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n","\n","def write_file_hashes(run_root: Path):\n","    rows = []\n","    for r, _, fs in os.walk(run_root):\n","        for fn in fs:\n","            p = Path(r) / fn\n","            h = hashlib.sha256()\n","            with open(p, \"rb\") as f:\n","                for chunk in iter(lambda: f.read(1<<20), b\"\"):\n","                    h.update(chunk)\n","            rows.append({\"path\": str(p.relative_to(run_root)).replace(\"\\\\\",\"/\"),\n","                         \"size\": p.stat().st_size,\n","                         \"sha256\": h.hexdigest()})\n","    (run_root / \"file_hashes.json\").write_text(json.dumps(rows, indent=2), encoding=\"utf-8\")\n","\n","def write_provenance(run_root: Path):\n","    rows = [{\n","        \"repo_name\": \"keras-team/keras-io\",\n","        \"repo_url\": \"https://github.com/keras-team/keras-io\",\n","        \"licence\": \"Apache-2.0\",\n","        \"commit_sha\": COMMIT_SHA,\n","        \"imported_files\": \"examples/timeseries/timeseries_weather_forecasting.py\",\n","        \"adaptation_notes\": \"Adapted stacked-LSTM pattern; project-specific loaders, zero-preserving scaler, monthly refit, metrics.\"\n","    }]\n","    (run_root / \"code_provenance.csv\").write_text(pd.DataFrame(rows).to_csv(index=False), encoding=\"utf-8\")\n","    tpd = run_root / \"third_party_licenses\"\n","    tpd.mkdir(parents=True, exist_ok=True)\n","    (tpd / \"README.txt\").write_text(f\"Keras-io example adapted\\nCommit: {COMMIT_SHA}\\nLicence: Apache-2.0\\n\", encoding=\"utf-8\")\n","\n","# -------------------- finalisation / packaging --------------------\n","def features_sha256(feats: List[str]) -> str:\n","    blob = \"\\n\".join([str(x) for x in feats]).encode(\"utf-8\")\n","    return hashlib.sha256(blob).hexdigest()\n","\n","def write_outputs_index(run_root: Path):\n","    idx = {}\n","    for t in TICKERS:\n","        d = run_root / MODEL_ID / t\n","        idx[t] = sorted([p.name for p in d.glob(\"*\")]) if d.exists() else []\n","    (run_root / \"outputs_index.json\").write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n","\n","def assert_checklist_files(run_root: Path):\n","    root = run_root / MODEL_ID\n","    required_top = [\n","        run_root / \"features_manifest.json\",\n","        run_root / \"env_manifest.txt\",\n","        run_root / \"file_hashes.json\",\n","        run_root / \"code_provenance.csv\",\n","        run_root / \"outputs_index.json\",\n","        run_root / \"third_party_licenses\" / \"README.txt\",\n","        run_root / \"README.txt\",\n","    ]\n","    for p in required_top:\n","        if not p.exists():\n","            raise RuntimeError(f\"Missing top-level artefact: {p}\")\n","    for t in TICKERS:\n","        d = root / t\n","        if not d.exists():\n","            raise RuntimeError(f\"Missing ticker directory: {d}\")\n","        req = {\n","            \"predictions\": d / f\"predictions_{MODEL_ID}_{t}.csv\",\n","            \"metrics\":     d / f\"metrics_{MODEL_ID}_{t}.json\",\n","            \"run_config\":  d / f\"run_config_{MODEL_ID}_{t}.json\",\n","            \"curves\":      d / f\"training_curves_{MODEL_ID}_{t}.png\",\n","            \"zero_check\":  d / f\"zero_preservation_check_{t}.csv\",\n","        }\n","        for name, p in req.items():\n","            if not p.exists():\n","                raise RuntimeError(f\"[{t}] missing {name}: {p}\")\n","        n_pred = sum(1 for _ in open(req[\"predictions\"], \"r\", encoding=\"utf-8\")) - 1\n","        if n_pred != ASSERT_N_TEST:\n","            raise RuntimeError(f\"[{t}] predictions rows={n_pred}, expected {ASSERT_N_TEST}\")\n","        m = json.loads(req[\"metrics\"].read_text(encoding=\"utf-8\"))\n","        if m.get(\"n\", None) != ASSERT_N_TEST:\n","            raise RuntimeError(f\"[{t}] metrics 'n' must be {ASSERT_N_TEST}, got {m.get('n')}\")\n","\n","def write_readme(run_root: Path, feats_used: List[str]):\n","    txt = (\n","        \"LSTM_SE pack\\n\"\n","        \"============\\n\\n\"\n","        f\"Model ID: {MODEL_ID}\\n\"\n","        f\"Tickers: {', '.join(TICKERS)}\\n\"\n","        \"Splits:\\n\"\n","        f\"  - Train: {TRAIN_START} to {TRAIN_END}\\n\"\n","        f\"  - Val:   {VAL_START} to {VAL_END}\\n\"\n","        f\"  - Test:  {TEST_START} to {TEST_END} (n=146)\\n\\n\"\n","        \"Contents\\n\"\n","        \"--------\\n\"\n","        \"Top-level:\\n\"\n","        \"  - features_manifest.json\\n\"\n","        \"  - env_manifest.txt\\n\"\n","        \"  - file_hashes.json\\n\"\n","        \"  - code_provenance.csv\\n\"\n","        \"  - outputs_index.json\\n\"\n","        \"  - third_party_licenses/\\n\"\n","        \"  - README.txt\\n\\n\"\n","        \"Per ticker under LSTM_SE/<TICKER>/:\\n\"\n","        f\"  - predictions_{MODEL_ID}_<TICKER>.csv\\n\"\n","        f\"  - metrics_{MODEL_ID}_<TICKER>.json\\n\"\n","        f\"  - run_config_{MODEL_ID}_<TICKER>.json\\n\"\n","        f\"  - training_curves_{MODEL_ID}_<TICKER>.png\\n\"\n","        \"  - zero_preservation_check_<TICKER>.csv\\n\\n\"\n","        \"Reproduction\\n\"\n","        \"------------\\n\"\n","        \"1) Ensure final_inputs/*.csv exist with the expected schema.\\n\"\n","        \"2) Run this script from the project root.\\n\"\n","        \"3) The finalised ZIP is created automatically in the project root.\\n\\n\"\n","        \"Evidence (global invariants)\\n\"\n","        \"----------------------------\\n\"\n","        f\"- Clock: {json.dumps(CLOCK_INVARIANTS)}\\n\"\n","        f\"- Target provenance: {json.dumps(TARGET_PROVENANCE)}\\n\"\n","        f\"- Features parity hash (LSTM_SE): {features_sha256(feats_used)}\\n\"\n","        f\"- Target scaling: {'standard' if SCALE_TARGET else 'none'}; inverse-transform applied before writing predictions and computing metrics.\\n\"\n","        \"- Errors (RMSE/MAE/U2) are computed on level prices; Directional Accuracy uses daily returns with epsilon = 0.0010.\\n\"\n","        \"- Trading diagnostics use the pure-sign rule; costs apply only when the position changes; Turnover counts position changes.\\n\"\n","        \"- Cross-model features parity: compare this SHA256 across LSTM_SE, TRANSFORMER, and HYBRID packs.\\n\"\n","    )\n","    (run_root / \"README.txt\").write_text(txt, encoding=\"utf-8\")\n","\n","def build_zip(run_root: Path, outfile: Optional[Path] = None) -> Path:\n","    run_root = run_root.resolve()\n","    if outfile is None:\n","        stamp = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\n","        outfile = run_root.parent / f\"{run_root.name}_pack_{stamp}.zip\"\n","    with zipfile.ZipFile(outfile, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as zf:\n","        for p in sorted(run_root.rglob(\"*\")):\n","            if p.is_dir(): continue\n","            arc = str(p.relative_to(run_root)).replace(\"\\\\\",\"/\")\n","            zf.write(p, arcname=arc)\n","    h = hashlib.sha256(outfile.read_bytes()).hexdigest()\n","    (run_root / \"bundle_sha256.txt\").write_text(f\"{outfile.name}  {h}\\n\", encoding=\"utf-8\")\n","    print(\"Pack created:\", outfile)\n","    print(\"SHA256:\", (run_root / \"bundle_sha256.txt\").read_text(encoding=\"utf-8\").strip())\n","    return outfile\n","\n","def finalise_pack(feats_used: List[str]):\n","    write_outputs_index(OUT_ROOT)\n","    write_readme(OUT_ROOT, feats_used)\n","    assert_checklist_files(OUT_ROOT)\n","    build_zip(OUT_ROOT)"],"metadata":{"id":"PO8nQUsFRKbn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Per-ticker Runner\n","**What:** Execute monthly refit forecasting for one ticker and write artefacts.  \n","**Why:** Deterministic, leakage-safe forecasts and complete outputs per ticker.  \n","**Method choices:** Refit on first trading day per month; use history `< t`; scalers re-fit per refit using Train-only policy."],"metadata":{"id":"EQSdfDoVRLeg"}},{"cell_type":"code","source":["# -------------------- core runner (per ticker) --------------------\n","def run_one_ticker(ticker: str):\n","    out_dir = MODEL_DIR / ticker\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    df = load_frame(ticker)\n","    splits = fixed_splits(df)\n","    feats = infer_features(df)\n","    sent_cols = sentiment_cols_from(feats)\n","    non_sent_cols = [c for c in feats if c not in sent_cols]\n","\n","    tr_raw = splits[\"train\"].copy()\n","    va_raw = splits[\"val\"].copy()\n","    te_raw = splits[\"test\"].copy()\n","    y_col = \"Target\"\n","\n","    zs_sent = ZeroPreservingStandardScaler(sent_cols).fit(tr_raw) if sent_cols else None\n","    zs_other = StdScaler(non_sent_cols).fit(tr_raw) if non_sent_cols else None\n","\n","    def apply_feat_scalers(df_in: pd.DataFrame) -> pd.DataFrame:\n","        z = df_in.copy()\n","        if zs_sent:  z = zs_sent.transform(z)\n","        if zs_other: z = zs_other.transform(z)\n","        return z\n","\n","    tr = apply_feat_scalers(tr_raw)\n","    va = apply_feat_scalers(va_raw)\n","    te = apply_feat_scalers(te_raw)\n","\n","    emit_zero_preservation_check(\n","        pd.concat([tr_raw, va_raw, te_raw], ignore_index=True),\n","        pd.concat([tr, va, te],    ignore_index=True),\n","        sent_cols,\n","        out_dir / f\"zero_preservation_check_{ticker}.csv\"\n","    )\n","\n","    y_scaler_cls = StdScaler1D if SCALE_TARGET else Identity1D\n","    y_scaler = y_scaler_cls().fit(tr_raw[y_col].to_numpy())\n","\n","    y_tr_scaled = y_scaler.transform(tr_raw[y_col].to_numpy())\n","    y_va_scaled = y_scaler.transform(va_raw[y_col].to_numpy())\n","    x_tr, y_tr = make_sequences(tr[feats], pd.Series(y_tr_scaled, index=tr.index), L)\n","    x_va, y_va = make_sequences(va[feats], pd.Series(y_va_scaled, index=va.index), L)\n","\n","    dl_tr = DataLoader(SeqDS(x_tr, y_tr), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","    dl_va = DataLoader(SeqDS(x_va, y_va), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","    model0 = StackedLSTM(in_dim=len(feats), hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT).to(DEVICE)\n","    tr_hist, va_hist = train_loop(model0, dl_tr, dl_va, EPOCHS, EARLY_STOP_PATIENCE, DEVICE)\n","    save_training_curves(out_dir / f\"training_curves_{MODEL_ID}_{ticker}.png\", tr_hist, va_hist)\n","\n","    test_dates = te_raw[\"date\"].tolist()\n","    first_days = first_trading_days(df)\n","    idx_test = te_raw.reset_index(drop=True)\n","    i_map = {d:i for i,d in enumerate(idx_test[\"date\"])}\n","\n","    # Monthly refit: first trading day per month, exclude day t from history\n","    refit_indices = sorted(set([0] + [i_map[d] for d in first_days if d in i_map]))\n","    chunks = []\n","    for k, start in enumerate(refit_indices):\n","        end = (refit_indices[k+1]-1) if (k+1)<len(refit_indices) else (len(idx_test)-1)\n","        chunks.append((start, end))\n","\n","    y_true_all, y_hat_all, in_flag_all = [], [], []\n","    refit_dates_log, epochs_per_refit = [], []\n","    parameter_count = int(sum(p.numel() for p in model0.parameters()))\n","\n","    for (start_i, end_i) in chunks:\n","        d0 = te_raw.iloc[start_i][\"date\"]\n","        refit_dates_log.append(str(pd.to_datetime(d0).date()))\n","\n","        sub_hist = df[df[\"date\"] < d0].copy()\n","        tr_sub = sub_hist[(sub_hist[\"date\"]>=TRAIN_START) & (sub_hist[\"date\"]<=TRAIN_END)].copy()\n","        va_sub = sub_hist[(sub_hist[\"date\"]>=VAL_START) & (sub_hist[\"date\"]<=VAL_END)].copy()\n","\n","        zs_sent_m = ZeroPreservingStandardScaler(sent_cols).fit(tr_sub) if sent_cols else None\n","        zs_other_m = StdScaler(non_sent_cols).fit(tr_sub) if non_sent_cols else None\n","        y_scaler_m = (StdScaler1D() if SCALE_TARGET else Identity1D()).fit(tr_sub[y_col].to_numpy())\n","\n","        def apply_scalers_now(df_in: pd.DataFrame) -> pd.DataFrame:\n","            z = df_in.copy()\n","            if zs_sent_m:  z = zs_sent_m.transform(z)\n","            if zs_other_m: z = zs_other_m.transform(z)\n","            return z\n","\n","        tr_s = apply_scalers_now(tr_sub)\n","        va_s = apply_scalers_now(va_sub)\n","\n","        y_tr_s = y_scaler_m.transform(tr_sub[y_col].to_numpy())\n","        y_va_s = y_scaler_m.transform(va_sub[y_col].to_numpy())\n","\n","        x_tr_m, y_tr_m = make_sequences(tr_s[feats], pd.Series(y_tr_s, index=tr_s.index), L)\n","        x_va_m, y_va_m = make_sequences(va_s[feats], pd.Series(y_va_s, index=va_s.index), L)\n","\n","        dl_tr_m = DataLoader(SeqDS(x_tr_m, y_tr_m), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","        dl_va_m = DataLoader(SeqDS(x_va_m, y_va_m), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","        model_m = StackedLSTM(in_dim=len(feats), hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT).to(DEVICE)\n","        tr_hist_m, va_hist_m = train_loop(model_m, dl_tr_m, dl_va_m, EPOCHS, EARLY_STOP_PATIENCE, DEVICE)\n","        epochs_per_refit.append(len(tr_hist_m))\n","        parameter_count = int(sum(p.numel() for p in model_m.parameters()))\n","\n","        for i in range(start_i, end_i+1):\n","            ctx_raw = pd.concat([tr_sub, va_sub, te_raw.iloc[:i]], ignore_index=True)\n","            ctx_s   = apply_scalers_now(ctx_raw)\n","            x_seq = ctx_s[feats].to_numpy(dtype=np.float32)[-L:]\n","            yhat_scaled = predict_one_step(model_m, x_seq)\n","            yhat_level  = float(y_scaler_m.inverse_transform(np.array([yhat_scaled], dtype=np.float32))[0])\n","            y_true_all.append(float(te_raw.iloc[i][y_col]))\n","            y_hat_all.append(yhat_level)\n","            in_flag_all.append(0)\n","\n","    # -------------------- metrics and emissions --------------------\n","    y_true = np.array(y_true_all, dtype=float)\n","    y_hat  = np.array(y_hat_all,  dtype=float)\n","    assert len(y_true) == ASSERT_N_TEST == len(y_hat)\n","\n","    # Level-scale assertion and sanity ratio\n","    mean_true, mean_hat = float(np.nanmean(y_true)), float(np.nanmean(y_hat))\n","    if not (0.25 <= (mean_hat + 1e-9) / (mean_true + 1e-9) <= 4.0):\n","        raise RuntimeError(\n","            f\"[{ticker}] Scale sanity check failed: mean(y_hat)={mean_hat:.2f}, mean(y_true)={mean_true:.2f}. \"\n","            \"This indicates an inverse-transform or target mapping problem.\"\n","        )\n","\n","    # Metrics on level scale\n","    rmse_v = rmse(y_true, y_hat)\n","    mae_v  = mae(y_true, y_hat)\n","    u2_v   = theil_u2(y_true, y_hat)\n","\n","    # Directional Accuracy — epsilon-gated, coverage on non-neutral actuals\n","    da_v, cov_v, n_da = dir_accuracy_eps(y_true, y_hat, EPSILON_RET)\n","\n","    # Trading metrics — pure-sign rule, costs only on position changes\n","    sh0, dd0, turn = sharpe_maxdd_turnover(y_true, y_hat, bps=0.0)\n","    sh10, dd10, _  = sharpe_maxdd_turnover(y_true, y_hat, bps=10.0)\n","\n","    # Predictions file (level scale)\n","    pred_df = pd.DataFrame({\n","        \"date\": pd.to_datetime(te_raw[\"date\"]).iloc[:len(y_true)].to_list(),\n","        \"y_true\": y_true,\n","        \"y_hat\": y_hat,\n","        \"residual\": y_true - y_hat,\n","        \"in_sample_flag\": in_flag_all\n","    })\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    pred_df.to_csv(out_dir / f\"predictions_{MODEL_ID}_{ticker}.csv\", index=False)\n","\n","    # Metrics JSON (include Turnover and n)\n","    metrics = {\n","        \"RMSE\": rmse_v, \"MAE\": mae_v, \"U2\": u2_v,\n","        \"DA_epsilon\": da_v, \"Coverage\": cov_v,\n","        \"n\": ASSERT_N_TEST,\n","        \"n_da\": n_da,\n","        \"Sharpe_0bps\": sh0, \"Sharpe_10bps\": sh10,\n","        \"MaxDD_0bps\": dd0, \"MaxDD_10bps\": dd10,\n","        \"Turnover\": int(turn)\n","    }\n","    (out_dir / f\"metrics_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n","\n","    # Run config JSON (architecture, cadence, parity)\n","    run_cfg = {\n","        \"model_id\": MODEL_ID,\n","        \"ticker\": ticker,\n","        \"splits\": {\"train\": [TRAIN_START, TRAIN_END], \"val\": [VAL_START, VAL_END], \"test\": [TEST_START, TEST_END], \"n_test\": ASSERT_N_TEST},\n","        \"cadence\": \"monthly_refit\",\n","        \"refit_dates\": refit_dates_log,\n","        \"seeds\": {\"python\": RANDOM_SEED, \"numpy\": RANDOM_SEED, \"torch\": RANDOM_SEED, \"cuda_seed\": RANDOM_SEED if torch.cuda.is_available() else None},\n","        \"features_used\": feats,\n","        \"features_sha256\": features_sha256(feats),\n","        \"sentiment_zero_columns\": [c for c in feats if c in sent_cols],\n","        \"parameter_count\": int(parameter_count),\n","        \"window_length\": L,\n","        \"batch_size\": BATCH_SIZE,\n","        \"epochs_per_refit\": epochs_per_refit,\n","        \"hyperparameters\": {\"hidden\": HIDDEN, \"layers\": LAYERS, \"dropout\": DROPOUT, \"lr\": LR, \"loss\": \"MSE\", \"optimiser\": \"AdamW\", \"early_stop_patience\": EARLY_STOP_PATIENCE},\n","        \"target_scaling\": \"standard\" if SCALE_TARGET else \"none\",\n","        \"inverse_transform_applied\": True,\n","        \"clock_invariants\": CLOCK_INVARIANTS,\n","        \"target_provenance\": TARGET_PROVENANCE,\n","        \"provenance\": {\"repos\": [{\"name\": \"keras-io example\", \"commit_sha\": COMMIT_SHA, \"licence\": \"Apache-2.0\"}]},\n","        \"device\": DEVICE\n","    }\n","    (out_dir / f\"run_config_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")\n","\n","    return feats"],"metadata":{"id":"mQ2CuEUkRM1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Features Manifest\n","**What:** Record features used and parity hash across tickers.  \n","**Why:** Evidence identical feature set across files and models.  \n","**Method choices:** SHA256 of ordered feature list; list sentiment-zero columns for audits."],"metadata":{"id":"FRK6uOOoRO4X"}},{"cell_type":"code","source":["# -------------------- feature manifest --------------------\n","def write_features_manifest(all_feats_lists: List[List[str]]):\n","    base = all_feats_lists[0]\n","    ok = all(base == f for f in all_feats_lists[1:])\n","    if not ok:\n","        raise RuntimeError(\"Features parity mismatch across tickers\")\n","    features_manifest = {\n","        \"features_used\": base,\n","        \"features_sha256\": features_sha256(base),\n","        \"sentiment_zero_columns\": [c for c in base if c.startswith((\"Tw_\",\"Rd_\",\"Nw_SP500_\"))],\n","        \"intended_cross_model_parity\": \"Compare this SHA256 across packs to evidence parity.\"\n","    }\n","    (OUT_ROOT / \"features_manifest.json\").write_text(json.dumps(features_manifest, indent=2), encoding=\"utf-8\")\n","    (OUT_ROOT / \"cross_model_features_parity_stub.json\").write_text(\n","        json.dumps({\"model_id\": MODEL_ID, \"features_sha256\": features_sha256(base)}, indent=2), encoding=\"utf-8\"\n","    )\n","    return base"],"metadata":{"id":"6Y0ky7puRQSX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Driver\n","**What:** Run all tickers, write manifests, and finalise bundle.  \n","**Why:** Produce a complete, portable artefact pack.  \n","**Method choices:** Deterministic order over tickers; asserts required outputs; builds ZIP and writes bundle hash.\n"],"metadata":{"id":"g773gqWgRSVw"}},{"cell_type":"code","source":["# -------------------- main --------------------\n","def main():\n","    feats_lists = []\n","    for t in TICKERS:\n","        feats_lists.append(run_one_ticker(t))\n","    feats_used = write_features_manifest(feats_lists)\n","    write_provenance(OUT_ROOT)\n","    write_env_manifest(OUT_ROOT)\n","    write_file_hashes(OUT_ROOT)\n","    finalise_pack(feats_used)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"H5SkOAZVRSCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build (if missing) and download the zip from Colab to your PC\n","from pathlib import Path\n","import zipfile, os\n","\n","zip_path = Path(\"/content/lstm_se.zip\")\n","root = Path(\"/content/LSTM_SE_FINAL\")  # where the pipeline writes outputs\n","\n","if not root.exists():\n","    raise FileNotFoundError(\"LSTM_SE_FINAL not found. Run the training/build cell first.\")\n","\n","# Create the zip if it isn't there already\n","if not zip_path.exists():\n","    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as zf:\n","        for p in sorted(root.rglob(\"*\")):\n","            if p.is_file():\n","                arc = \"lstm_se/\" + str(p.relative_to(root)).replace(\"\\\\\",\"/\")\n","                zf.write(str(p), arcname=arc)\n","print(\"Zip ready at:\", zip_path, \"size:\", zip_path.stat().st_size, \"bytes\")\n","\n","from google.colab import files\n","files.download(str(zip_path))  # triggers browser download"],"metadata":{"id":"aRFZsPEwRaMP"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpCuiYPlL1VuGx6G4VkFWW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}