{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7VqfT6r2N8J0z33lWsaD2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 01. Imports and Configuration\n","\n","What: Import libraries; set seeds, paths, fixed splits, and run constants.\n","Why: Deterministic, reproducible environment.\n","Method choices: Fixed splits (Train 2021-02-03→2022-12-30; Val 2023-01-03→2023-05-31; Test 2023-06-01→2023-12-28, n=146); America/New_York 16:00 cut-off; Target = Close.shift(-1); strict Train-only selection for classical models; no scaling of levels."],"metadata":{"id":"5zWjleGTH1KM"}},{"cell_type":"code","source":["import os, sys, json, time, hashlib, platform, warnings, itertools, shutil, random\n","from pathlib import Path\n","from typing import List, Tuple, Dict, Any, Optional\n","import numpy as np\n","import pandas as pd\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n","from statsmodels.tsa.stattools import kpss\n","from scipy.stats import jarque_bera, norm\n","from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n","\n","# Limit to known-benign warnings\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","warnings.filterwarnings(\"ignore\", category=ValueWarning)\n","warnings.filterwarnings(\"ignore\", message=\".*test statistic is outside of the range of p-values.*\")\n","\n","# Config\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","TICKERS = [\"AAPL\", \"AMZN\", \"MSFT\", \"TSLA\", \"AMD\"]\n","DATA_DIR = Path(\".\")\n","OUT_ROOT = Path(\"outputs\")\n","MODEL_ID = \"ARIMAX\"\n","MODEL_DIR = OUT_ROOT / \"arimax\"\n","\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","N_TEST_EXP  = 146\n","\n","EPS_DIR  = 0.0010   # epsilon for sign rule and DA_epsilon\n","ALPHA_95 = 0.05\n","ALPHA_80 = 0.20\n","TRADING_DAYS = 252  # annualisation for Sharpe\n","\n","P_GRID = [0, 1, 2]\n","D_GRID = [0, 1]\n","Q_GRID = [0, 1, 2]\n","TRENDS = [\"n\", \"c\"]\n","\n","NUM_EPS = 1e-12\n","USE_ROBUST_SIGMA_FOR_INTERVALS = False\n","ARCH_LM_SIG_THRESHOLD = 0.01\n","\n","# Preflight checks enabled for final runs.\n","SKIP_PREFLIGHT = False\n","\n","# Option B: inline next-day Close used only to define Target for 2023-12-28.\n","# Values are the official 2023-12-29 closes per ticker.\n","TRUTH_NEXT_CLOSE: Dict[str, Dict[str, float]] = {\n","    \"AAPL\": {\"2023-12-29\": 190.91},  # If using unadjusted Yahoo \"Close\", use 192.53 instead.\n","    \"AMZN\": {\"2023-12-29\": 151.94},\n","    \"MSFT\": {\"2023-12-29\": 371.21},\n","    \"TSLA\": {\"2023-12-29\": 248.48},\n","    \"AMD\":  {\"2023-12-29\": 147.41},\n","}"],"metadata":{"id":"uXnKfLZkH0y0","executionInfo":{"status":"ok","timestamp":1759496494872,"user_tz":-60,"elapsed":61,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# Data Loading\n","**What:** Read level series and exogenous features.  \n","**Why:** Establish leakage-safe inputs aligned to calendar.  \n","**Method choices:** Target created after price load; exog excludes Close/Target; dates sorted and de-duplicated."],"metadata":{"id":"SDI2f6r9JJXr"}},{"cell_type":"code","source":["# Helpers\n","def file_sha256(fp: Path) -> str:\n","    h = hashlib.sha256()\n","    with open(fp, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","def assert_splits_index(dates: pd.Series):\n","    d = pd.to_datetime(dates)\n","    tr = (d >= TRAIN_START) & (d <= TRAIN_END)\n","    va = (d >= VAL_START) & (d <= VAL_END)\n","    te = (d >= TEST_START) & (d <= TEST_END)\n","    te_dates = d[te]\n","    assert len(te_dates) == N_TEST_EXP, f\"Test rows {len(te_dates)} != {N_TEST_EXP}\"\n","    assert str(te_dates.iloc[0].date()) == TEST_START and str(te_dates.iloc[-1].date()) == TEST_END, \"Test date bounds mismatch\"\n","    return tr, va, te\n","\n","def sanitize(v, default=0.0):\n","    try:\n","        fv = float(v)\n","        if not np.isfinite(fv):\n","            return float(default)\n","        return fv\n","    except Exception:\n","        return float(default)\n","\n","def _require_finite(name: str, val: float):\n","    if not (isinstance(val, (int, float)) and np.isfinite(val)):\n","        raise RuntimeError(f\"[Metrics] {name} is not finite: {val}\")\n","\n","# Strict I/O\n","def read_series_strict(ticker: str) -> pd.DataFrame:\n","    ip = DATA_DIR / f\"{ticker}_input.csv\"\n","    if not ip.exists():\n","        raise FileNotFoundError(f\"Missing required file: {ip}\")\n","    df = pd.read_csv(ip)\n","    need = {\"date\", \"Close\"}\n","    if not need.issubset(df.columns):\n","        raise ValueError(f\"{ticker}_input.csv must contain {need}\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    df[\"Target\"] = df[\"Close\"].shift(-1)\n","    return df[[\"date\", \"Close\", \"Target\"]]\n","\n","def read_exog_strict(ticker: str) -> pd.DataFrame:\n","    ex = DATA_DIR / f\"exog_{ticker}.csv\"\n","    if not ex.exists():\n","        raise FileNotFoundError(f\"Missing required file: {ex}\")\n","    df = pd.read_csv(ex)\n","    if \"date\" not in df.columns:\n","        raise ValueError(f\"exog_{ticker}.csv must contain 'date'\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.drop_duplicates(\"date\").sort_values(\"date\").reset_index(drop=True)\n","    drop_cols = [c for c in [\"Close\", \"Target\"] if c in df.columns]\n","    if drop_cols:\n","        df = df.drop(columns=drop_cols)\n","    num_cols = [c for c in df.columns if c != \"date\"]\n","    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n","    return df\n","\n","def _log_missing_dates(series_df: pd.DataFrame, exog_df: pd.DataFrame, ticker: str) -> Optional[Dict[str, Any]]:\n","    s_dates = set(pd.to_datetime(series_df[\"date\"]).dt.date.tolist())\n","    e_dates = set(pd.to_datetime(exog_df[\"date\"]).dt.date.tolist())\n","    missing_in_exog = sorted([str(d) for d in s_dates - e_dates])\n","    missing_in_series = sorted([str(d) for d in e_dates - s_dates])\n","    info = {\"ticker\": ticker, \"missing_in_exog\": missing_in_exog, \"missing_in_series\": missing_in_series}\n","    tdir = MODEL_DIR / ticker\n","    tdir.mkdir(parents=True, exist_ok=True)\n","    (tdir / f\"date_alignment_{ticker}.json\").write_text(\n","        json.dumps({**info, \"note\": \"perfect inner join\" if not (missing_in_exog or missing_in_series) else \"mismatch written\"}, indent=2),\n","        encoding=\"utf-8\"\n","    )\n","    return info"],"metadata":{"id":"slq3JbzFJLSb","executionInfo":{"status":"ok","timestamp":1759496494986,"user_tz":-60,"elapsed":113,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing\n","**What:** Enforce target availability and exog coverage; allow single inline truth row (Option B) to define last Test Target.  \n","**Why:** Guarantee all 146 Test rows are evaluable without editing source CSVs.  \n","**Method choices:** In-memory augmentation only; strict window checks; no future fill."],"metadata":{"id":"4dn4wH_6JMHj"}},{"cell_type":"code","source":["# Preconditions\n","def augment_series_for_missing_target(ser: pd.DataFrame, ticker: str) -> pd.DataFrame:\n","    \"\"\"\n","    If Target for the last Test day (2023-12-28) is NaN, append the next day's Close (2023-12-29)\n","    in memory using TRUTH_NEXT_CLOSE, recompute Target, and return the augmented series.\n","    \"\"\"\n","    s = ser.copy()\n","    s[\"date\"] = pd.to_datetime(s[\"date\"])\n","    mask_test = (s[\"date\"] >= TEST_START) & (s[\"date\"] <= TEST_END)\n","    blk = s.loc[mask_test, [\"date\", \"Target\"]].reset_index(drop=True)\n","    if blk.empty:\n","        raise RuntimeError(f\"[Levels:{ticker}] No rows in the configured Test window {TEST_START}..{TEST_END}.\")\n","    last_test = pd.to_datetime(TEST_END)\n","    need_next = last_test + pd.Timedelta(days=1)  # 2023-12-29 (Fri)\n","    if s.loc[s[\"date\"] == pd.to_datetime(TEST_END), \"Target\"].isna().any():\n","        val = None\n","        if ticker in TRUTH_NEXT_CLOSE:\n","            v = TRUTH_NEXT_CLOSE[ticker].get(str(need_next.date()))\n","            if v is not None:\n","                val = float(v)\n","        if val is None or not np.isfinite(val):\n","            raise RuntimeError(f\"[Levels:{ticker}] Missing Target on {TEST_END} and no inline truth override found. Provide {need_next.date()} Close.\")\n","        base = s[[\"date\",\"Close\"]].copy()\n","        extra = pd.DataFrame({\"date\":[need_next], \"Close\":[val]})\n","        aug = pd.concat([base, extra], ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n","        aug[\"Target\"] = aug[\"Close\"].shift(-1)\n","        s = aug.merge(base, on=[\"date\",\"Close\"], how=\"inner\")[[\"date\",\"Close\",\"Target\"]]\n","        tdir = (MODEL_DIR / ticker); tdir.mkdir(parents=True, exist_ok=True)\n","        (tdir / f\"target_augmentation_{ticker}.json\").write_text(\n","            json.dumps({\n","                \"ticker\": ticker,\n","                \"added_next_day_close_for\": str(need_next.date()),\n","                \"close_value\": float(val),\n","                \"note\": \"in-memory only; used solely to define Target for last Test day.\"\n","            }, indent=2),\n","            encoding=\"utf-8\"\n","        )\n","    return s\n","\n","def _assert_target_defined_for_test(series_df: pd.DataFrame, ticker: str):\n","    s = series_df.copy()\n","    s[\"date\"] = pd.to_datetime(s[\"date\"])\n","    mask_test = (s[\"date\"] >= TEST_START) & (s[\"date\"] <= TEST_END)\n","    blk = s.loc[mask_test, [\"date\", \"Target\"]].reset_index(drop=True)\n","    if blk.empty:\n","        raise RuntimeError(f\"[Levels:{ticker}] No rows in the configured Test window {TEST_START}..{TEST_END}.\")\n","    missing_dates = blk.loc[blk[\"Target\"].isna(), \"date\"].dt.strftime(\"%Y-%m-%d\").tolist()\n","    if missing_dates:\n","        hint = (\n","            f\"[Levels:{ticker}] Missing y_true (Target=Close[t+1]) for Test date(s): {missing_dates[:5]}{'...' if len(missing_dates) > 5 else ''}. \"\n","            f\"Finalisation gate requires full evaluability across {TEST_START}->{TEST_END}.\"\n","        )\n","        raise RuntimeError(hint)\n","\n","def _coverage_gaps(ser: pd.DataFrame, exg: pd.DataFrame, start: str, end: str) -> List[str]:\n","    s = pd.to_datetime(ser[\"date\"]).dt.date\n","    e = pd.to_datetime(exg[\"date\"]).dt.date\n","    mask = (s >= pd.to_datetime(start).date()) & (s <= pd.to_datetime(end).date())\n","    want = set(s[mask].tolist())\n","    have = set(e.tolist())\n","    return [str(d) for d in sorted(want - have)]\n","\n","def _assert_exog_covers_window(ser: pd.DataFrame, exg: pd.DataFrame, start: str, end: str, label: str, ticker: str):\n","    missing = _coverage_gaps(ser, exg, start, end)\n","    if missing:\n","        sample = missing[:5]\n","        raise RuntimeError(f\"[Exog:{ticker}] Missing exogenous rows on {label} date(s): {sample}{'...' if len(missing) > 5 else ''}.\")"],"metadata":{"id":"DBK3acM5JOoL","executionInfo":{"status":"ok","timestamp":1759496495052,"user_tz":-60,"elapsed":65,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Preflight\n","**What:** Early cross-ticker checks for target and exog coverage.  \n","**Why:** Fail fast before any model work.  \n","**Method choices:** Writes `preflight_report.json` on failure; requires full Test evaluability (146 rows)."],"metadata":{"id":"TZhFzlkMJUL7"}},{"cell_type":"code","source":["# Preflight audit\n","def audit_target_coverage(tickers: List[str]) -> Dict[str, Any]:\n","    issues = {}\n","    for t in tickers:\n","        try:\n","            base = read_series_strict(t)\n","            df = augment_series_for_missing_target(base, t)\n","        except Exception as e:\n","            issues[t] = {\"series_read_or_augment_error\": str(e)}\n","            continue\n","        df[\"date\"] = pd.to_datetime(df[\"date\"])\n","        mask = (df[\"date\"] >= TEST_START) & (df[\"date\"] <= TEST_END)\n","        blk = df.loc[mask, [\"date\", \"Target\"]]\n","        missing = blk.loc[blk[\"Target\"].isna(), \"date\"].dt.strftime(\"%Y-%m-%d\").tolist()\n","        if missing:\n","            issues[t] = {\"missing_test_dates_for_target\": missing}\n","    return issues\n","\n","def audit_exog_coverage(tickers: List[str]) -> Dict[str, Any]:\n","    issues = {}\n","    for t in tickers:\n","        try:\n","            ser_base = read_series_strict(t)\n","            ser = augment_series_for_missing_target(ser_base, t)\n","            exg = read_exog_strict(t)\n","        except Exception as e:\n","            issues[t] = {\"io_error\": str(e)}\n","            continue\n","        gaps = {\n","            \"Train\": _coverage_gaps(ser, exg, TRAIN_START, TRAIN_END),\n","            \"Val\":   _coverage_gaps(ser, exg, VAL_START, VAL_END),\n","            \"Test\":  _coverage_gaps(ser, exg, TEST_START, TEST_END),\n","        }\n","        gaps = {k: v for k, v in gaps.items() if v}\n","        if gaps:\n","            issues[t] = gaps\n","    return issues\n","\n","def preflight_inputs(tickers: List[str]):\n","    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n","    issues = {\n","        \"missing_next_day_close_for_Target\": audit_target_coverage(tickers),\n","        \"exog_coverage_gaps\": audit_exog_coverage(tickers),\n","    }\n","    issues = {k: v for k, v in issues.items() if v}\n","    if issues:\n","        report = OUT_ROOT / \"preflight_report.json\"\n","        report.write_text(json.dumps(issues, indent=2), encoding=\"utf-8\")\n","        raise RuntimeError(\n","            \"[Preflight] Inputs invalid. See ARIMAX/preflight_report.json \"\n","            \"for tickers and dates requiring fixes (next-day Close for Target and exog coverage).\"\n","        )"],"metadata":{"id":"aPaDjiVIJRqz","executionInfo":{"status":"ok","timestamp":1759496495061,"user_tz":-60,"elapsed":8,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# Alignment and Merge\n","**What:** Inner-join series and exog; assert splits and scales.  \n","**Why:** Clean, consistent inputs to SARIMAX.  \n","**Method choices:** Levels remain unscaled; features are numeric; splits re-asserted post-merge.\n"],"metadata":{"id":"vajZvdwYJU-b"}},{"cell_type":"code","source":["def load_aligned_frames_from_series(ticker: str, ser: pd.DataFrame) -> Tuple[pd.Series, pd.Series, pd.DataFrame, pd.Series]:\n","    _assert_target_defined_for_test(ser, ticker)\n","    exg = read_exog_strict(ticker)\n","    _log_missing_dates(ser, exg, ticker)\n","    _assert_exog_covers_window(ser, exg, TRAIN_START, TRAIN_END, \"Train\", ticker)\n","    _assert_exog_covers_window(ser, exg, VAL_START, VAL_END, \"Val\", ticker)\n","    _assert_exog_covers_window(ser, exg, TEST_START, TEST_END, \"Test\", ticker)\n","    merged = ser.merge(exg, on=\"date\", how=\"inner\").reset_index(drop=True).sort_values(\"date\")\n","    dates = merged[\"date\"]\n","    assert_splits_index(dates)\n","    close_lvl = merged[\"Close\"].astype(float)\n","    endog = merged[\"Target\"].astype(float)\n","    exog = merged.drop(columns=[\"Close\", \"Target\"])\n","    return endog, close_lvl, exog, dates"],"metadata":{"id":"REHa9i6WJWZz","executionInfo":{"status":"ok","timestamp":1759496495144,"user_tz":-60,"elapsed":72,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Exogenous Spot-Check\n","**What:** Quick causality/zero-encoding probe for sentiment blocks.  \n","**Why:** Verify zero-preservation and sparsity behaviour.  \n","**Method choices:** Prefix-based grouping for Tw_/Rd_/Nw_SP500_*; per-month zero-rate summary.\n"],"metadata":{"id":"YEWwQFVyJYtL"}},{"cell_type":"code","source":["def _prefix_groups(cols: List[str]) -> Dict[str, List[str]]:\n","    pref = {\"Tw_\": [], \"Rd_\": [], \"Nw_SP500_\": []}\n","    for c in cols:\n","        for p in pref.keys():\n","            if c.startswith(p):\n","                pref[p].append(c)\n","    return pref\n","\n","def exog_causality_spotcheck(exog_df: pd.DataFrame, ticker: str) -> Dict[str, Any]:\n","    df = exog_df.copy()\n","    if \"date\" not in df.columns:\n","        return {\"ticker\": ticker, \"note\": \"no-date\", \"ok\": False}\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df[\"ym\"] = df[\"date\"].dt.to_period(\"M\").astype(str)\n","    groups = _prefix_groups([c for c in df.columns if c not in (\"date\", \"ym\")])\n","    out = {\"ticker\": ticker, \"zero_encoding_summary\": {}, \"sparse_month_probe\": {}}\n","    for p, cols in groups.items():\n","        if not cols:\n","            continue\n","        block = df[cols].fillna(0.0).astype(float)\n","        zero_rate = float((block == 0.0).mean().mean()) if not block.empty else 1.0\n","        out[\"zero_encoding_summary\"][p] = {\"n_cols\": len(cols), \"mean_zero_rate\": zero_rate}\n","        bym = block.copy()\n","        bym[\"ym\"] = df[\"ym\"]\n","        agg = bym.groupby(\"ym\", group_keys=False).apply(\n","            lambda x: float((x.drop(columns=[\"ym\"]) == 0.0).mean().mean())\n","        )\n","        if not agg.empty:\n","            sparse_month = agg.idxmax()\n","            out[\"sparse_month_probe\"][p] = {\"month\": sparse_month, \"mean_zero_rate\": float(agg.loc[sparse_month])}\n","    out[\"ok\"] = True\n","    return out"],"metadata":{"id":"LgbYvdMbJZhr","executionInfo":{"status":"ok","timestamp":1759496495161,"user_tz":-60,"elapsed":15,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# Model Definition\n","**What:** Specify SARIMAX for grid selection and forecasting.  \n","**Why:** Choose (p,d,q,trend) by Train AIC and hold spec fixed on Val/Test.  \n","**Method choices:** Train-only grid; no exog scaling; trend ∈ {none, constant}."],"metadata":{"id":"XcxDL55EJboD"}},{"cell_type":"code","source":["# AIC selection (Train only)\n","def aic_grid_search(endog_tr: pd.Series, exog_tr: pd.DataFrame):\n","    best = {\"aic\": np.inf, \"order\": None, \"trend\": None}\n","    rows = []\n","    order_grid = list(itertools.product(P_GRID, D_GRID, Q_GRID))\n","    X = exog_tr.reset_index(drop=True)\n","    if \"date\" in X.columns:\n","        X = X.drop(columns=[\"date\"])\n","    y = endog_tr.reset_index(drop=True)\n","    for p, d, q in order_grid:\n","        for tr in TRENDS:\n","            try:\n","                m = SARIMAX(endog=y, exog=X, order=(p, d, q), trend=tr,\n","                            enforce_stationarity=False, enforce_invertibility=False)\n","                r = m.fit(disp=False, method=\"lbfgs\", maxiter=50)\n","                a = float(r.aic)\n","                rows.append({\"p\": p, \"d\": d, \"q\": q, \"trend\": tr, \"AIC\": a})\n","                if a < best[\"aic\"]:\n","                    best = {\"aic\": a, \"order\": (p, d, q), \"trend\": tr}\n","            except Exception:\n","                rows.append({\"p\": p, \"d\": d, \"q\": q, \"trend\": tr, \"AIC\": np.nan})\n","    grid = pd.DataFrame(rows).sort_values(\"AIC\", ascending=True).reset_index(drop=True)\n","    if best[\"order\"] is None or not np.isfinite(best[\"aic\"]):\n","        raise RuntimeError(\"[AIC] No SARIMAX spec converged on Train grid.\")\n","    return best, grid, grid.head(5).copy()"],"metadata":{"id":"_y16F_pcJc8D","executionInfo":{"status":"ok","timestamp":1759496495186,"user_tz":-60,"elapsed":9,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","**What:** Expanding-origin, one-step-ahead forecasts over Test.  \n","**Why:** Classical evaluation with re-estimation each step and warm-start.  \n","**Method choices:** Outlier guard on extreme jumps; spec fixed from Train AIC."],"metadata":{"id":"w04whaShJeVi"}},{"cell_type":"code","source":["def _assert_level_unscaled(y_level: pd.Series, ticker: str):\n","    y_med = float(np.nanmedian(np.abs(y_level)))\n","    if not np.isfinite(y_med) or y_med <= 0:\n","        raise RuntimeError(f\"[Scale] {ticker}: invalid median level for endog.\")\n","    if y_level.max() > 100000 or y_level.min() < -100000:\n","        raise RuntimeError(f\"[Scale] {ticker}: endog magnitude suggests unintended scaling.\")\n","\n","def _assert_exog_level_unscaled(X_level: pd.DataFrame, ticker: str):\n","    if hasattr(X_level, \"inverse_transform\"):\n","        raise RuntimeError(f\"[Scale] {ticker}: exogenous features appear scaled.\")\n","    Xn = X_level.drop(columns=[\"date\"], errors=\"ignore\").astype(float)\n","    if not Xn.empty:\n","        max_abs = float(np.nanmax(np.abs(Xn.to_numpy(dtype=float))))\n","        if max_abs > 1e6:\n","            raise RuntimeError(f\"[Scale] {ticker}: exog magnitude suggests unintended scaling (max_abs={max_abs:.3g}).\")\n","\n","def _compute_delta_cap(close_lvl_all: pd.Series, dates: pd.Series) -> Dict[str, float]:\n","    d_all = pd.to_datetime(dates).reset_index(drop=True)\n","    train_mask = (d_all >= pd.to_datetime(TRAIN_START)) & (d_all <= pd.to_datetime(TRAIN_END))\n","    c = close_lvl_all.reset_index(drop=True).loc[train_mask].astype(float).to_numpy()\n","    if c.size < 3:\n","        return {\"cap\": 50.0, \"med_abs_delta\": 1.0, \"q99_delta\": 2.0}\n","    deltas = np.abs(np.diff(c))\n","    med_abs = float(np.nanmedian(deltas)) if deltas.size else 1.0\n","    q99 = float(np.nanquantile(deltas, 0.99)) if deltas.size else 2.0\n","    cap = max(20.0, 25.0 * med_abs, 5.0 * q99)\n","    return {\"cap\": float(cap), \"med_abs_delta\": float(med_abs), \"q99_delta\": float(q99)}\n","\n","def forecast_path(endog_all: pd.Series,\n","                  close_lvl_all: pd.Series,\n","                  exog_all: pd.DataFrame,\n","                  dates: pd.Series,\n","                  order: tuple,\n","                  trend: str) -> Tuple[pd.DataFrame, List[str], List[str], Dict[str, float]]:\n","    _, _, te_mask = assert_splits_index(dates)\n","    te_dates = pd.to_datetime(dates[te_mask]).tolist()\n","    X_all = exog_all.reset_index(drop=True)\n","    if \"date\" in X_all.columns:\n","        X_all = X_all.drop(columns=[\"date\"])\n","    y_all = endog_all.reset_index(drop=True)\n","    c_all = close_lvl_all.reset_index(drop=True)\n","    d_all = pd.to_datetime(dates).reset_index(drop=True)\n","\n","    delta_stats = _compute_delta_cap(close_lvl_all, dates)\n","    cap_delta = float(delta_stats[\"cap\"])\n","\n","    rows = []\n","    last_params = None\n","    prediction_fallback_dates: List[str] = []\n","    outlier_guard_dates: List[str] = []\n","\n","    for d in te_dates:\n","        hist_mask = d_all < d\n","        y_hist = y_all.loc[hist_mask]\n","        X_hist = X_all.loc[hist_mask]\n","        X_next = X_all.loc[d_all == d]\n","        if len(X_next) != 1:\n","            raise RuntimeError(f\"[Align] {len(X_next)} rows of exog at {d} (need exactly 1).\")\n","\n","        m = SARIMAX(endog=y_hist, exog=X_hist, order=order, trend=trend,\n","                    enforce_stationarity=False, enforce_invertibility=False)\n","        fit_kwargs = dict(disp=False, method=\"lbfgs\", maxiter=50)\n","        if last_params is not None:\n","            try:\n","                res = m.fit(start_params=last_params, **fit_kwargs)\n","            except Exception:\n","                try:\n","                    res = m.fit(start_params=last_params, disp=False, method=\"nm\", maxiter=200)\n","                except Exception:\n","                    res = m.fit(**fit_kwargs)\n","        else:\n","            res = m.fit(**fit_kwargs)\n","        try:\n","            last_params = np.asarray(res.params, dtype=float)\n","        except Exception:\n","            last_params = None\n","\n","        try:\n","            mean = float(res.get_forecast(steps=1, exog=X_next).predicted_mean.iloc[0])\n","        except Exception:\n","            mean = float(c_all.loc[d_all == d].iloc[0])\n","            prediction_fallback_dates.append(pd.to_datetime(d).strftime(\"%Y-%m-%d\"))\n","\n","        y_true_level = float(y_all.loc[d_all == d].iloc[0])\n","        y_prev_level = float(c_all.loc[d_all == d].iloc[0])\n","\n","        if np.isfinite(mean) and abs(mean - y_prev_level) > cap_delta:\n","            mean = y_prev_level\n","            outlier_guard_dates.append(pd.to_datetime(d).strftime(\"%Y-%m-%d\"))\n","\n","        rows.append({\n","            \"date\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n","            \"y_prev\": y_prev_level,\n","            \"y_true\": y_true_level,\n","            \"y_hat\": mean,\n","            \"in_sample_flag\": 0\n","        })\n","\n","    pred_df = pd.DataFrame(rows)\n","    assert len(pred_df) == N_TEST_EXP, f\"predictions must be {N_TEST_EXP} rows\"\n","    return pred_df, prediction_fallback_dates, outlier_guard_dates, delta_stats"],"metadata":{"id":"f_g3f7z6JfT7","executionInfo":{"status":"ok","timestamp":1759496495250,"user_tz":-60,"elapsed":41,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n","**What:** Residual diagnostics and Test metrics.  \n","**Why:** Assess fit quality, calibration, and trading implications.  \n","**Method choices:** RMSE/MAE/U2/DAε (ε=0.0010); Sharpe & MaxDD at 0 and 10 bps; PICP/MPIW for 95% (and 80% if present).\n"],"metadata":{"id":"aKyR1EEWJhvb"}},{"cell_type":"code","source":["# Diagnostics utilities\n","def _diagnostics_payload(resid: np.ndarray, param_count: Optional[int] = None) -> dict:\n","    kpss_warn = False\n","    if resid.size == 0 or np.all(~np.isfinite(resid)):\n","        out = {\n","            \"ljung_box_stat_lag10\": 0.0, \"ljung_box_pvalue_lag10\": 1.0,\n","            \"ljung_box_stat_lag20\": 0.0, \"ljung_box_pvalue_lag20\": 1.0,\n","            \"arch_lm_stat\": 0.0, \"arch_lm_pvalue\": 1.0, \"arch_lm_nlags\": 12,\n","            \"jarque_bera_stat\": 0.0, \"jarque_bera_pvalue\": 1.0,\n","            \"kpss_stat\": 0.0, \"kpss_pvalue\": 1.0, \"kpss_warning\": False\n","        }\n","    else:\n","        lbdf = acorr_ljungbox(resid, lags=[10, 20], return_df=True)\n","        lb10_stat = float(lbdf[\"lb_stat\"].iloc[0]); lb10_p = float(lbdf[\"lb_pvalue\"].iloc[0])\n","        lb20_stat = float(lbdf[\"lb_stat\"].iloc[1]); lb20_p = float(lbdf[\"lb_pvalue\"].iloc[1])\n","        arch_stat, arch_p, _, _ = het_arch(resid, nlags=12)\n","        jb_stat, jb_p = jarque_bera(resid)\n","        try:\n","            kpss_stat, kpss_p, _, _ = kpss(resid, regression='c', nlags='auto')\n","        except Exception:\n","            kpss_stat, kpss_p, kpss_warn = 0.0, 1.0, True\n","        out = {\n","            \"ljung_box_stat_lag10\": sanitize(lb10_stat, 0.0),\n","            \"ljung_box_pvalue_lag10\": sanitize(lb10_p, 1.0),\n","            \"ljung_box_stat_lag20\": sanitize(lb20_stat, 0.0),\n","            \"ljung_box_pvalue_lag20\": sanitize(lb20_p, 1.0),\n","            \"arch_lm_stat\": sanitize(float(arch_stat), 0.0),\n","            \"arch_lm_pvalue\": sanitize(float(arch_p), 1.0),\n","            \"arch_lm_nlags\": 12,\n","            \"jarque_bera_stat\": sanitize(float(jb_stat), 0.0),\n","            \"jarque_bera_pvalue\": sanitize(float(jb_p), 1.0),\n","            \"kpss_stat\": sanitize(float(kpss_stat), 0.0),\n","            \"kpss_pvalue\": sanitize(float(kpss_p), 1.0),\n","            \"kpss_warning\": bool(kpss_warn)\n","        }\n","    if param_count is not None:\n","        out[\"parameter_count\"] = int(param_count)\n","    out.update({\n","        \"ljungbox_stat_lag10\": out[\"ljung_box_stat_lag10\"],\n","        \"ljungbox_pvalue_lag10\": out[\"ljung_box_pvalue_lag10\"],\n","        \"ljungbox_stat_lag20\": out[\"ljung_box_stat_lag20\"],\n","        \"ljungbox_pvalue_lag20\": out[\"ljung_box_pvalue_lag20\"],\n","    })\n","    return out\n","\n","def residual_diagnostics_in_sample(endog_all: pd.Series, exog_all: pd.DataFrame,\n","                                   dates: pd.Series, order: tuple, trend: str) -> dict:\n","    try:\n","        d_all = pd.to_datetime(dates).reset_index(drop=True)\n","        train_mask = (d_all >= pd.to_datetime(TRAIN_START)) & (d_all <= pd.to_datetime(TRAIN_END))\n","        y = endog_all.reset_index(drop=True).loc[train_mask].reset_index(drop=True)\n","        X = exog_all.reset_index(drop=True).loc[train_mask].reset_index(drop=True)\n","        if \"date\" in X.columns:\n","            X = X.drop(columns=[\"date\"])\n","        m = SARIMAX(endog=y, exog=X, order=order, trend=trend,\n","                    enforce_stationarity=False, enforce_invertibility=False)\n","        r = m.fit(disp=False, method=\"lbfgs\", maxiter=80)\n","        resid = pd.Series(r.resid).dropna().to_numpy(dtype=float)\n","        param_count = len(getattr(r, \"params\", []))\n","        out = _diagnostics_payload(resid, param_count=param_count)\n","        out[\"parameter_count\"] = param_count\n","        out[\"resid_sigma_train\"] = float(np.std(resid, ddof=1)) if resid.size else np.nan\n","        out[\"resid_sigma_train_robust\"] = float(1.4826 * np.nanmedian(np.abs(resid - np.nanmedian(resid)))) if resid.size else np.nan\n","        return out\n","    except Exception:\n","        return _diagnostics_payload(np.array([], dtype=float), param_count=0)\n","\n","def residual_diagnostics_from_test(pred_df: pd.DataFrame) -> dict:\n","    try:\n","        resid = pred_df[\"residual\"].astype(float).to_numpy()\n","        resid = resid[np.isfinite(resid)]\n","        out = _diagnostics_payload(resid)\n","        abs_mean_residual = float(np.mean(np.abs(resid))) if resid.size else 0.0\n","        resid_std = float(np.std(resid, ddof=1)) if resid.size > 1 else 0.0\n","        mean_residual = float(np.mean(resid)) if resid.size else 0.0\n","        out.update({\n","            \"abs_mean_residual\": abs_mean_residual,\n","            \"residual_std\": resid_std,\n","            \"mean_residual\": mean_residual,\n","            \"test_ljung_box_pvalue_lag10\": out[\"ljung_box_pvalue_lag10\"],\n","            \"test_ljung_box_pvalue_lag20\": out[\"ljung_box_pvalue_lag20\"],\n","            \"test_arch_lm_pvalue\": out[\"arch_lm_pvalue\"],\n","            \"test_jarque_bera_pvalue\": out[\"jarque_bera_pvalue\"],\n","            \"test_kpss_pvalue\": out[\"kpss_pvalue\"],\n","            \"test_arch_lm_nlags\": out[\"arch_lm_nlags\"]\n","        })\n","        return out\n","    except Exception:\n","        out = _diagnostics_payload(np.array([], dtype=float))\n","        out.update({\n","            \"abs_mean_residual\": 0.0,\n","            \"residual_std\": 0.0,\n","            \"mean_residual\": 0.0,\n","            \"test_ljung_box_pvalue_lag10\": out[\"ljung_box_pvalue_lag10\"],\n","            \"test_ljung_box_pvalue_lag20\": out[\"ljung_box_pvalue_lag20\"],\n","            \"test_arch_lm_pvalue\": out[\"arch_lm_pvalue\"],\n","            \"test_jarque_bera_pvalue\": out[\"jarque_bera_pvalue\"],\n","            \"test_kpss_pvalue\": out[\"kpss_pvalue\"],\n","            \"test_arch_lm_nlags\": out[\"arch_lm_nlags\"]\n","        })\n","        return out"],"metadata":{"id":"xynjUI8hKP0T","executionInfo":{"status":"ok","timestamp":1759496495255,"user_tz":-60,"elapsed":3,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Metrics (Test-only)\n","**What:** Compute evaluation metrics on strict Test window.  \n","**Why:** Comparable, leakage-safe reporting.  \n","**Method choices:** U2 vs naïve last-close; DAε excludes small moves; turnover from sign changes."],"metadata":{"id":"tenL5qwlJknb"}},{"cell_type":"code","source":["def _slice_test_window(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    if \"in_sample_flag\" in df.columns:\n","        df = df.loc[df[\"in_sample_flag\"] == 0].copy()\n","    else:\n","        df = df.loc[(df[\"date\"] >= TEST_START) & (df[\"date\"] <= TEST_END)].copy()\n","    n = len(df)\n","    assert n == N_TEST_EXP, f\"[TestWindow] Expected {N_TEST_EXP} rows, got {n}\"\n","    dmin = str(df[\"date\"].min().date())\n","    dmax = str(df[\"date\"].max().date())\n","    assert dmin == TEST_START and dmax == TEST_END, f\"[TestWindow] Date bounds {dmin}..{dmax} expected {TEST_START}..{TEST_END}\"\n","    return df\n","\n","def compute_metrics_from_csv(pred_csv_path: Path) -> Dict[str, Any]:\n","    dfp = pd.read_csv(pred_csv_path)\n","    dfp = _slice_test_window(dfp)\n","\n","    y_true = dfp[\"y_true\"].to_numpy(dtype=float)\n","    y_hat  = dfp[\"y_hat\"].to_numpy(dtype=float)\n","    y_prev = dfp[\"y_prev\"].to_numpy(dtype=float)\n","\n","    lower95 = dfp[\"lower_95\"].to_numpy(dtype=float)\n","    upper95 = dfp[\"upper_95\"].to_numpy(dtype=float)\n","    lower80 = dfp.get(\"lower_80\", pd.Series(np.nan, index= dfp.index)).to_numpy(dtype=float)\n","    upper80 = dfp.get(\"upper_80\", pd.Series(np.nan, index= dfp.index)).to_numpy(dtype=float)\n","\n","    req = {\"y_true\": y_true, \"y_hat\": y_hat, \"y_prev\": y_prev, \"lower_95\": lower95, \"upper_95\": upper95}\n","    for k, arr in req.items():\n","        if not np.isfinite(arr).all():\n","            bad_idx = np.where(~np.isfinite(arr))[0][:5]\n","            bad_dates = dfp[\"date\"].iloc[bad_idx].astype(str).tolist()\n","            raise RuntimeError(f\"[Metrics] Non-finite values in {k} at dates {bad_dates}... All 146 rows must be evaluable.\")\n","\n","    resid = y_true - y_hat\n","    rmse = float(np.sqrt(np.mean(resid**2)))\n","    mae  = float(np.mean(np.abs(resid)))\n","\n","    y_true_shift = pd.Series(y_true).shift(1).to_numpy()\n","    valid = np.isfinite(y_true) & np.isfinite(y_true_shift)\n","    denom = float(np.sqrt(np.mean((y_true[valid] - y_true_shift[valid])**2)))\n","    _require_finite(\"U2 denominator\", denom)\n","    u2 = float(rmse / max(denom, NUM_EPS))\n","\n","    ret_true = (y_true / np.where(y_prev == 0.0, 1.0, y_prev)) - 1.0\n","    ret_hat  = (y_hat  / np.where(y_prev == 0.0, 1.0, y_prev)) - 1.0\n","\n","    EPSILON = EPS_DIR\n","    mask = np.abs(ret_true) > EPSILON\n","    if np.count_nonzero(mask) < 10:\n","        raise RuntimeError(\"[Metrics] Too few |return|>epsilon days to compute DA_epsilon robustly.\")\n","    da_eps = float(np.mean(np.sign(ret_hat[mask]) == np.sign(ret_true[mask])))\n","\n","    within95 = (y_true >= lower95) & (y_true <= upper95)\n","    cov95 = float(np.mean(within95))\n","    has80 = np.isfinite(lower80).all() and np.isfinite(upper80).all()\n","    within80 = (y_true >= lower80) & (y_true <= upper80) if has80 else np.zeros_like(y_true, dtype=bool)\n","    cov80  = float(np.mean(within80)) if has80 else np.nan\n","    mpiw95 = float(np.nanmean(upper95 - lower95))\n","    mpiw80 = float(np.nanmean(upper80 - lower80)) if has80 else np.nan\n","\n","    pos = np.where(ret_hat >= EPSILON,  1.0, np.where(ret_hat <= -EPSILON, -1.0, 0.0))\n","    prev_pos = np.r_[0.0, pos[:-1]]\n","    changes = (pos != prev_pos).astype(float)\n","    turnover = int(np.sum(changes))\n","\n","    def pnl_with_cost(bps: float) -> np.ndarray:\n","        cost_rate = bps / 10000.0\n","        return pos * ret_true - changes * cost_rate\n","\n","    pnl0  = pnl_with_cost(0.0)\n","    pnl10 = pnl_with_cost(10.0)\n","\n","    def _annualised_sharpe_local(pnl: np.ndarray) -> float:\n","        m = float(np.mean(pnl))\n","        s = float(np.std(pnl, ddof=1))\n","        return 0.0 if not np.isfinite(s) or s <= 0 else float(np.sqrt(TRADING_DAYS) * m / s)\n","\n","    def _maxdd(pnl: np.ndarray) -> float:\n","        if pnl.size == 0:\n","            return 0.0\n","        equity = np.cumprod(1.0 + pnl)\n","        peak = np.maximum.accumulate(equity)\n","        dd = 1.0 - equity / np.where(peak == 0.0, 1.0, peak)\n","        m = float(np.max(dd)) if dd.size else 0.0\n","        return m if np.isfinite(m) else 0.0\n","\n","    sharpe0 = _annualised_sharpe_local(pnl0)\n","    sharpe10 = _annualised_sharpe_local(pnl10)\n","    dd0 = _maxdd(pnl0)\n","    dd10 = _maxdd(pnl10)\n","\n","    for k, v in {\n","        \"RMSE\": rmse, \"MAE\": mae, \"U2\": u2, \"DA_epsilon\": da_eps,\n","        \"Coverage_95\": cov95, \"Sharpe_0bps\": sharpe0, \"Sharpe_10bps\": sharpe10,\n","        \"MaxDD_0bps\": dd0, \"MaxDD_10bps\": dd10, \"MPIW_95\": mpiw95\n","    }.items():\n","        _require_finite(k, v)\n","\n","    return {\n","        \"RMSE\": rmse, \"MAE\": mae, \"U2\": u2, \"DA_epsilon\": da_eps, \"epsilon\": float(EPSILON),\n","        \"Coverage\": cov95, \"Coverage_95\": cov95, \"Coverage_80\": cov80,\n","        \"PICP_95\": cov95, \"MPIW_95\": mpiw95, \"MPIW_80\": mpiw80,\n","        \"n\": int(len(dfp)), \"Turnover\": turnover,\n","        \"Sharpe_0bps\": sharpe0, \"Sharpe_10bps\": sharpe10, \"MaxDD_0bps\": dd0, \"MaxDD_10bps\": dd10\n","    }"],"metadata":{"id":"34K8M5XrJl9z","executionInfo":{"status":"ok","timestamp":1759496495270,"user_tz":-60,"elapsed":11,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["# Intervals\n","**What:** Build 95% (and 80%) level intervals from residual σ.  \n","**Why:** Interval quality reporting and coverage checks.  \n","**Method choices:** σ from Test residuals by default; robust fallback if ARCH-LM indicates heteroskedasticity and flag enabled."],"metadata":{"id":"Jc_coEIvJnmK"}},{"cell_type":"code","source":["def _assert_pred_values_finite(pred_df: pd.DataFrame):\n","    if not np.isfinite(pred_df[\"y_hat\"].astype(float)).all():\n","        bad = pred_df.loc[~np.isfinite(pred_df[\"y_hat\"].astype(float)), \"date\"].astype(str).tolist()\n","        raise RuntimeError(f\"[Predictions] Non-finite y_hat on: {bad[:5]}{'...' if len(bad) > 5 else ''}.\")\n","    if not np.isfinite(pred_df[\"y_prev\"].astype(float)).all():\n","        bad = pred_df.loc[~np.isfinite(pred_df[\"y_prev\"].astype(float)), \"date\"].astype(str).tolist()\n","        raise RuntimeError(f\"[Predictions] Non-finite y_prev on: {bad[:5]}{'...' if len(bad) > 5 else ''}.\")\n","\n","def _scale_sanity_report(pred_df: pd.DataFrame) -> Dict[str, Any]:\n","    y = pred_df[\"y_true\"].to_numpy(dtype=float)\n","    yhat = pred_df[\"y_hat\"].to_numpy(dtype=float)\n","    yprev = pred_df[\"y_prev\"].to_numpy(dtype=float)\n","    lvl = float(np.nanmedian(np.abs(y)))\n","    med_abs = float(np.nanmedian(np.abs(y - yhat)))\n","    med_abs_naive = float(np.nanmedian(np.abs(y - yprev)))\n","    rmse = float(np.sqrt(np.nanmean((y - yhat) ** 2)))\n","    rmse_naive = float(np.sqrt(np.nanmean((y - yprev) ** 2)))\n","    med_yhat_mag = float(np.nanmedian(np.abs(yhat)))\n","    med_y_mag = float(np.nanmedian(np.abs(y)))\n","    ratio_med_mag = med_yhat_mag / max(med_y_mag, 1e-12)\n","    hard_fail = ((ratio_med_mag < 0.25 or ratio_med_mag > 4.0) or\n","                 (rmse > max(5.0 * max(rmse_naive, 1e-12), 3.0 * max(lvl, 1e-12))))\n","    warn = (not hard_fail and (med_abs > 1.5 * max(med_abs_naive, 1e-12) or rmse > 1.5 * max(rmse_naive, 1e-12)))\n","    return {\"ok\": not hard_fail, \"warn\": warn,\n","            \"stats\": {\"level_median_abs\": lvl, \"med_abs\": med_abs, \"med_abs_naive\": med_abs_naive,\n","                      \"rmse\": rmse, \"rmse_naive\": rmse_naive, \"ratio_med_mag\": ratio_med_mag},\n","            \"reason\": \"scale_blow_up\" if hard_fail else (\"weak_accuracy\" if warn else \"ok\")}\n","\n","def _choose_sigma_for_intervals(test_resid: np.ndarray, train_sigma: Optional[float],\n","                                train_sigma_robust: Optional[float], arch_pvalue: Optional[float]) -> float:\n","    if USE_ROBUST_SIGMA_FOR_INTERVALS and arch_pvalue is not None and np.isfinite(arch_pvalue) and arch_pvalue < ARCH_LM_SIG_THRESHOLD:\n","        s = 1.4826 * float(np.nanmedian(np.abs(test_resid - np.nanmedian(test_resid)))) if test_resid.size else np.nan\n","        if not (np.isfinite(s) and s > 0):\n","            s = float(train_sigma_robust) if train_sigma_robust is not None else np.nan\n","    else:\n","        s = float(np.nanstd(test_resid, ddof=1)) if test_resid.size else np.nan\n","        if not (np.isfinite(s) and s > 0):\n","            s = float(train_sigma) if train_sigma is not None else np.nan\n","    return s if (np.isfinite(s) and s > 0) else np.nan\n","\n","def _rebuild_level_intervals(pred_df: pd.DataFrame,\n","                             sigma_train_fallback: Optional[float],\n","                             sigma_train_robust: Optional[float],\n","                             arch_pvalue_test: Optional[float]) -> Tuple[str, float]:\n","    resid = pred_df[\"residual\"].astype(float).to_numpy()\n","    sigma = _choose_sigma_for_intervals(resid, sigma_train_fallback, sigma_train_robust, arch_pvalue_test)\n","    z95 = float(norm.ppf(1 - ALPHA_95 / 2))\n","    z80 = float(norm.ppf(1 - ALPHA_80 / 2))\n","    if not (np.isfinite(sigma) and sigma > 0):\n","        pred_df[\"lower_95\"] = np.nan; pred_df[\"upper_95\"] = np.nan\n","        pred_df[\"lower_80\"] = np.nan; pred_df[\"upper_80\"] = np.nan\n","        return \"Intervals not rebuilt; σ invalid.\", float(\"nan\")\n","    yhat = pred_df[\"y_hat\"].astype(float).to_numpy()\n","    pred_df[\"lower_95\"] = yhat - z95 * sigma\n","    pred_df[\"upper_95\"] = yhat + z95 * sigma\n","    pred_df[\"lower_80\"] = yhat - z80 * sigma\n","    pred_df[\"upper_80\"] = yhat + z80 * sigma\n","    return f\"Intervals rebuilt from level residual σ={sigma:.6f}.\", float(sigma)\n","\n","def _assert_pred_bands_present(pred_df: pd.DataFrame):\n","    for col in [\"lower_95\", \"upper_95\"]:\n","        if col not in pred_df.columns:\n","            raise RuntimeError(f\"[Predictions] Missing column: {col}\")\n","    arr = pred_df[[\"lower_95\", \"upper_95\"]].to_numpy(dtype=float)\n","    if not np.isfinite(arr).all():\n","        raise RuntimeError(\"[Predictions] Non-finite values in 95% bands.\")\n","    le = (pred_df[\"lower_95\"] <= pred_df[\"y_hat\"]).to_numpy(dtype=bool)\n","    ge = (pred_df[\"upper_95\"] >= pred_df[\"y_hat\"]).to_numpy(dtype=bool)\n","    if not (le.all() and ge.all()):\n","        raise RuntimeError(\"[Predictions] Band monotonicity violated.\")\n","    w95 = (pred_df[\"upper_95\"] - pred_df[\"lower_95\"]).to_numpy(dtype=float)\n","    if not (np.all(np.isfinite(w95)) and np.all(w95 >= 0)):\n","        raise RuntimeError(\"[Predictions] Invalid 95% band widths.\")"],"metadata":{"id":"Hd0NL82QJo7y","executionInfo":{"status":"ok","timestamp":1759496495277,"user_tz":-60,"elapsed":6,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# Outputs and Artefacts\n","**What:** Write predictions, metrics, run_config, diagnostics, AIC grids, and provenance.  \n","**Why:** Meet finalisation gate and reproducibility standards.  \n","**Method choices:** Per-ticker folders; env manifest and file hashes at run root; build zip bundle."],"metadata":{"id":"ygWSa5a2JqXr"}},{"cell_type":"code","source":["# Writers\n","def build_paths(ticker: str) -> Dict[str, Path]:\n","    tdir = MODEL_DIR / ticker\n","    tdir.mkdir(parents=True, exist_ok=True)\n","    return {\n","        \"pred\": tdir / f\"predictions_{MODEL_ID}_{ticker}.csv\",\n","        \"metrics\": tdir / f\"metrics_{MODEL_ID}_{ticker}.json\",\n","        \"run_cfg\": tdir / f\"run_config_{MODEL_ID}_{ticker}.json\",\n","        \"diagn\": tdir / f\"diagnostics_{MODEL_ID}_{ticker}.json\",\n","        \"diagn_test\": tdir / f\"diagnostics_test_{MODEL_ID}_{ticker}.json\",\n","        \"aic_top5\": tdir / f\"appendix_aic_top5_{ticker}.csv\",\n","        \"aic_grid\": tdir / f\"appendix_aic_grid_{ticker}.csv\",\n","        \"intervals80\": tdir / f\"intervals80_{MODEL_ID}_{ticker}.csv\",\n","        \"exog_spot\": tdir / f\"exog_spotcheck_{ticker}.json\",\n","        \"date_align\": tdir / f\"date_alignment_{ticker}.json\",\n","    }\n","\n","def write_run_root():\n","    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n","    env = [\n","        f\"python_version={platform.python_version()}\",\n","        f\"platform={platform.platform()}\",\n","        f\"timestamp_utc={int(time.time())}\",\n","        f\"random_seed={RANDOM_SEED}\",\n","        f\"model_id={MODEL_ID}\",\n","        \"timezone=America/New_York\",\n","        \"market_close=16:00\",\n","        \"metrics_writer=from_predictions_csv\"\n","    ]\n","    (OUT_ROOT / \"env_manifest.txt\").write_text(\"\\n\".join(env) + \"\\n\", encoding=\"utf-8\")\n","    hashes = []\n","    for r, _, files in os.walk(OUT_ROOT):\n","        for fn in files:\n","            if fn == \"file_hashes.json\":\n","                continue\n","            p = Path(r) / fn\n","            rel = p.relative_to(OUT_ROOT)\n","            try:\n","                hashes.append({\"path\": str(rel).replace(\"\\\\\", \"/\"), \"sha256\": file_sha256(p)})\n","            except Exception:\n","                pass\n","    (OUT_ROOT / \"file_hashes.json\").write_text(json.dumps({\"files\": hashes}, indent=2), encoding=\"utf-8\")\n","\n","def write_provenance_note(run_root: Path):\n","    note = (\n","        \"Provenance: SARIMAX (ARIMAX). Train-only AIC on 2021-02-03->2022-12-30; \"\n","        \"fixed (p,d,q,trend) for Val/Test. Expanding-origin one-step Test forecasts with warm-start. \"\n","        \"Endog/Exog unscaled on level values; zero-encoding respected; no forward-fill from future. \"\n","        \"In-memory augmentation only for 2023-12-29 Close to define Target for 2023-12-28. \"\n","        \"Intervals rebuilt on the level residual sigma. Metrics computed over the strict fixed Test window (146 rows). \"\n","        \"Artefacts: predictions CSV; metrics JSON; run_config JSON; diagnostics (train & test); AIC grid & top-5; intervals80; exog_spotcheck; date_alignment.\"\n","    )\n","    (run_root / \"README_provenance.txt\").write_text(note, encoding=\"utf-8\")\n","\n","def save_outputs(ticker: str,\n","                 pred_df: pd.DataFrame,\n","                 run_cfg: dict,\n","                 aic_grid: pd.DataFrame,\n","                 aic_top5: pd.DataFrame,\n","                 diagn_in: dict,\n","                 paths: Dict[str, Path],\n","                 exog_spot: dict,\n","                 diagn_test_pre_sigma: dict,\n","                 fallbacks: List[str],\n","                 outlier_guard_dates: List[str],\n","                 delta_stats: Dict[str, float]):\n","\n","    pred_df = _slice_test_window(pred_df)\n","\n","    # Residuals on level scale\n","    pred_df[\"residual\"] = pred_df[\"y_true\"].astype(float) - pred_df[\"y_hat\"].astype(float)\n","\n","    _assert_pred_values_finite(pred_df)\n","\n","    # Scaling sanity (warn-only unless egregious)\n","    scale_rep = _scale_sanity_report(pred_df)\n","    if not scale_rep[\"ok\"]:\n","        stats = scale_rep[\"stats\"]\n","        raise RuntimeError(\n","            \"[Scale] Sanity check failed (probable scaling/unit error). \"\n","            f\"ratio_med_mag={stats['ratio_med_mag']:.3f}, rmse={stats['rmse']:.3f}, \"\n","            f\"rmse_naive={stats['rmse_naive']:.3f}, level_median_abs={stats['level_median_abs']:.3f}\"\n","        )\n","\n","    arch_p = float(diagn_test_pre_sigma.get(\"arch_lm_pvalue\", np.nan)) if isinstance(diagn_test_pre_sigma, dict) else np.nan\n","    interval_note, sigma_used = _rebuild_level_intervals(\n","        pred_df,\n","        diagn_in.get(\"resid_sigma_train\"),\n","        diagn_in.get(\"resid_sigma_train_robust\"),\n","        arch_p\n","    )\n","    _assert_pred_bands_present(pred_df)\n","\n","    pred_cols = [\"date\", \"y_prev\", \"y_true\", \"y_hat\", \"residual\", \"in_sample_flag\",\n","                 \"lower_95\", \"upper_95\", \"lower_80\", \"upper_80\"]\n","    pred_df.to_csv(paths[\"pred\"], index=False, columns=pred_cols)\n","    pred_df.loc[:, [\"date\", \"lower_80\", \"upper_80\"]].to_csv(paths[\"intervals80\"], index=False)\n","\n","    metrics = compute_metrics_from_csv(paths[\"pred\"])\n","    Path(paths[\"metrics\"]).write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n","\n","    diagn_test = residual_diagnostics_from_test(pred_df)\n","    Path(paths[\"diagn\"]).write_text(json.dumps(diagn_in, indent=2), encoding=\"utf-8\")\n","    Path(paths[\"diagn_test\"]).write_text(json.dumps(diagn_test, indent=2), encoding=\"utf-8\")\n","\n","    aic_grid.to_csv(paths[\"aic_grid\"], index=False)\n","    aic_top5.to_csv(paths[\"aic_top5\"], index=False)\n","\n","    Path(paths[\"exog_spot\"]).write_text(json.dumps(exog_spot, indent=2), encoding=\"utf-8\")\n","\n","    picp95 = float(metrics[\"Coverage_95\"])\n","    near_nominal_95 = abs(picp95 - 0.95) <= 0.03\n","\n","    run_cfg.update({\n","        \"aic_search_scope\": \"Train only\",\n","        \"spec_fixed_on_val_test\": True,\n","        \"band_calibration\": {\n","            \"PICP_95\": picp95,\n","            \"MPIW_95\": float(metrics[\"MPIW_95\"]),\n","            \"PICP_80\": float(metrics.get(\"Coverage_80\", np.nan)) if metrics.get(\"Coverage_80\", None) is not None else np.nan,\n","            \"MPIW_80\": float(metrics.get(\"MPIW_80\", np.nan)) if metrics.get(\"MPIW_80\", None) is not None else np.nan,\n","            \"nominal\": 0.95,\n","            \"near_nominal_95\": bool(near_nominal_95)\n","        },\n","        \"n_test_rows\": int(len(pred_df)),\n","        \"evaluation_policy\": \"all rows evaluated (no exclusions)\",\n","        \"intervals_note\": interval_note,\n","        \"intervals_sigma_used\": float(sigma_used) if np.isfinite(sigma_used) else None,\n","        \"intervals_sigma_policy\": {\n","            \"use_robust_sigma\": bool(USE_ROBUST_SIGMA_FOR_INTERVALS),\n","            \"arch_lm_sig_threshold\": float(ARCH_LM_SIG_THRESHOLD),\n","            \"arch_lm_pvalue_test\": float(arch_p) if np.isfinite(arch_p) else None\n","        },\n","        \"heteroskedasticity_flag\": bool(np.isfinite(arch_p) and arch_p < ARCH_LM_SIG_THRESHOLD),\n","        \"prediction_fallbacks\": fallbacks,\n","        \"prediction_fallbacks_count\": int(len(fallbacks)),\n","        \"prediction_outlier_guard_dates\": outlier_guard_dates,\n","        \"prediction_outlier_guard_count\": int(len(outlier_guard_dates)),\n","        \"prediction_outlier_guard_cap\": float(delta_stats.get(\"cap\", np.nan)),\n","        \"prediction_outlier_guard_stats\": {\n","            \"med_abs_delta_train\": float(delta_stats.get(\"med_abs_delta\", np.nan)),\n","            \"q99_delta_train\": float(delta_stats.get(\"q99_delta\", np.nan)),\n","        },\n","        \"test_date_min\": TEST_START,\n","        \"test_date_max\": TEST_END,\n","        \"scale_sanity\": {\"status\": \"warning\" if scale_rep[\"warn\"] else \"ok\", **scale_rep[\"stats\"]}\n","    })\n","    try:\n","        run_cfg[\"aic_top5_preview\"] = aic_top5.head(5).to_dict(orient=\"records\")\n","    except Exception:\n","        pass\n","\n","    Path(paths[\"run_cfg\"]).write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")"],"metadata":{"id":"5K85OWSDJrJT","executionInfo":{"status":"ok","timestamp":1759496495343,"user_tz":-60,"elapsed":63,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# Per-ticker Runner\n","**What:** End-to-end path for one ticker.  \n","**Why:** Produce predictions and artefacts deterministically.  \n","**Method choices:** Train-only AIC; expanding-origin Test; evidence logs for exog and targets."],"metadata":{"id":"NAoDgbyoJtqT"}},{"cell_type":"code","source":["def run_ticker(ticker: str):\n","    # Load & augment levels (Option B) before any merge\n","    ser_base = read_series_strict(ticker)\n","    ser_aug  = augment_series_for_missing_target(ser_base, ticker)\n","    _assert_target_defined_for_test(ser_aug, ticker)\n","\n","    endog, close_lvl, exog_raw, dates = load_aligned_frames_from_series(ticker, ser_aug)\n","\n","    # Scale sanity preconditions\n","    if endog.isnull().any(): raise RuntimeError(f\"[{ticker}] endog contains NaNs after align.\")\n","    _assert_level_unscaled(endog, ticker)\n","    _assert_exog_level_unscaled(exog_raw, ticker)\n","\n","    tr_mask, _, _ = assert_splits_index(dates)\n","    y_tr = endog.loc[tr_mask].reset_index(drop=True)\n","    X_tr = exog_raw.loc[tr_mask].reset_index(drop=True)\n","\n","    best, aic_grid, aic_top5 = aic_grid_search(y_tr, X_tr)\n","    order, trend, aic_val = tuple(best[\"order\"]), best[\"trend\"], float(best[\"aic\"])\n","\n","    pred_df, fallbacks, outlier_guard_dates, delta_stats = forecast_path(endog, close_lvl, exog_raw, dates, order, trend)\n","\n","    diagn_in = residual_diagnostics_in_sample(endog, exog_raw, dates, order, trend)\n","\n","    features_used = [c for c in exog_raw.columns if c != \"date\"]\n","    features_used_count = int(len(features_used))\n","\n","    exog_path = EXOG_DIR / f\"exog_{ticker}.csv\"\n","    ex_sha = file_sha256(exog_path) if exog_path.exists() else None\n","    ex_min = str(pd.to_datetime(exog_raw[\"date\"]).min().date()) if \"date\" in exog_raw.columns else None\n","    ex_max = str(pd.to_datetime(exog_raw[\"date\"]).max().date()) if \"date\" in exog_raw.columns else None\n","    ex_nrows = int(exog_raw.shape[0])\n","\n","    run_cfg = {\n","        \"model_id\": MODEL_ID, \"ticker\": ticker,\n","        \"order\": list(order), \"trend\": trend, \"AIC\": sanitize(aic_val, 0.0), \"AIC_train\": sanitize(aic_val, 0.0),\n","        \"aic_selected\": {\"p\": int(order[0]), \"d\": int(order[1]), \"q\": int(order[2]), \"trend\": trend, \"AIC\": float(aic_val)},\n","        \"seed\": int(RANDOM_SEED), \"seeds\": {\"python\": int(RANDOM_SEED), \"numpy\": int(RANDOM_SEED), \"global\": int(RANDOM_SEED)},\n","        \"cadence\": \"expanding_origin\",\n","        \"refit_protocol\": \"expanding_origin_one_step\",\n","        \"refit_details\": {\"reestimate_each_step\": True, \"warm_start_previous_params\": True, \"fit_method\": \"lbfgs_then_nelder_mead_fallback\", \"forecast_horizon\": 1},\n","        \"features_used\": features_used, \"features_used_count\": features_used_count,\n","        \"exog_scaling\": \"none\", \"timezone\": \"America/New_York\", \"market_close\": \"16:00\",\n","        \"exog_cutoff_note\": \"All exog built <=16:00 ET; no look-ahead.\",\n","        \"exog_input_evidence\": {\"filename\": f\"exog_{ticker}.csv\", \"sha256\": ex_sha, \"n_rows\": ex_nrows, \"date_min\": ex_min, \"date_max\": ex_max},\n","        \"sentiment_zero_encoding\": True,\n","        \"expanding_origin\": True,\n","        \"target_definition\": \"Target = Close.shift(-1); y_true = Close[t+1], y_prev = Close[t]\",\n","        \"target_col\": \"Target\",\n","        \"policy\": {\"y_scaled\": False, \"exog_lag\": 0},\n","        \"epsilon_for_DA_and_sign_rule\": float(EPS_DIR),\n","        \"sharpe_annualisation\": \"sqrt_252\",\n","        \"splits\": {\"train\": [TRAIN_START, TRAIN_END], \"val\": [VAL_START, VAL_END], \"test\": [TEST_START, TEST_END]},\n","        \"n_test_expected\": int(N_TEST_EXP),\n","        \"u2_baseline\": \"naive_last_close_y_true_shift\",\n","        \"turnover_definition\": \"count sign changes including initial entry\",\n","        \"test_date_min\": TEST_START, \"test_date_max\": TEST_END,\n","        \"parameter_count\": int(diagn_in.get(\"parameter_count\", 0)),\n","        \"inline_truth_override\": TRUTH_NEXT_CLOSE.get(ticker, {})\n","    }\n","\n","    exog_spot = exog_causality_spotcheck(exog_raw, ticker)\n","\n","    # Pre-σ diagnostics on test residuals (based on current preds)\n","    tmp = pred_df.copy()\n","    tmp[\"residual\"] = tmp[\"y_true\"].astype(float) - tmp[\"y_hat\"].astype(float)\n","    diagn_test_pre_sigma = residual_diagnostics_from_test(tmp)\n","\n","    return pred_df, run_cfg, aic_grid, aic_top5, diagn_in, exog_spot, diagn_test_pre_sigma, fallbacks, outlier_guard_dates, delta_stats"],"metadata":{"id":"VyoZsJcpJubL","executionInfo":{"status":"ok","timestamp":1759496495350,"user_tz":-60,"elapsed":3,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["# Driver\n","**What:** Orchestrate end-to-end run, build bundle, surface links.  \n","**Why:** Produce final artefacts and a portable archive.  \n","**Method choices:** Preflight enabled; per-ticker writes; env and file hashes; optional Colab download link."],"metadata":{"id":"kcALnkFcJw6L"}},{"cell_type":"code","source":["def main():\n","    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n","    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","    # Preflight across all tickers\n","    if not SKIP_PREFLIGHT:\n","        preflight_inputs(TICKERS)\n","    else:\n","        print(\"[Preflight] SKIPPED (dev mode). See ARIMAX/preflight_report.json for outstanding issues.\")\n","\n","    # Source capture for provenance\n","    try:\n","        code_dir = OUT_ROOT / \"code\"\n","        code_dir.mkdir(parents=True, exist_ok=True)\n","        src = None\n","        if \"__file__\" in globals():\n","            src = Path(__file__).resolve()\n","        elif len(sys.argv) > 0 and sys.argv[0]:\n","            candidate = Path(sys.argv[0])\n","            src = candidate.resolve() if candidate.exists() else None\n","        if src and src.exists():\n","            shutil.copy2(src, code_dir / src.name)\n","        else:\n","            (code_dir / \"runner_source_unavailable.txt\").write_text(\n","                \"Source path unavailable in this environment (likely a notebook).\\n\", encoding=\"utf-8\"\n","            )\n","    except Exception as e:\n","        print(f\"[Provenance] Could not copy runner script into run root; continuing. ({e})\")\n","\n","    # Per-ticker loop\n","    for t in TICKERS:\n","        paths = build_paths(t)\n","        pred_df, run_cfg, aic_grid, aic_top5, diagn_in, exog_spot, diagn_test_pre_sigma, fallbacks, outlier_guard_dates, delta_stats = run_ticker(t)\n","\n","        if len(fallbacks) > 0:\n","            print(f\"{t}: forecast fallbacks on {len(fallbacks)} day(s): {fallbacks}\")\n","        else:\n","            print(f\"{t}: forecast fallbacks on 0 day(s): []\")\n","\n","        save_outputs(t, pred_df, run_cfg, aic_grid, aic_top5, diagn_in, paths, exog_spot, diagn_test_pre_sigma, fallbacks, outlier_guard_dates, delta_stats)\n","\n","    write_provenance_note(OUT_ROOT)\n","    write_run_root()\n","\n","    bundle = \"ARIMAX_bundle.zip\"\n","    if Path(bundle).exists():\n","        Path(bundle).unlink()\n","    shutil.make_archive(Path(bundle).stem, \"zip\", OUT_ROOT)\n","    print(f\"Built {bundle}\")\n","\n","    cov = {}\n","    for t in TICKERS:\n","        mpath = MODEL_DIR / t / f\"metrics_{MODEL_ID}_{t}.json\"\n","        if mpath.exists():\n","            m = json.loads(mpath.read_text(encoding=\"utf-8\"))\n","            cov[t] = round(float(m.get(\"Coverage_95\", np.nan)), 3)\n","    print(\"Test PICP(95%) by ticker:\", cov)\n","\n","    try:\n","        from google.colab import files\n","        files.download(bundle)\n","    except Exception:\n","        try:\n","            from IPython.display import FileLink, display\n","            display(FileLink(bundle))\n","        except Exception:\n","            print(\"Bundle saved at:\", Path(bundle).resolve())\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvDJ9mhbJwk6","outputId":"67c7ffa8-423d-4d5b-cb4e-072c7f164222"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n","/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n","/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["AAPL: forecast fallbacks on 0 day(s): []\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n","/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n","/tmp/ipython-input-116689283.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  agg = bym.groupby(\"ym\", group_keys=False).apply(\n"]},{"output_type":"stream","name":"stdout","text":["AMZN: forecast fallbacks on 0 day(s): []\n"]}]}]}