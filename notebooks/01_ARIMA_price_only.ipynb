{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUR/zza/IfHSCHGrBxMhnc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Baseline: ARIMA (Price-Only)"],"metadata":{"id":"cC1Lfp5_oHNl"}},{"cell_type":"markdown","source":["##01. Imports and Configuration\n","\n","What this block does. Declares the script header, imports all dependencies, defines fixed splits and invariants, sets random seeds, configures Ljung–Box gates and AIC search policy, and initialises output directories and warnings.\n","Why it is needed. Centralising configuration guarantees reproducibility and keeps the ARIMA baseline aligned with the dissertation protocol.\n","Method choices. Fixed splits: Train = 2021-02-03 → 2022-12-30; Validation = 2023-01-03 → 2023-05-31; Test = 2023-06-01 → 2023-12-28 (n = 146). Target = Close.shift(-1) for policy comparison (naïve last-close used for U2). America/New_York with 16:00 cut-off assumed via upstream data creation. No scaling; univariate Close only; selection by train-only AIC with Ljung–Box double pass. Early stopping on Validation: not applicable. Test cadence: expanding-origin; monthly refit: not applicable here."],"metadata":{"id":"B9vvkiIOoCf5"}},{"cell_type":"code","source":["import os, sys, json, time, hashlib, random, platform, warnings, math, shutil, zipfile\n","from pathlib import Path\n","from typing import Tuple, Dict, Any, List, Optional\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime, timezone\n","import json, shutil, sys\n","\n","\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.stats.diagnostic import het_arch, acorr_ljungbox\n","from statsmodels.stats.stattools import jarque_bera\n","from statsmodels.tsa.stattools import kpss\n","from statsmodels.tools.sm_exceptions import ConvergenceWarning\n","\n","# ---------------- Configuration (proposal-aligned) ----------------\n","MODEL_ID = \"ARIMA\"\n","TICKERS = [\"AAPL\", \"AMZN\", \"MSFT\", \"TSLA\", \"AMD\"]\n","\n","# Fixed splits and invariants\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","TEST_LEN    = 146\n","TIMEZONE    = \"America/New_York\"\n","MKT_CLOSE   = \"16:00\"\n","\n","DA_EPS = 0.0010\n","\n","# AIC grids and adaptive expansion policy\n","P_MAX_STEPS   = [4, 5, 6, 7]\n","D_GRID        = [0, 1, 2]\n","TRENDS_RAW    = [\"none\", \"constant\"]\n","DELTA_AIC_SEQ = [4.0, 5.0, 6.0]\n","TOP_K_SEQ     = [12, 16, 20]\n","\n","# Ljung-Box whiteness double gate\n","LB_LAGS   = (10, 20)\n","LB_ALPHA  = 0.05\n","LB_DOUBLE_PASS_REQUIRED = True\n","\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED)\n","\n","OUT_ROOT   = Path(\"ARIMA_FINAL\")\n","MODEL_ROOT = OUT_ROOT / MODEL_ID\n","OUT_ROOT.mkdir(parents=True, exist_ok=True); MODEL_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","warnings.filterwarnings(\"ignore\", message=\"Non-invertible\")\n","warnings.filterwarnings(\"ignore\", message=\"Non-stationary\")\n","warnings.filterwarnings(\"ignore\", message=\"Maximum Likelihood optimization failed\")\n","\n","from statsmodels.tools.sm_exceptions import InterpolationWarning\n","warnings.filterwarnings(\"ignore\", category=InterpolationWarning)"],"metadata":{"id":"g89O-Rl85hUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Outputs and Artefacts — Provenance Helpers\n","\n","What this block does. Defines utilities to hash files and capture the runtime environment.\n","Why it is needed. Provenance manifests (environment and file hashes) are required for dissertation-grade reproducibility and the finalisation gate.\n","Method choices. Writes env_manifest.txt and file_hashes.json at the run root after artefacts are created."],"metadata":{"id":"Gsd7yH27oLML"}},{"cell_type":"code","source":["# ---------------- Utility: provenance ----------------\n","def sha256_of_file(p: Path) -> str:\n","    h = hashlib.sha256()\n","    with p.open(\"rb\") as f:\n","        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","def write_env_manifest(root: Path):\n","    lines = [\n","        f\"python_version={sys.version.split()[0]}\",\n","        f\"platform={platform.platform()}\",\n","        f\"timestamp_utc={int(time.time())}\",\n","        f\"model_id={MODEL_ID}\",\n","        f\"seed={SEED}\",\n","        f\"timezone={TIMEZONE}\",\n","        f\"market_close={MKT_CLOSE}\",\n","    ]\n","    try:\n","        import numpy, pandas, statsmodels\n","        lines += [f\"numpy={numpy.__version__}\", f\"pandas={pandas.__version__}\", f\"statsmodels={statsmodels.__version__}\"]\n","    except Exception:\n","        pass\n","    (root / \"env_manifest.txt\").write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n","\n","def write_file_hashes(root: Path):\n","    rows = []\n","    for r, _, files in os.walk(root):\n","        for fn in files:\n","            p = Path(r) / fn\n","            rel = p.relative_to(root)\n","            rows.append({\"path\": str(rel).replace(\"\\\\\", \"/\"), \"sha256\": sha256_of_file(p)})\n","    (root / \"file_hashes.json\").write_text(json.dumps({\"files\": rows}, indent=2), encoding=\"utf-8\")"],"metadata":{"id":"lX61EaLc5mfs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data Loading\n","\n","What this block does. Loads per-ticker CSVs and splits them into Train, Validation, and Test windows.\n","Why it is needed. Enforces the fixed temporal protocol and guards the Test length (n = 146) and boundaries.\n","Method choices. Expects columns date and Close; sorts by date; asserts Test window equals 2023-06-01 → 2023-12-28. America/New_York 16:00 cut-off assumed to be handled in data preparation."],"metadata":{"id":"gCbvIIOBoO4e"}},{"cell_type":"code","source":["# ---------------- Data IO ----------------\n","def load_input_csv(ticker: str) -> pd.DataFrame:\n","    p = Path(f\"{ticker}_input.csv\")\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing {p.name} next to the script.\")\n","    df = pd.read_csv(p)\n","    if not {\"date\", \"Close\"}.issubset(df.columns):\n","        raise AssertionError(f\"{p.name} missing required columns: 'date', 'Close'.\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    return df[[\"date\", \"Close\"]].copy()\n","\n","def split_data(df: pd.DataFrame):\n","    tr = df[(df[\"date\"] >= pd.to_datetime(TRAIN_START)) & (df[\"date\"] <= pd.to_datetime(TRAIN_END))].copy().reset_index(drop=True)\n","    va = df[(df[\"date\"] >= pd.to_datetime(VAL_START)) & (df[\"date\"] <= pd.to_datetime(VAL_END))].copy().reset_index(drop=True)\n","    te = df[(df[\"date\"] >= pd.to_datetime(TEST_START)) & (df[\"date\"] <= pd.to_datetime(TEST_END))].copy().reset_index(drop=True)\n","    assert len(te) == TEST_LEN, f\"Test must be {TEST_LEN} rows, got {len(te)}\"\n","    assert str(te[\"date\"].iloc[0].date()) == TEST_START and str(te[\"date\"].iloc[-1].date()) == TEST_END, \"Test window mismatch\"\n","    return tr, va, te"],"metadata":{"id":"a1xzZS5i5v9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Preprocessing\n","\n","What this block does. Provides metric utilities and trading-diagnostic helpers used later.\n","Why it is needed. Ensures consistent evaluation and trading-style summaries across model families.\n","Method choices. No scaling in ARIMA; zero-preservation for Tw_/Rd_/Nw_SP500_* not applicable (no exogenous features). DA_ε computed on returns with ε = 0.0010. Target concept: next-day close for comparison metrics; ARIMA itself predicts level. ES on Validation: not applicable."],"metadata":{"id":"OVJXn5kzoRXV"}},{"cell_type":"code","source":["# ---------------- Metrics ----------------\n","def rmse(a, b): a=np.asarray(a,dtype=np.float64); b=np.asarray(b,dtype=np.float64); return float(np.sqrt(np.mean((a-b)**2)))\n","def mae(a, b):  a=np.asarray(a,dtype=np.float64); b=np.asarray(b,dtype=np.float64); return float(np.mean(np.abs(a-b)))\n","def theils_u2(pred, actual, naive): return rmse(pred, actual) / max(rmse(naive, actual), 1e-12)\n","\n","def directional_accuracy_eps_from_levels(pred_levels, actual_levels, eps=DA_EPS):\n","    pred = np.asarray(pred_levels, dtype=np.float64)\n","    act  = np.asarray(actual_levels, dtype=np.float64)\n","    prev = np.concatenate([[act[0]], act[:-1]])\n","    ret_pred = (pred - prev) / np.where(prev == 0.0, 1.0, prev)\n","    ret_act  = (act  - prev) / np.where(prev == 0.0, 1.0, prev)\n","    mask = np.abs(ret_act) > eps\n","    if mask.sum() == 0: return float(\"nan\")\n","    return float(np.mean(np.sign(ret_pred[mask]) == np.sign(ret_act[mask])))\n","\n","def sharpe_maxdd_turnover_from_levels(pred_levels, actual_levels, cost_bps=0):\n","    pred = np.asarray(pred_levels, dtype=np.float64)\n","    act  = np.asarray(actual_levels, dtype=np.float64)\n","    prev = np.concatenate([[act[0]], act[:-1]])\n","    ret_act = (act - prev) / np.where(prev == 0.0, 1.0, prev)\n","    prd = np.diff(pred)\n","    sig = np.where(prd > 0, 1, np.where(prd < 0, -1, 0))           # length n-1\n","    change = np.concatenate([[int(sig[0] != 0)], (sig[1:] != sig[:-1]).astype(int)]) if len(sig) else np.array([0], dtype=int)\n","    strat = sig * ret_act[1:] - (cost_bps/10000.0) * change\n","    eq = np.concatenate([[1.0], (1.0 + strat).cumprod()])\n","    r = np.diff(eq)\n","    sharpe = float(np.mean(r) / (np.std(r, ddof=1) + 1e-12)) if len(r) > 1 else float(\"nan\")\n","    peak = np.maximum.accumulate(eq)\n","    maxdd = float(np.max(1.0 - eq/peak)) if len(eq) > 1 else 0.0\n","    turnover = int(np.sum(change))\n","    return sharpe, maxdd, turnover"],"metadata":{"id":"egwzadbW5z6u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Definition\n","\n","What this block does. Defines ARIMA fitting, train-only AIC grid search, Ljung–Box screen, and adaptive selection (p-max expansion and δAIC/Top-K bands).\n","Why it is needed. Separates model specification from evaluation and enforces a principled train-only selection policy..\n","Method choices. AIC grid over p,d,q and trend ∈ {none, constant}; adaptive expansion; mandatory Ljung–Box double pass on Train and Train+Val."],"metadata":{"id":"dcYKu0fOoWYS"}},{"cell_type":"code","source":["# ---------------- Fitting and selection ----------------\n","def fit_once(y: np.ndarray, order: Tuple[int,int,int], trend_raw: str):\n","    m = SARIMAX(endog=y, order=order, trend=None if trend_raw==\"none\" else \"c\",\n","                enforce_stationarity=False, enforce_invertibility=False)\n","    return m.fit(disp=False)\n","\n","def aic_grid(y_train: np.ndarray, p_max: int) -> pd.DataFrame:\n","    rows = []\n","    for p in range(p_max+1):\n","        for d in D_GRID:\n","            for q in range(p_max+1):\n","                for tr in TRENDS_RAW:\n","                    try:\n","                        res  = fit_once(y_train, (p,d,q), tr)\n","                        rows.append({\"p\":p,\"d\":d,\"q\":q,\"trend\":tr,\"AIC\":float(res.aic)})\n","                    except Exception:\n","                        rows.append({\"p\":p,\"d\":d,\"q\":q,\"trend\":tr,\"AIC\":np.inf})\n","    grid = pd.DataFrame(rows).sort_values(\"AIC\").reset_index(drop=True)\n","    return grid\n","\n","def lb_pvalues_from_resid(resid: np.ndarray, lags=(10,20)) -> Dict[str, float]:\n","    out = {}\n","    for lag in lags:\n","        df = acorr_ljungbox(resid, lags=[lag], return_df=True)\n","        out[f\"p{lag}\"] = float(df[\"lb_pvalue\"].iloc[0])\n","    return out\n","\n","def candidate_passes_lb(y_train: np.ndarray, y_trainval: np.ndarray, order, trend) -> Dict[str, Any]:\n","    out = {\"train\": {}, \"trainval\": {}, \"double_pass\": False}\n","    res_tr   = fit_once(y_train, order, trend)\n","    resid_tr = np.asarray(res_tr.resid, dtype=np.float64)\n","    p_tr     = lb_pvalues_from_resid(resid_tr, lags=LB_LAGS)\n","    out[\"train\"] = p_tr\n","    res_tv   = fit_once(y_trainval, order, trend)\n","    resid_tv = np.asarray(res_tv.resid, dtype=np.float64)\n","    p_tv     = lb_pvalues_from_resid(resid_tv, lags=LB_LAGS)\n","    out[\"trainval\"] = p_tv\n","    pass_tr  = all(p > LB_ALPHA for p in p_tr.values())\n","    pass_tv  = all(p > LB_ALPHA for p in p_tv.values())\n","    out[\"double_pass\"] = (pass_tr and pass_tv) if LB_DOUBLE_PASS_REQUIRED else pass_tr\n","    return out\n","\n","def select_order_train_aic_lb_adaptive(y_train: np.ndarray, y_trainval: np.ndarray) -> Tuple[Tuple[int,int,int], str, float, Dict[str, Any], pd.DataFrame, pd.DataFrame]:\n","    final_grid = None\n","    top5_final = None\n","    last_best  = ((0,1,0), \"none\", math.inf)\n","    for pmax in P_MAX_STEPS:\n","        grid = aic_grid(y_train, pmax)\n","        if final_grid is None: final_grid = grid.copy()\n","        best_row = grid.iloc[0]\n","        last_best = ((int(best_row.p), int(best_row.d), int(best_row.q)), str(best_row.trend), float(best_row.AIC))\n","        for delta_aic, top_k in zip(DELTA_AIC_SEQ, TOP_K_SEQ):\n","            band = grid[grid[\"AIC\"] <= best_row.AIC + delta_aic].head(top_k).to_dict(orient=\"records\")\n","            passing = []\n","            for row in band:\n","                order = (int(row[\"p\"]), int(row[\"d\"]), int(row[\"q\"]))\n","                tr    = str(row[\"trend\"])\n","                try:\n","                    lb_info = candidate_passes_lb(y_train, y_trainval, order, tr)\n","                except Exception:\n","                    continue\n","                if lb_info[\"double_pass\"]:\n","                    passing.append((row, lb_info))\n","            if passing:\n","                passing.sort(key=lambda tup: (tup[0][\"AIC\"], tup[0][\"p\"]+tup[0][\"q\"]+tup[0][\"d\"], tup[0][\"p\"]+tup[0][\"q\"], tup[0][\"d\"], tup[0][\"p\"], tup[0][\"q\"]))\n","                chosen_row, lb_info = passing[0]\n","                order = (int(chosen_row[\"p\"]), int(chosen_row[\"d\"]), int(chosen_row[\"q\"]))\n","                trend = str(chosen_row[\"trend\"])\n","                sel_meta = {\n","                    \"lb_screen_pass\": True,\n","                    \"lb_double_pass\": True,\n","                    \"lb_train\":  {\"p10\": lb_info[\"train\"].get(\"p10\"),  \"p20\": lb_info[\"train\"].get(\"p20\")},\n","                    \"lb_trainval\": {\"p10\": lb_info[\"trainval\"].get(\"p10\"), \"p20\": lb_info[\"trainval\"].get(\"p20\")},\n","                    \"grid_pmax\": pmax,\n","                    \"delta_aic_used\": float(delta_aic),\n","                    \"top_k_used\": int(top_k)\n","                }\n","                top5_final = grid.head(5).copy()\n","                # Return TRAIN AIC of chosen spec\n","                return order, trend, float(chosen_row[\"AIC\"]), sel_meta, final_grid, top5_final\n","        final_grid = grid.copy()\n","        top5_final = grid.head(5).copy()\n","    order, trend, aic_best = last_best\n","    sel_meta = {\n","        \"lb_screen_pass\": False,\n","        \"lb_double_pass\": False,\n","        \"grid_pmax\": P_MAX_STEPS[-1],\n","        \"delta_aic_used\": float(DELTA_AIC_SEQ[-1]),\n","        \"top_k_used\": int(TOP_K_SEQ[-1])\n","    }\n","    return order, trend, float(aic_best), sel_meta, final_grid, top5_final"],"metadata":{"id":"AIgNUie354ht"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Training\n","\n","What this block does. Implements one-step-ahead forecasting used in walk-forward evaluation.\n","Why it is needed. Re-estimates parameters on each expanding history during Test to mimic operational deployment.\n","Method choices. Expanding-origin; 95% and 80% bands from model CIs when available, otherwise Gaussian fallback from model scale. Monthly refit: not applicable here."],"metadata":{"id":"Pfh0gh2OobQi"}},{"cell_type":"code","source":["# ---------------- Forecasting ----------------\n","Z95 = 1.959963984540054\n","Z80 = 1.2815515655446004\n","\n","def forecast_with_bands(res):\n","    fc = res.get_forecast(steps=1)\n","    mean = float(np.asarray(fc.predicted_mean)[0])\n","    try:\n","        ci95 = fc.conf_int(alpha=0.05); lo95 = float(np.asarray(ci95)[0,0]); hi95 = float(np.asarray(ci95)[0,1])\n","    except Exception:\n","        lo95 = hi95 = None\n","    try:\n","        ci80 = fc.conf_int(alpha=0.20); lo80 = float(np.asarray(ci80)[0,0]); hi80 = float(np.asarray(ci80)[0,1])\n","    except Exception:\n","        lo80 = hi80 = None\n","    if None not in (lo95, hi95, lo80, hi80):\n","        return mean, lo95, hi95, lo80, hi80\n","    se = float(np.sqrt(getattr(res, \"scale\", 1.0)))\n","    if not np.isfinite(se) or se <= 0: se = max(1e-6, 0.001*abs(mean))\n","    return mean, mean - Z95*se, mean + Z95*se, mean - Z80*se, mean + Z80*se\n","\n","def walk_forward(y_all: np.ndarray, idx_val_end: int, order: Tuple[int,int,int], trend_raw: str):\n","    n = len(y_all)\n","    means, lo95, hi95, lo80, hi80 = [], [], [], [], []\n","    for t in range(idx_val_end, n):\n","        y_hist = y_all[:t]\n","        res = fit_once(y_hist, order, trend_raw)\n","        m, l95, u95, l80, u80 = forecast_with_bands(res)\n","        means.append(m); lo95.append(l95); hi95.append(u95); lo80.append(l80); hi80.append(u80)\n","    return np.array(means), np.array(lo95), np.array(hi95), np.array(lo80), np.array(hi80)"],"metadata":{"id":"YZLVZVP_58ui"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation\n","\n","What this block does. Runs the per-ticker pipeline: selection, diagnostics on Train+Val, expanding-origin Test forecasts, metrics (RMSE, MAE, U2, DA_ε, Coverage, Sharpe/MaxDD at 0/10 bps, Turnover), and residual summary.\n","Why it is needed. Produces dissertation-ready measurements and diagnostics.\n","Method choices. Naïve baseline for U2 = last Validation close plus Test lag; DA_ε computed on returns with ε = 0.0010; Coverage from 95% bands; trading diagnostics via pure-sign rule on percentage returns; n must equal 146. No scaling; zero-preservation not applicable."],"metadata":{"id":"jlJyKc3CofJy"}},{"cell_type":"code","source":["# ---------------- Per-ticker run ----------------\n","def run_ticker(ticker: str) -> Dict[str, Any]:\n","    t_dir = (MODEL_ROOT / ticker); t_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Load and split\n","    df = load_input_csv(ticker)\n","    tr, va, te = split_data(df)\n","    y_tr = tr[\"Close\"].to_numpy(np.float64)\n","    y_va = va[\"Close\"].to_numpy(np.float64)\n","    y_te = te[\"Close\"].to_numpy(np.float64)\n","    y_trva = np.concatenate([y_tr, y_va], axis=0)\n","    y_all  = np.concatenate([y_tr, y_va, y_te], axis=0)\n","    idx_val_end = len(y_tr) + len(y_va)\n","    dates_test = te[\"date\"].dt.strftime(\"%Y-%m-%d\").to_numpy()\n","\n","    # Selection (returns TRAIN AIC of chosen spec)\n","    order, trend_raw, aic_train, sel_meta, aic_grid_df, aic_top5 = select_order_train_aic_lb_adaptive(y_tr, y_trva)\n","\n","    # Diagnostics on Train+Val for chosen spec\n","    res_in = fit_once(y_all[:idx_val_end], order, trend_raw)\n","    resid_tv = np.asarray(res_in.resid, dtype=np.float64)\n","    diagnostics = {}\n","\n","    # Ljung-Box on Train+Val\n","    try:\n","        lb10 = acorr_ljungbox(resid_tv, lags=[LB_LAGS[0]], return_df=True)\n","        lb20 = acorr_ljungbox(resid_tv, lags=[LB_LAGS[1]], return_df=True)\n","        diagnostics[\"ljung_box_trainval\"] = {\n","            f\"lag{LB_LAGS[0]}\": {\"stat\": float(lb10[\"lb_stat\"].iloc[0]), \"p\": float(lb10[\"lb_pvalue\"].iloc[0])},\n","            f\"lag{LB_LAGS[1]}\": {\"stat\": float(lb20[\"lb_stat\"].iloc[0]), \"p\": float(lb20[\"lb_pvalue\"].iloc[0])},\n","        }\n","    except Exception as e:\n","        diagnostics[\"ljung_box_trainval\"] = {\"error\": str(e)}\n","\n","    # Jarque-Bera on Train+Val\n","    try:\n","        jb_stat, jb_p, _, _ = jarque_bera(resid_tv)\n","        diagnostics[\"jarque_bera_trainval\"] = {\"stat\": float(jb_stat), \"p\": float(jb_p)}\n","    except Exception as e:\n","        diagnostics[\"jarque_bera_trainval\"] = {\"error\": str(e)}\n","\n","    # ARCH-LM on Train+Val\n","    try:\n","        lm_stat, lm_p, f_stat, f_p = het_arch(resid_tv, nlags=10)\n","        diagnostics[\"arch_lm_trainval\"] = {\"lm_stat\": float(lm_stat), \"lm_p\": float(lm_p), \"f_stat\": float(f_stat), \"f_p\": float(f_p)}\n","    except Exception as e:\n","        diagnostics[\"arch_lm_trainval\"] = {\"error\": str(e)}\n","\n","    # KPSS on Train+Val\n","    try:\n","        kpss_stat, kpss_p, kpss_lags, kpss_crit = kpss(resid_tv, regression=\"c\", nlags=\"auto\")\n","        diagnostics[\"kpss_trainval\"] = {\n","            \"stat\": float(kpss_stat),\n","            \"p\": float(kpss_p),\n","            \"lag\": int(kpss_lags),\n","            \"crit\": {str(k): float(v) for k, v in kpss_crit.items()}\n","        }\n","    except Exception as e:\n","        diagnostics[\"kpss_trainval\"] = {\"error\": str(e)}\n","\n","    # Walk-forward on Test\n","    y_hat, lo95, hi95, lo80, hi80 = walk_forward(y_all, idx_val_end, order, trend_raw)\n","\n","    # Metrics\n","    naive = np.concatenate([[y_va[-1]], y_te[:-1]])\n","    RMSE = rmse(y_hat, y_te)\n","    MAE  = mae(y_hat, y_te)\n","    U2   = theils_u2(y_hat, y_te, naive)\n","    DA_epsilon = directional_accuracy_eps_from_levels(y_hat, y_te, eps=DA_EPS)\n","    Coverage95 = float(np.mean((y_te >= lo95) & (y_te <= hi95)))\n","    nobs = int(len(y_hat))\n","    sh0, mdd0, turn0 = sharpe_maxdd_turnover_from_levels(y_hat, y_te, cost_bps=0)\n","    sh10, mdd10, turn10 = sharpe_maxdd_turnover_from_levels(y_hat, y_te, cost_bps=10)\n","\n","    # Residual summary on Test\n","    resid_test = y_te - y_hat\n","    resid_summary = {\n","        \"mean\": float(np.mean(resid_test)),\n","        \"std\": float(np.std(resid_test, ddof=1)) if nobs > 1 else float(\"nan\"),\n","        \"min\": float(np.min(resid_test)) if nobs > 0 else float(\"nan\"),\n","        \"max\": float(np.max(resid_test)) if nobs > 0 else float(\"nan\")\n","    }\n","    diagnostics[\"residuals_test_summary\"] = resid_summary\n","\n","    # Predictions CSV (with 95% and 80%)\n","    pred_df = pd.DataFrame({\n","        \"date\": dates_test,\n","        \"y_true\": y_te,\n","        \"y_hat\": y_hat,\n","        \"residual\": resid_test,\n","        \"in_sample_flag\": np.zeros_like(y_te, dtype=int),\n","        \"lower_95\": lo95, \"upper_95\": hi95,\n","        \"lower_80\": lo80, \"upper_80\": hi80\n","    })\n","    assert len(pred_df) == TEST_LEN\n","    pred_df.to_csv(t_dir / f\"predictions_{MODEL_ID}_{ticker}.csv\", index=False)\n","\n","    # Appendix 80 percent intervals\n","    appendix_80 = pd.DataFrame({\n","        \"date\": dates_test,\n","        \"y_hat\": y_hat,\n","        \"lower_80\": lo80,\n","        \"upper_80\": hi80\n","    })\n","    appendix_80.to_csv(t_dir / f\"appendix_intervals80_{MODEL_ID}_{ticker}.csv\", index=False)\n","\n","    # Metrics JSON\n","    metrics = {\n","        \"RMSE\": RMSE, \"MAE\": MAE, \"U2\": U2, \"DA_epsilon\": DA_epsilon,\n","        \"Coverage\": Coverage95, \"n\": nobs,\n","        \"Sharpe_0bps\": sh0, \"Sharpe_10bps\": sh10,\n","        \"MaxDD_0bps\": mdd0, \"MaxDD_10bps\": mdd10,\n","        \"Turnover\": int(turn0)  # parity field, cost-agnostic count\n","    }\n","    (t_dir / f\"metrics_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n","\n","    # run_config JSON (with AIC_train recorded explicitly)\n","    trend_short = \"n\" if trend_raw == \"none\" else \"c\"\n","    run_cfg = {\n","        \"model_id\": MODEL_ID,\n","        \"ticker\": ticker,\n","        \"order\": [int(order[0]), int(order[1]), int(order[2])],\n","        \"trend\": trend_short,\n","        \"AIC_train\": float(aic_train),           # <-- added per checklist\n","        \"AIC\": float(aic_train),                 # retained for backward-compat\n","        \"features_used\": [\"Close\"],\n","        \"seed\": SEED,\n","        \"seeds\": {\"global\": SEED},\n","        \"timezone\": TIMEZONE,\n","        \"market_close\": MKT_CLOSE,\n","        \"refit_policy\": \"expanding-origin\",\n","        \"refit_protocol\": \"expanding_origin_one_step\",\n","        \"cadence\": \"expanding_origin\",\n","        \"expanding_origin\": True,\n","        \"target_definition\": \"Close.shift(-1) created after all features\",\n","        \"target_col\": \"Close\",\n","        \"policy\": {\"y_scaled\": False, \"exog_used\": False},\n","        \"splits\": {\n","            \"train_start\": TRAIN_START, \"train_end\": TRAIN_END,\n","            \"val_start\": VAL_START,     \"val_end\": VAL_END,\n","            \"test_start\": TEST_START,   \"test_end\": TEST_END\n","        },\n","        \"selection\": {\"train_only_aic\": True, \"lb_double_pass\": LB_DOUBLE_PASS_REQUIRED, **sel_meta}\n","    }\n","    (t_dir / f\"run_config_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")\n","\n","    # Diagnostics JSON\n","    (t_dir / f\"diagnostics_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(diagnostics, indent=2), encoding=\"utf-8\")\n","\n","    # Appendix AIC grids\n","    aic_grid_df.to_csv(t_dir / f\"appendix_aic_grid_{ticker}.csv\", index=False)\n","    aic_top5.to_csv(t_dir / f\"appendix_aic_top5_{ticker}.csv\", index=False)\n","\n","    lb_tv = diagnostics.get(\"ljung_box_trainval\", {})\n","    tv_p10 = lb_tv.get(f\"lag{LB_LAGS[0]}\", {}).get(\"p\", None)\n","    tv_p20 = lb_tv.get(f\"lag{LB_LAGS[1]}\", {}).get(\"p\", None)\n","    whiteness_pass = (tv_p10 is not None and tv_p20 is not None and tv_p10 > LB_ALPHA and tv_p20 > LB_ALPHA)\n","    return {\n","        \"ticker\": ticker,\n","        \"order\": list(order),\n","        \"trend\": trend_short,\n","        \"AIC_train\": float(aic_train),\n","        \"AIC\": float(aic_train),\n","        \"LB_train_p10\": run_cfg[\"selection\"].get(\"lb_train\", {}).get(\"p10\"),\n","        \"LB_train_p20\": run_cfg[\"selection\"].get(\"lb_train\", {}).get(\"p20\"),\n","        \"LB_trainval_p10\": run_cfg[\"selection\"].get(\"lb_trainval\", {}).get(\"p10\"),\n","        \"LB_trainval_p20\": run_cfg[\"selection\"].get(\"lb_trainval\", {}).get(\"p20\"),\n","        \"LB_tv_whiteness_pass\": bool(whiteness_pass),\n","        \"residual_mean_test\": diagnostics[\"residuals_test_summary\"][\"mean\"]\n","    }"],"metadata":{"id":"KQxJRxTD6BId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Outputs and Artefacts\n","\n","What this block does. Orchestrates runs across tickers, writes provenance manifests and family summary, produces a finalisation gate note, and packages a portable zip after normalising metrics and run_config fields.\n","Why it is needed. Consolidates outcomes and ensures the bundle is submission-ready and reproducible.\n","Method choices. Cadence recorded as expanding-origin with explicit refit_protocol. Packaging fixer recomputes Coverage and Turnover, fills seeds, normalises fields, preserves AIC_train, rebuilds file hashes, and zips ARIMA_FINAL_bundle.zip."],"metadata":{"id":"caO0LUSuoskt"}},{"cell_type":"code","source":["# ============================ Outputs and Artefacts (one-cell orchestration) ============================\n","\n","# ---------- Orchestration ----------\n","def main():\n","    per_ticker: List[Dict[str, Any]] = []\n","    for t in TICKERS:\n","        per_ticker.append(run_ticker(t))\n","\n","    # Provenance manifests (assumes these helpers exist upstream)\n","    try:\n","        write_env_manifest(OUT_ROOT)\n","    except Exception:\n","        pass\n","    try:\n","        write_file_hashes(OUT_ROOT)\n","    except Exception:\n","        pass\n","\n","    family = {\n","        \"model_id\": MODEL_ID,\n","        \"tickers\": per_ticker,\n","        \"family_finalisable\": all(x.get(\"LB_tv_whiteness_pass\", False) for x in per_ticker),\n","        \"gate_policy\": {\n","            \"lb_double_pass_required\": LB_DOUBLE_PASS_REQUIRED,\n","            \"lb_lags\": list(LB_LAGS),\n","            \"alpha\": LB_ALPHA,\n","            \"train_only_aic\": True,\n","            \"adaptive_grid_pmax\": P_MAX_STEPS,\n","            \"delta_aic_seq\": DELTA_AIC_SEQ,\n","            \"top_k_seq\": TOP_K_SEQ\n","        }\n","    }\n","    (OUT_ROOT / \"family_summary_ARIMA.json\").write_text(json.dumps(family, indent=2), encoding=\"utf-8\")\n","\n","    lines = [\"ARIMA family finalisation:\",\n","             f\"- Family finalisable: {family['family_finalisable']}\",\n","             \"Per-ticker whiteness on Train+Val and residual mean on Test:\"]\n","    for x in per_ticker:\n","        lines.append(\n","            f\"  {x['ticker']}: pass={x.get('LB_tv_whiteness_pass')} \"\n","            f\"(Train p10={x.get('LB_train_p10')}, p20={x.get('LB_train_p20')}; \"\n","            f\"Train+Val p10={x.get('LB_trainval_p10')}, p20={x.get('LB_trainval_p20')}); \"\n","            f\"AIC_train={x.get('AIC_train', float('nan')):.3f}; residual_mean_test={x.get('residual_mean_test', float('nan')):.6f}\"\n","        )\n","    gate_text = \"\\n\".join(lines) + \"\\n\"\n","    (OUT_ROOT / \"finalisation_gate.txt\").write_text(gate_text, encoding=\"utf-8\")\n","    print(gate_text, end=\"\")\n","\n","# ---------- Packaging helpers ----------\n","def _iter_json_files(root, pattern) -> List[Path]:\n","    root = root if isinstance(root, Path) else Path(root)\n","    if not root.exists():\n","        return []\n","    return list(root.rglob(pattern))\n","\n","def _iter_csv_files(root, pattern) -> List[Path]:\n","    root = root if isinstance(root, Path) else Path(root)\n","    if not root.exists():\n","        return []\n","    return list(root.rglob(pattern))\n","\n","def _infer_ticker_from_path(p: Path) -> Optional[str]:\n","    cand = p.parent.name.upper()\n","    if cand.isalpha() and 3 <= len(cand) <= 5:\n","        return cand\n","    return None\n","\n","def _safe_read_json(path: Path) -> Optional[Dict[str, Any]]:\n","    try:\n","        return json.loads(path.read_text(encoding=\"utf-8\"))\n","    except Exception:\n","        return None\n","\n","def _safe_write_json(path: Path, obj: Dict[str, Any]):\n","    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n","\n","def _recompute_coverage_and_turnover(pred_csv: Path) -> Tuple[Optional[float], Optional[int]]:\n","    # Returns (coverage_95, turnover_count) computed on Test rows if in_sample_flag is available; else all rows.\n","    try:\n","        df = pd.read_csv(pred_csv)\n","    except Exception:\n","        return (None, None)\n","\n","    # Select evaluation slice (prefer Test)\n","    if \"in_sample_flag\" in df.columns:\n","        eval_df = df[df[\"in_sample_flag\"] == False].copy()\n","        if eval_df.empty:\n","            eval_df = df.copy()\n","    else:\n","        eval_df = df.copy()\n","\n","    # Coverage(95) if bands present\n","    cov = None\n","    if {\"y_true\", \"lower_95\", \"upper_95\"}.issubset(eval_df.columns):\n","        within = (eval_df[\"y_true\"] >= eval_df[\"lower_95\"]) & (eval_df[\"y_true\"] <= eval_df[\"upper_95\"])\n","        cov = float(within.mean()) if len(eval_df) else None\n","\n","    # Turnover from pure-sign rule: position_t = sign(ŷ_{t+1} − y_t).\n","    # We need y_t (previous true). Prefer provided y_prev; otherwise compute from y_true.shift(1).\n","    turn = None\n","    if {\"y_true\", \"y_hat\"}.issubset(eval_df.columns):\n","        if \"y_prev\" in eval_df.columns:\n","            y_prev = eval_df[\"y_prev\"]\n","        else:\n","            y_prev = eval_df[\"y_true\"].shift(1)\n","        # If any NaNs, drop first row for turnover calc\n","        pos = (eval_df[\"y_hat\"] - y_prev).apply(lambda x: 0.0 if pd.isna(x) else (1.0 if x > 0 else (-1.0 if x < 0 else 0.0)))\n","        pos = pos.fillna(0.0)\n","        # Count switches (ignore the very first position which has no previous)\n","        switches = (pos != pos.shift(1)).astype(int)\n","        # The first comparison is NaN -> fill 0\n","        switches.iloc[0] = 0\n","        turn = int(switches.sum())\n","    return (cov, turn)\n","\n","def _update_metrics_from_predictions(root: Path) -> Dict[str, int]:\n","    \"\"\"Recompute Coverage(95) and Turnover for each metrics_*.json where matching predictions exist.\n","       Non-destructive: only fills/updates 'Coverage' and 'Turnover' if we can compute them.\"\"\"\n","    updated = 0\n","    preds_by_ticker: Dict[str, Path] = {}\n","\n","    # Map tickers to their predictions files (choose the one in the same ticker folder if multiple)\n","    for p in _iter_csv_files(root, \"predictions_*.csv\"):\n","        tk = _infer_ticker_from_path(p)\n","        if tk and tk not in preds_by_ticker:\n","            preds_by_ticker[tk] = p\n","\n","    for mf in _iter_json_files(root, \"metrics_*.json\"):\n","        obj = _safe_read_json(mf)\n","        if obj is None:\n","            continue\n","        tk = obj.get(\"ticker\") or _infer_ticker_from_path(mf)\n","        pred_path = preds_by_ticker.get(tk) if tk else None\n","        if pred_path and pred_path.exists():\n","            cov, turn = _recompute_coverage_and_turnover(pred_path)\n","            changed = False\n","            if cov is not None:\n","                # Metrics key name per your spec: 'Coverage' (95%)\n","                if obj.get(\"Coverage\") != cov:\n","                    obj[\"Coverage\"] = cov\n","                    changed = True\n","            if turn is not None:\n","                if obj.get(\"Turnover\") != turn:\n","                    obj[\"Turnover\"] = turn\n","                    changed = True\n","            if changed:\n","                _safe_write_json(mf, obj)\n","                updated += 1\n","    return {\"metrics_recomputed\": updated}\n","\n","def _normalise_metadata(root: Path) -> Dict[str, int]:\n","    touched_metrics = touched_runconfig = 0\n","    # run_config_*.json\n","    for jf in _iter_json_files(root, \"run_config_*.json\"):\n","        obj = _safe_read_json(jf)\n","        if obj is None:\n","            continue\n","        changed = False\n","        if \"model_id\" not in obj:\n","            obj[\"model_id\"] = MODEL_ID\n","            changed = True\n","        if \"ticker\" not in obj:\n","            tk = _infer_ticker_from_path(jf)\n","            if tk:\n","                obj[\"ticker\"] = tk\n","                changed = True\n","        # Preserve AIC_train/seeds if present; do not overwrite.\n","        if changed:\n","            _safe_write_json(jf, obj)\n","            touched_runconfig += 1\n","    # metrics_*.json\n","    for jf in _iter_json_files(root, \"metrics_*.json\"):\n","        obj = _safe_read_json(jf)\n","        if obj is None:\n","            continue\n","        changed = False\n","        if \"model_id\" not in obj:\n","            obj[\"model_id\"] = MODEL_ID\n","            changed = True\n","        if \"ticker\" not in obj:\n","            tk = _infer_ticker_from_path(jf)\n","            if tk:\n","                obj[\"ticker\"] = tk\n","                changed = True\n","        if changed:\n","            _safe_write_json(jf, obj)\n","            touched_metrics += 1\n","    return {\"metrics\": touched_metrics, \"run_config\": touched_runconfig}\n","\n","def _rebuild_file_hashes_root():\n","    try:\n","        write_file_hashes(OUT_ROOT if isinstance(OUT_ROOT, Path) else Path(OUT_ROOT))\n","    except Exception:\n","        pass\n","\n","def _build_bundle(zip_name: str = \"ARIMA_FINAL_bundle.zip\", show_link: bool = True) -> Path:\n","    root = OUT_ROOT if isinstance(OUT_ROOT, Path) else Path(OUT_ROOT)\n","    if not root.exists():\n","        raise FileNotFoundError(f\"OUT_ROOT not found: {root}\")\n","\n","    zpath = Path(zip_name)\n","    if zpath.exists():\n","        zpath.unlink()\n","    shutil.make_archive(zpath.stem, \"zip\", root)\n","\n","    preds = len(list(root.rglob(\"predictions_*.csv\")))\n","    mets  = len(list(root.rglob(\"metrics_*.json\")))\n","    cfgs  = len(list(root.rglob(\"run_config_*.json\")))\n","    # Permissive: matches diagnostics_ARIMA_*.json, diagnostics_test_ARIMA_*.json, etc.\n","    diags = len(list(root.rglob(\"diagnostics*ARIMA*.json\")))\n","\n","    ts = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n","    print(\n","        f\"Bundle: {zpath} | {ts}\\n\"\n","        f\"Counts -> predictions={preds}, metrics={mets}, run_config={cfgs}, diagnostics={diags}\"\n","    )\n","\n","    if show_link:\n","        try:\n","            from IPython.display import FileLink, display, HTML\n","            display(FileLink(str(zpath)))\n","            display(HTML(f'<a href=\"{zpath}\" download>{zpath.name} (click to download)</a>'))\n","        except Exception:\n","            pass\n","\n","    try:\n","        print(\"Bundle absolute path:\", Path(zpath).resolve())\n","    except Exception:\n","        pass\n","\n","    # Optional forced download for Colab\n","    try:\n","        from google.colab import files\n","        files.download(str(zpath))\n","    except Exception:\n","        pass\n","\n","    return zpath\n","\n","# ---------- Single tail ----------\n","if __name__ == \"__main__\":\n","    main()\n","    # Normalise metadata, recompute Coverage/Turnover from predictions (non-destructive), rebuild hashes, zip\n","    changes_meta = _normalise_metadata(OUT_ROOT)\n","    changes_cov_turn = _update_metrics_from_predictions(OUT_ROOT)\n","    _rebuild_file_hashes_root()\n","    _build_bundle(\"ARIMA_FINAL_bundle.zip\", show_link=True)\n","    print(\n","        f\"Patched files -> metrics_meta={changes_meta['metrics']}, \"\n","        f\"run_config_meta={changes_meta['run_config']}, \"\n","        f\"metrics_recomputed={changes_cov_turn['metrics_recomputed']}\"\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"MojbfQED5bTo","executionInfo":{"status":"ok","timestamp":1759075308099,"user_tz":-60,"elapsed":2679331,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"5e1eea00-663f-41f6-abb9-ba583ad318d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ARIMA family finalisation:\n","- Family finalisable: True\n","Per-ticker whiteness on Train+Val and residual mean on Test:\n","  AAPL: pass=True (Train p10=0.9998975717071399, p20=0.9999984758023575; Train+Val p10=0.9999006118380567, p20=0.9999960601192237); AIC_train=2366.368; residual_mean_test=0.120288\n","  AMZN: pass=True (Train p10=0.999999921814528, p20=0.9999997725942792; Train+Val p10=0.9999993317890863, p20=0.9999984546723262); AIC_train=2515.710; residual_mean_test=0.212717\n","  MSFT: pass=True (Train p10=0.9999996424433911, p20=0.9999999965619389; Train+Val p10=0.9999984873504009, p20=0.9999999570808367); AIC_train=2883.122; residual_mean_test=0.358507\n","  TSLA: pass=True (Train p10=0.09608079882323414, p20=0.4555473677092405; Train+Val p10=0.06561186493274906, p20=0.3313120670256253); AIC_train=3553.322; residual_mean_test=0.277793\n","  AMD: pass=True (Train p10=0.9961477440656766, p20=0.9996491629109464; Train+Val p10=0.981515533167941, p20=0.9989028029819025); AIC_train=2522.870; residual_mean_test=0.210925\n","Bundle: ARIMA_FINAL_bundle.zip | 2025-09-28T16:01:47+00:00\n","Counts -> predictions=5, metrics=5, run_config=5, diagnostics=5\n"]},{"output_type":"display_data","data":{"text/plain":["/content/ARIMA_FINAL_bundle.zip"],"text/html":["<a href='ARIMA_FINAL_bundle.zip' target='_blank'>ARIMA_FINAL_bundle.zip</a><br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<a href=\"ARIMA_FINAL_bundle.zip\" download>ARIMA_FINAL_bundle.zip (click to download)</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Bundle absolute path: /content/ARIMA_FINAL_bundle.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_966f6756-3657-4d71-a08b-cd1e91f21df2\", \"ARIMA_FINAL_bundle.zip\", 97631)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Patched files -> metrics_meta=5, run_config_meta=0, metrics_recomputed=5\n"]}]}]}