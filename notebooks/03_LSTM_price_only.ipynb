{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKGWIRJ5gOZtXz8Hv5LZrx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##01. Imports and Configuration\n","\n","What: Set up imports, constants, seeds, paths, and lightweight utility/provenance writers.\n","Why: Ensures fixed environment, deterministic behaviour, and manifests without altering any modelling code.\n","Method choices: Fixed splits (Train 2021-02-03→2022-12-30; Val 2023-01-03→2023-05-31; Test 2023-06-01→2023-12-28), Target = Close.shift(-1), America/New_York with 16:00 cut-off, Train-only scaling for deep models, early stopping on Validation; deep ablations use frozen weights on Test (no monthly refit; expanding-origin is used for ARIMA/ARIMAX, not here)."],"metadata":{"id":"93x5BmYzkMsD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj0RMHolxdIO"},"outputs":[],"source":["import os, sys, json, math, time, hashlib, platform, warnings, shutil, random, io\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Tuple, Dict, List, Optional\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","SCHEMA_VERSION = \"1.0\"\n","\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","TEST_DAYS_EXPECTED = 146\n","\n","LOOKBACK_L = 90\n","HORIZON = 1\n","\n","BATCH_TRAIN = 64\n","BATCH_VAL   = 32\n","MAX_EPOCHS  = 300\n","PATIENCE    = 12\n","LR          = 1e-3\n","GRAD_CLIP   = 0.5\n","\n","HIDDEN_SIZE = 64\n","NUM_LAYERS  = 2\n","DROPOUT     = 0.20\n","BIDIR       = False\n","\n","SEED = 42\n","EPSILON_RET = 0.0010\n","\n","INPUT_DIR = Path(\"final_inputs\")\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","\n","REQUIRED_COLS = [\"date\",\"ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n","\n","def set_all_seeds(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def env_manifest(model_id: str, extra: Optional[Dict] = None) -> Dict[str, str]:\n","    env = {\n","        \"python\": sys.version.split()[0],\n","        \"platform\": platform.platform(),\n","        \"numpy\": np.__version__,\n","        \"pandas\": pd.__version__,\n","        \"torch\": torch.__version__,\n","        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        \"timezone\": \"America/New_York\",\n","        \"market_close\": \"16:00\",\n","        \"model_id\": model_id,\n","        \"seed\": SEED,\n","    }\n","    try:\n","        import sklearn\n","        env[\"sklearn\"] = sklearn.__version__\n","    except Exception:\n","        env[\"sklearn\"] = \"unknown\"\n","    if env[\"device\"] == \"cuda\":\n","        try:\n","            env[\"cuda_name\"] = torch.cuda.get_device_name(0)\n","            env[\"cuda_capability\"] = \".\".join(map(str, torch.cuda.get_device_capability(0)))\n","        except Exception:\n","            pass\n","    if extra:\n","        env.update(extra)\n","    return env\n","\n","def write_run_root_manifests(run_root: Path, model_id: str):\n","    run_root.mkdir(parents=True, exist_ok=True)\n","    with open(run_root / \"env_manifest.txt\", \"w\", encoding=\"utf-8\") as f:\n","        for k, v in env_manifest(model_id).items():\n","            f.write(f\"{k}={v}\\n\")\n","    files = []\n","    for r, _, fs in os.walk(run_root):\n","        for fn in fs:\n","            p = Path(r) / fn\n","            if p.is_file():\n","                h = hashlib.sha256()\n","                with open(p, \"rb\") as fh:\n","                    for chunk in iter(lambda: fh.read(1 << 20), b\"\"):\n","                        h.update(chunk)\n","                rel = str(p.relative_to(run_root)).replace(\"\\\\\",\"/\")\n","                files.append({\"path\": rel, \"sha256\": h.hexdigest()})\n","    (run_root / \"file_hashes.json\").write_text(json.dumps({\"files\": files}, indent=2), encoding=\"utf-8\")\n","\n","def write_methods_note(run_root: Path, model_id: str, features: List[str], cadence: str, label: Optional[str] = None):\n","    note = io.StringIO()\n","    label_str = f\"{label}\" if label else model_id\n","    note.write(f\"{label_str}\\n\")\n","    note.write(\"----\\n\")\n","    note.write(\"This price-only LSTM ablation follows the project invariants.\\n\")\n","    note.write(f\"Inputs: {', '.join(features)}; Target: next-day Close (Close.shift(-1)).\\n\")\n","    note.write(\"Scaling: Train-only z-scaling of X; y is scaled internally and inverse-transformed for metrics.\\n\")\n","    note.write(f\"Cadence: {cadence} on Test (frozen weights). Residual means can be mildly non-zero under drift.\\n\")\n","    note.write(\"Splits: Train 2021-02-03 → 2022-12-30; Val 2023-01-03 → 2023-05-31; Test 2023-06-01 → 2023-12-28.\\n\")\n","    note.write(\"Artefacts: predictions, metrics, run_config, training curves, env manifest, file hashes.\\n\")\n","    (run_root / \"METHODS_NOTE.txt\").write_text(note.getvalue(), encoding=\"utf-8\")\n","\n","def zip_out(root_dir: Path, zip_name: str):\n","    z = Path(zip_name)\n","    if z.exists():\n","        z.unlink()\n","    shutil.make_archive(base_name=str(z).replace(\".zip\",\"\"), format=\"zip\", root_dir=str(root_dir))\n","    print(f\"[zip] {z.resolve()}\")"]},{"cell_type":"markdown","source":["## Data Loading\n","\n","What: Load per-ticker panels, normalise column names, enforce date sorting, and assert the Test window size.\n","Why: Guarantees schema compatibility and correct temporal coverage (n = 146).\n","Method choices: Fixed splits as above; Target = Close.shift(-1) is used downstream; America/New_York 16:00 discipline is recorded in manifests; scaling is Train-only; deep ablations are frozen on Test."],"metadata":{"id":"-B5H_68YkWSO"}},{"cell_type":"code","source":["def load_panel(ticker: str) -> pd.DataFrame:\n","    p = INPUT_DIR / f\"{ticker}_input.csv\"\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing input file: {p}\")\n","    df = pd.read_csv(p)\n","    if \"Date\" in df.columns and \"date\" not in df.columns:\n","        df = df.rename(columns={\"Date\": \"date\"})\n","        df.to_csv(p, index=False)\n","    for c in REQUIRED_COLS:\n","        if c not in df.columns:\n","            raise ValueError(f\"{p.name} missing required column: {c}\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df[df[\"ticker\"] == ticker].sort_values(\"date\").reset_index(drop=True)\n","    return df\n","\n","def mask_window(df: pd.DataFrame, start: str, end: str) -> pd.Series:\n","    return (df[\"date\"] >= pd.to_datetime(start)) & (df[\"date\"] <= pd.to_datetime(end))\n","\n","def assert_test_days(df: pd.DataFrame):\n","    n = int(mask_window(df, TEST_START, TEST_END).sum())\n","    assert n == TEST_DAYS_EXPECTED, f\"Test days != {TEST_DAYS_EXPECTED}. Got {n}.\""],"metadata":{"id":"ET1plr1akZHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing\n","\n","What: Fit Train-only scalers and build causal sequences for Train/Val windows; define dataset wrapper.\n","Why: Ensures no leakage and consistent input shapes (L = 90, horizon 1).\n","Method choices: Train-only z-scaling; zero-preservation applies only to sentiment columns (not used here); fixed splits; Target = Close.shift(-1); early stopping on Val."],"metadata":{"id":"msOh-jnWkZ2f"}},{"cell_type":"code","source":["class SeqDataset(Dataset):\n","    def __init__(self, X: np.ndarray, y: np.ndarray):\n","        self.X = X.astype(np.float32)\n","        self.y = y.astype(np.float32).reshape(-1, 1)\n","    def __len__(self): return len(self.y)\n","    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n","\n","def fit_x_scaler_train_only(df: pd.DataFrame, features: List[str]) -> StandardScaler:\n","    m = mask_window(df, TRAIN_START, TRAIN_END)\n","    sc = StandardScaler()\n","    sc.fit(df.loc[m, features].values)\n","    return sc\n","\n","def fit_y_scaler_train_only(df: pd.DataFrame, target: str) -> StandardScaler:\n","    m = mask_window(df, TRAIN_START, TRAIN_END)\n","    y = df.loc[m, target].values.reshape(-1, 1)\n","    sc = StandardScaler()\n","    sc.fit(y)\n","    return sc\n","\n","def build_sequences(df: pd.DataFrame, x_scaler: StandardScaler, y_scaler: Optional[StandardScaler],\n","                    features: List[str], target: str,\n","                    start: str, end: str, L: int, horizon: int,\n","                    scale_y: bool) -> Tuple[np.ndarray, np.ndarray, List[int]]:\n","    Xs, ys, idxs = [], [], []\n","    F_scaled = x_scaler.transform(df[features].values)\n","    y_level = df[target].values\n","    dates = df[\"date\"].values\n","    for t in range(L - 1, len(df) - horizon):\n","        t_label = t + horizon\n","        d_label = dates[t_label]\n","        if (d_label >= np.datetime64(start)) and (d_label <= np.datetime64(end)):\n","            Xs.append(F_scaled[t - L + 1:t + 1, :])\n","            y_val = y_level[t_label]\n","            if scale_y:\n","                y_val = float(y_scaler.transform(np.array([[y_val]])).ravel()[0])\n","            ys.append(y_val)\n","            idxs.append(t)\n","    if not Xs:\n","        return np.empty((0, L, len(features))), np.empty((0,)), []\n","    return np.stack(Xs, axis=0), np.array(ys), idxs"],"metadata":{"id":"Kb5fbMRGkbvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Definition\n","\n","What: Define the stacked LSTM regressor with last-timestep linear head; utility to count parameters.\n","Why: Establishes the price-only ablation model used as a frozen-weights baseline.\n","Method choices: No bidirectional layers; conservative hidden size; deterministic initialisation."],"metadata":{"id":"m2s5472Pkdl0"}},{"cell_type":"code","source":["class LSTMPrice(nn.Module):\n","    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 2,\n","                 dropout: float = 0.20, bidirectional: bool = False):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            dropout=dropout if num_layers > 1 else 0.0,\n","            batch_first=True,\n","            bidirectional=bidirectional\n","        )\n","        d = hidden_size * (2 if bidirectional else 1)\n","        self.head = nn.Linear(d, 1)\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        last = out[:, -1, :]\n","        yhat = self.head(last)\n","        return yhat\n","\n","def count_params(model: nn.Module) -> int:\n","    return int(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"id":"bqoTn79SkfUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","\n","What: Epoch loop with Adam, MSE loss, gradient clipping, and early stopping on Validation; returns best state.\n","Why: Prevents overfitting and locks the model at the best validation epoch before the frozen Test pass.\n","Method choices: Early stopping on Val only; seeds fixed; loaders deterministic."],"metadata":{"id":"GpFHMWhIkg59"}},{"cell_type":"code","source":["@dataclass\n","class TrainHistory:\n","    train_loss: List[float]\n","    val_loss: List[float]\n","    best_epoch: int\n","    best_val: float\n","\n","def run_epoch(model, loader, loss_fn, device, optim=None, grad_clip=None):\n","    losses = []\n","    if optim is None:\n","        model.eval()\n","        with torch.no_grad():\n","            for xb, yb in loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                yhat = model(xb)\n","                losses.append(loss_fn(yhat, yb).item())\n","        return float(np.mean(losses)) if losses else float(\"nan\")\n","    else:\n","        model.train()\n","        for xb, yb in loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            optim.zero_grad()\n","            yhat = model(xb)\n","            loss = loss_fn(yhat, yb)\n","            loss.backward()\n","            if grad_clip is not None:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n","            optim.step()\n","            losses.append(loss.item())\n","        return float(np.mean(losses)) if losses else float(\"nan\")\n","\n","def train_with_early_stopping(model: nn.Module,\n","                              train_loader: DataLoader,\n","                              val_loader: DataLoader,\n","                              max_epochs: int = 300,\n","                              lr: float = 1e-3,\n","                              patience: int = 12,\n","                              grad_clip: float = 0.5,\n","                              device: str = \"cpu\") -> TrainHistory:\n","    model = model.to(device)\n","    opt = torch.optim.Adam(model.parameters(), lr=lr)\n","    loss_fn = nn.MSELoss()\n","    best_state, best_val = None, float(\"inf\")\n","    best_epoch, no_improve = -1, 0\n","    train_hist, val_hist = [], []\n","    for epoch in range(1, max_epochs + 1):\n","        tr = run_epoch(model, train_loader, loss_fn, device, optim=opt, grad_clip=grad_clip)\n","        va = run_epoch(model, val_loader,   loss_fn, device, optim=None)\n","        train_hist.append(tr); val_hist.append(va)\n","        if va < best_val - 1e-8:\n","            best_val, best_epoch, no_improve = va, epoch, 0\n","            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n","        else:\n","            no_improve += 1\n","        if no_improve >= patience:\n","            break\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    return TrainHistory(train_hist, val_hist, best_epoch, best_val)"],"metadata":{"id":"-fC0b7PykjrC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation\n","\n","What: Compute point-forecast metrics (RMSE/MAE/U2), ε-gated DA, pure-sign trading diagnostics (Sharpe/MaxDD/Turnover), and walk-forward inference under frozen weights.\n","Why: Quantifies accuracy vs naïve, directional skill with ε-gating, and realised trading diagnostics using the stated rule.\n","Method choices: DA uses ε = 0.0010; trading uses pure-sign positions with costs on changes; returns are percentage; frozen weights on Test (deep); expanding-origin cadence applies to ARIMA/ARIMAX, not here."],"metadata":{"id":"-10Q_k3Jklh2"}},{"cell_type":"code","source":["def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n","\n","def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    return float(np.mean(np.abs(y_true - y_pred)))\n","\n","def theils_u2(y_true: np.ndarray, y_pred: np.ndarray, y_last: np.ndarray) -> float:\n","    num = np.sqrt(np.mean((y_true - y_pred) ** 2))\n","    den = np.sqrt(np.mean((y_true - y_last) ** 2))\n","    return float(num / den) if den > 0 else float(\"inf\")\n","\n","def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray, y_last: np.ndarray,\n","                         epsilon_ret: float = 0.0010) -> Dict[str, float]:\n","    r_true = (y_true - y_last) / np.where(y_last==0.0, 1.0, y_last)\n","    r_pred = (y_pred - y_last) / np.where(y_last==0.0, 1.0, y_last)\n","    mask = np.abs(r_true) > epsilon_ret\n","    n = int(mask.sum())\n","    if n == 0:\n","        return {\"DA\": float(\"nan\"), \"coverage\": 0.0, \"n\": 0}\n","    agree = np.sign(r_true[mask]) == np.sign(r_pred[mask])\n","    return {\"DA\": float(np.mean(agree)), \"coverage\": float(n / len(r_true)), \"n\": n}\n","\n","def sharpe_and_maxdd(returns: np.ndarray) -> Tuple[float, float]:\n","    if returns.size == 0:\n","        return float(\"nan\"), float(\"nan\")\n","    mu = np.mean(returns)\n","    sd = np.std(returns, ddof=1)\n","    sharpe = float(mu / sd) if sd > 0 else float(\"nan\")\n","    equity = (1.0 + returns).cumprod()\n","    peak = np.maximum.accumulate(equity)\n","    maxdd = float(np.max(1.0 - equity / peak))\n","    return sharpe, maxdd\n","\n","def direction_only_returns(y_true: np.ndarray, y_pred: np.ndarray, y_last: np.ndarray,\n","                           epsilon_ret: float, bps_cost: float) -> Tuple[np.ndarray, np.ndarray]:\n","    r_true = (y_true - y_last) / np.where(y_last==0.0, 1.0, y_last)\n","    r_pred = (y_pred - y_last) / np.where(y_last==0.0, 1.0, y_last)\n","    pos = np.sign(r_pred).astype(float)\n","    pos_prev = np.roll(pos, 1); pos_prev[0] = 0.0\n","    changed = (pos != pos_prev).astype(float)\n","    tc = (bps_cost / 10000.0) * changed\n","    strat = pos * r_true - tc\n","    return strat, (changed > 0)\n","\n","def walkforward_predict(df: pd.DataFrame, x_scaler: StandardScaler, y_scaler: StandardScaler,\n","                        model: nn.Module, features: List[str], L: int) -> pd.DataFrame:\n","    model.eval()\n","    device = next(model.parameters()).device\n","    dates = df[\"date\"].values\n","    close = df[\"Close\"].values\n","    F_scaled = x_scaler.transform(df[features].values)\n","    rows = []\n","    for t in range(L - 1, len(df) - HORIZON):\n","        t_label = t + HORIZON\n","        d_label = dates[t_label]\n","        if (d_label < np.datetime64(TEST_START)) or (d_label > np.datetime64(TEST_END)):\n","            continue\n","        x_win = F_scaled[t - L + 1:t + 1, :]\n","        xb = torch.from_numpy(x_win.astype(np.float32)).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            yhat_scaled = model(xb).cpu().numpy().ravel()[0]\n","        yhat_level = float(y_scaler.inverse_transform(np.array([[yhat_scaled]])).ravel()[0])\n","        rows.append((pd.to_datetime(d_label), close[t_label], yhat_level, close[t]))\n","    pred_df = pd.DataFrame(rows, columns=[\"date\",\"y_true\",\"y_hat\",\"y_last\"]).sort_values(\"date\").reset_index(drop=True)\n","    pred_df[\"residual\"] = pred_df[\"y_true\"] - pred_df[\"y_hat\"]\n","    pred_df[\"in_sample_flag\"] = 0\n","    return pred_df"],"metadata":{"id":"gQ_L6hH0kmdR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Outputs and Artefacts\n","\n","What: Emit predictions/metrics/run_config/training curves per ticker; write run-root manifests; package bundles.\n","Why: Produces reproducible artefacts for dissertation and audit without behaviour changes.\n","Method choices: Metrics on inverse-transformed level values; DA uses ε = 0.0010; trading diagnostics use pure-sign rule with costs on changes; deep ablation cadence is frozen weights (no monthly refit; expanding-origin applies to ARIMA/ARIMAX only)."],"metadata":{"id":"dLhb8Wk-k7KQ"}},{"cell_type":"code","source":["def emit_run_config(model_id: str, ticker: str, out_dir: Path, features: List[str],\n","                    param_count: int, label: Optional[str] = None) -> Path:\n","    cfg = {\n","        \"model_id\": model_id,\n","        \"model_label\": label if label else model_id,\n","        \"ticker\": ticker,\n","        \"features_used\": features,\n","        \"parameter_count\": int(param_count),\n","        \"seeds\": {\"global\": SEED},\n","        \"cadence\": \"frozen_weights\",\n","        \"window_length\": LOOKBACK_L,\n","        \"horizon\": HORIZON,\n","        \"batch_size\": BATCH_TRAIN,\n","        \"epochs_max\": MAX_EPOCHS,\n","        \"timezone\": \"America/New_York\",\n","        \"market_close\": \"16:00\",\n","        \"policy\": {\n","            \"x_scaler\": \"train_only_z\",\n","            \"y_scaled\": True,\n","            \"inverse_transform_metrics\": True\n","        },\n","        \"target_definition\": \"Close.shift(-1) created after all features\",\n","        \"target_col\": \"Close\",\n","        \"splits\": {\n","            \"train\": [TRAIN_START, TRAIN_END],\n","            \"val\":   [VAL_START, VAL_END],\n","            \"test\":  [TEST_START, TEST_END]\n","        }\n","    }\n","    p = out_dir / f\"run_config_{model_id}_{ticker}.json\"\n","    p.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n","    return p\n","\n","def training_curves_png(history: TrainHistory, out_path: Path, ticker: str, model_id: str):\n","    fig = plt.figure(figsize=(7,4))\n","    plt.plot(history.train_loss, label=\"train_loss\")\n","    plt.plot(history.val_loss,   label=\"val_loss\")\n","    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n","    plt.title(f\"{model_id} {ticker} (best_epoch={history.best_epoch})\")\n","    plt.legend(); fig.tight_layout(); fig.savefig(out_path, dpi=160); plt.close(fig)\n","\n","def process_ticker(model_id: str, ticker: str, model_dir: Path, features: List[str],\n","                   model_label: Optional[str] = None) -> Dict[str, object]:\n","    set_all_seeds(SEED)\n","    df = load_panel(ticker)\n","    assert_test_days(df)\n","    x_scaler = fit_x_scaler_train_only(df, features)\n","    y_scaler = fit_y_scaler_train_only(df, \"Close\")\n","    X_tr, y_tr, _ = build_sequences(df, x_scaler, y_scaler, features, \"Close\",\n","                                    TRAIN_START, TRAIN_END, LOOKBACK_L, HORIZON, scale_y=True)\n","    X_va, y_va, _ = build_sequences(df, x_scaler, y_scaler, features, \"Close\",\n","                                    VAL_START, VAL_END, LOOKBACK_L, HORIZON, scale_y=True)\n","    train_loader = DataLoader(SeqDataset(X_tr, y_tr), batch_size=BATCH_TRAIN, shuffle=False, drop_last=False)\n","    val_loader   = DataLoader(SeqDataset(X_va, y_va), batch_size=BATCH_VAL,   shuffle=False, drop_last=False)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = LSTMPrice(input_size=len(features), hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS,\n","                      dropout=DROPOUT, bidirectional=BIDIR).to(device)\n","    n_params = count_params(model)\n","    hist = train_with_early_stopping(model, train_loader, val_loader,\n","                                     max_epochs=MAX_EPOCHS, lr=LR, patience=PATIENCE,\n","                                     grad_clip=GRAD_CLIP, device=device)\n","    preds_df = walkforward_predict(df, x_scaler, y_scaler, model, features, LOOKBACK_L)\n","    assert len(preds_df) == TEST_DAYS_EXPECTED, f\"{ticker}: predictions rows != {TEST_DAYS_EXPECTED}\"\n","    y_true = preds_df[\"y_true\"].to_numpy()\n","    y_hat  = preds_df[\"y_hat\"].to_numpy()\n","    y_last = preds_df[\"y_last\"].to_numpy()\n","    residual = y_true - y_hat\n","    m_rmse = rmse(y_true, y_hat)\n","    m_mae  = mae(y_true, y_hat)\n","    m_u2   = theils_u2(y_true, y_hat, y_last)\n","    da     = directional_accuracy(y_true, y_hat, y_last, epsilon_ret=EPSILON_RET)\n","    strat0, changed0  = direction_only_returns(y_true, y_hat, y_last, EPSILON_RET, 0.0)\n","    strat10, changed10 = direction_only_returns(y_true, y_hat, y_last, EPSILON_RET, 10.0)\n","    s0, mdd0   = sharpe_and_maxdd(strat0)\n","    s10, mdd10 = sharpe_and_maxdd(strat10)\n","    turnover = int(np.sum(changed0))\n","    tdir = model_dir / ticker\n","    tdir.mkdir(parents=True, exist_ok=True)\n","    pred_cols = [\"date\",\"y_true\",\"y_hat\",\"residual\",\"in_sample_flag\"]\n","    preds_out = preds_df[pred_cols].copy()\n","    preds_out[\"in_sample_flag\"] = preds_out[\"in_sample_flag\"].astype(int)\n","    pred_path = tdir / f\"predictions_{model_id}_{ticker}.csv\"\n","    preds_out.to_csv(pred_path, index=False)\n","    metrics = {\n","        \"RMSE\": float(m_rmse),\n","        \"MAE\": float(m_mae),\n","        \"U2\": float(m_u2),\n","        \"DA_epsilon\": float(da[\"DA\"]),\n","        \"Coverage\": float(da[\"coverage\"]),\n","        \"n\": int(TEST_DAYS_EXPECTED),\n","        \"Sharpe_0bps\": float(s0),\n","        \"Sharpe_10bps\": float(s10),\n","        \"MaxDD_0bps\": float(mdd0),\n","        \"MaxDD_10bps\": float(mdd10),\n","        \"Turnover\": turnover,\n","        \"ResidualMean\": float(np.mean(residual)),\n","        \"ResidualMeanAbs\": float(np.mean(np.abs(residual))),\n","        \"ResidualStd\": float(np.std(residual, ddof=1)),\n","        \"trading_rule\": \"pure_sign\",\n","        \"tie_policy\": \"zero\",\n","        \"return_type\": \"pct\",\n","        \"epsilon_DA\": EPSILON_RET,\n","        \"cost_bps\": [0, 10]\n","    }\n","    metrics_path = tdir / f\"metrics_{model_id}_{ticker}.json\"\n","    metrics_path.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n","    curves_path = tdir / f\"training_curves_{model_id}_{ticker}.png\"\n","    training_curves_png(hist, curves_path, ticker, model_id)\n","    run_cfg_path = emit_run_config(model_id, ticker, tdir, features, n_params, label=model_label)\n","    fig = plt.figure(figsize=(9,5))\n","    plt.plot(preds_df[\"date\"], preds_df[\"y_true\"], label=\"actual\")\n","    plt.plot(preds_df[\"date\"], preds_df[\"y_hat\"],  label=\"predicted\")\n","    plt.legend(); plt.grid(True, linestyle=\":\")\n","    fig.tight_layout(); fig.savefig(tdir / f\"actual_vs_pred_{model_id}_{ticker}.png\", dpi=140); plt.close(fig)\n","    return {\n","        \"predictions_csv\": str(pred_path),\n","        \"metrics_json\": str(metrics_path),\n","        \"run_config_json\": str(run_cfg_path),\n","        \"training_curves_png\": str(curves_path)\n","    }\n","\n","def run_suite(model_id: str, features: List[str], run_root_name: str, bundle_name: str,\n","              model_label: Optional[str] = None):\n","    set_all_seeds(SEED)\n","    RUN_ROOT = Path(run_root_name)\n","    MODEL_DIR = RUN_ROOT / model_id\n","    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n","    for t in TICKERS:\n","        print(f\"=== {t} ({features}) -> {model_id} ===\")\n","        arte = process_ticker(model_id, t, MODEL_DIR, features, model_label=model_label)\n","        for key in [\"predictions_csv\",\"metrics_json\",\"run_config_json\",\"training_curves_png\"]:\n","            p = Path(arte[key]); assert p.exists() and p.stat().st_size > 0, f\"Missing: {p}\"\n","    write_methods_note(RUN_ROOT, model_id, features, cadence=\"frozen_weights\", label=model_label)\n","    write_run_root_manifests(RUN_ROOT, model_id)\n","    zip_out(RUN_ROOT, bundle_name)\n","    downloaded = False\n","    try:\n","        from google.colab import files\n","        files.download(bundle_name)\n","        downloaded = True\n","    except Exception:\n","        pass\n","    if not downloaded:\n","        try:\n","            from IPython.display import FileLink, display\n","            display(FileLink(bundle_name))\n","        except Exception:\n","            print(f\"[saved] {Path(bundle_name).resolve()}\")\n","\n","if __name__ == \"__main__\":\n","    run_suite(\n","        model_id=\"LSTM_PO\",\n","        features=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],\n","        run_root_name=\"LSTM_PO_FINAL_OHLCV\",\n","        bundle_name=\"LSTM_PO_FINAL_OHLCV_bundle.zip\",\n","        model_label=\"LSTM_PO (price-only OHLCV)\"\n","    )\n","    run_suite(\n","        model_id=\"LSTM_PO_CloseOnly\",\n","        features=[\"Close\"],\n","        run_root_name=\"LSTM_PO_CloseOnly_ABL\",\n","        bundle_name=\"LSTM_PO_CloseOnly_ablation_bundle.zip\",\n","        model_label=\"LSTM_PO_CloseOnly (ablation)\"\n","    )"],"metadata":{"id":"u57s6Juck_a0","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1759070331702,"user_tz":-60,"elapsed":679756,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"7df0daf0-9694-4e3b-fb1c-1f415a4bc492"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== AAPL (['Open', 'High', 'Low', 'Close', 'Volume']) -> LSTM_PO ===\n","=== AMZN (['Open', 'High', 'Low', 'Close', 'Volume']) -> LSTM_PO ===\n","=== MSFT (['Open', 'High', 'Low', 'Close', 'Volume']) -> LSTM_PO ===\n","=== TSLA (['Open', 'High', 'Low', 'Close', 'Volume']) -> LSTM_PO ===\n","=== AMD (['Open', 'High', 'Low', 'Close', 'Volume']) -> LSTM_PO ===\n","[zip] /content/LSTM_PO_FINAL_OHLCV_bundle.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1e02979c-6bf1-48f0-ad61-db367c21e9ea\", \"LSTM_PO_FINAL_OHLCV_bundle.zip\", 782509)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== AAPL (['Close']) -> LSTM_PO_CloseOnly ===\n","=== AMZN (['Close']) -> LSTM_PO_CloseOnly ===\n","=== MSFT (['Close']) -> LSTM_PO_CloseOnly ===\n","=== TSLA (['Close']) -> LSTM_PO_CloseOnly ===\n","=== AMD (['Close']) -> LSTM_PO_CloseOnly ===\n","[zip] /content/LSTM_PO_CloseOnly_ablation_bundle.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_91ef799e-e1ac-436a-95ab-28377cadc218\", \"LSTM_PO_CloseOnly_ablation_bundle.zip\", 800594)"]},"metadata":{}}]}]}