{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNilL3x2xqbBR+tUd+skX/G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##01. Imports and Configuration\n","\n","This block imports all dependencies, sets deterministic seeds and cuDNN flags, configures device selection, and defines fixed splits and hyperparameters. It is required to ensure reproducibility and parity: train-only scaling, fixed splits (2021-02-03→2022-12-30, 2023-01-03→2023-05-31, 2023-06-01→2023-12-28), Target = Close.shift(-1) upstream, America/New_York 16:00 cut-off assumed upstream, early stopping on Validation, and monthly refit on Test."],"metadata":{"id":"tMZ14KkKsG9H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-a52dmBIpomP"},"outputs":[],"source":["import os, sys, json, time, math, hashlib, random, zipfile, shutil, platform, warnings\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","# ====================== Config ======================\n","\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","DATA_DIR = Path(\".\")\n","RUN_ROOT = Path(\"transformer_outputs\") / \"TRANSFORMER\"\n","RUN_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","\n","# Window length (parity with LSTM/Hybrid)\n","L = 90\n","\n","# Optimisation\n","BATCH_SIZE = 64\n","MAX_EPOCHS = 200\n","ES_PATIENCE = 20\n","LR = 1e-3\n","WEIGHT_DECAY = 0.0\n","GRAD_CLIP = 1.0\n","\n","# Transformer\n","D_MODEL = 64\n","NHEAD = 4\n","DIM_FF = 128\n","DEPTH = 2\n","DROPOUT = 0.1\n","\n","# Metrics policy\n","EPSILON_DA = 0.0010            # for DA epsilon-gating\n","TRADING_COSTS = [0.0, 0.0010]  # 0 bps, 10 bps (decimal return terms)\n","\n","# Feature families\n","SENTIMENT_PREFIXES = (\"Tw_\", \"Rd_\", \"Nw_SP500_\")"]},{"cell_type":"markdown","source":["## Outputs and Artefacts (helpers)\n","\n","This block provides helper functions for provenance capture and packaging. It writes env_manifest.txt and file_hashes.json at the run root, and builds a zip for submission. These are required to ensure reproducibility and verifiable artefacts."],"metadata":{"id":"9Os1eQBgsKC_"}},{"cell_type":"code","source":["# ====================== IO helpers ======================\n","\n","def sha256_file(p: Path) -> str:\n","    h = hashlib.sha256()\n","    with p.open(\"rb\") as fh:\n","        for chunk in iter(lambda: fh.read(1 << 20), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","def write_env_manifest(root: Path):\n","    f = root / \"env_manifest.txt\"\n","    lines = [\n","        f\"timestamp_utc={time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())}\",\n","        f\"python={sys.version.split()[0]}\",\n","        f\"platform={platform.platform()}\",\n","        f\"torch={getattr(torch, '__version__', 'unknown')}\",\n","        f\"numpy={getattr(np, '__version__', 'unknown')}\",\n","        f\"pandas={getattr(pd, '__version__', 'unknown')}\",\n","    ]\n","    f.write_text(\"\\n\".join(lines))\n","\n","def write_file_hashes(root: Path):\n","    mapping = {}\n","    for p in root.rglob(\"*\"):\n","        if p.is_file():\n","            rel = p.relative_to(root).as_posix()\n","            try:\n","                mapping[rel] = {\"sha256\": sha256_file(p), \"size\": p.stat().st_size, \"mtime\": int(p.stat().st_mtime)}\n","            except Exception:\n","                mapping[rel] = {\"sha256\": None, \"size\": None, \"mtime\": None}\n","    (root / \"file_hashes.json\").write_text(json.dumps(mapping, indent=2))\n","\n","def package_zip(run_root: Path) -> Path:\n","    out_dir = Path(\"outputs\") / \"transformer\"\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    base = out_dir / \"transformer\"\n","    if base.with_suffix(\".zip\").exists():\n","        base.with_suffix(\".zip\").unlink()\n","    shutil.make_archive(str(base), \"zip\", root_dir=str(run_root.parent), base_dir=run_root.name)\n","    (out_dir / \"transformer.sha256\").write_text(f\"{sha256_file(base.with_suffix('.zip'))}  transformer.zip\")\n","    return base.with_suffix(\".zip\")\n","\n","def maybe_download_zip(zip_path: Path):\n","    try:\n","        from google.colab import files  # type: ignore\n","        files.download(str(zip_path))\n","    except Exception:\n","        print(f\"Archive ready: {zip_path}  (sha256: {sha256_file(zip_path)[:12]})\")"],"metadata":{"id":"FvqRlOs0sMWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Loading\n","\n","This block loads each ticker’s input CSV, ensures chronological order, and selects feature columns. Assumes upstream created Target = Close.shift(-1) after all features and aligned to America/New_York 16:00 cut-off."],"metadata":{"id":"QcxPsayxsNsI"}},{"cell_type":"code","source":["# ====================== Data & scaling ======================\n","\n","def read_ticker_df(ticker: str) -> pd.DataFrame:\n","    p = DATA_DIR / f\"{ticker}_input.csv\"\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing input CSV for {ticker}: {p}\")\n","    df = pd.read_csv(p)\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    return df\n","\n","def feature_columns(df: pd.DataFrame) -> List[str]:\n","    return [c for c in df.columns if c not in [\"date\",\"ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Target\"]]"],"metadata":{"id":"DcXxMTApsQzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing\n","\n","This block defines the train-only zero-preserving scaler for features. It preserves exact zeros for Tw_/Rd_/Nw_SP500_* and does not scale binary indicators. Train statistics are computed once and reused for Validation and Test."],"metadata":{"id":"J-wCM-LVsT7H"}},{"cell_type":"code","source":["class ZeroPreservingScaler:\n","    def __init__(self, cols: List[str], sentiment_cols: List[str], binary_cols: List[str]):\n","        self.cols = cols\n","        self.sentiment_cols = sentiment_cols\n","        self.binary_cols = binary_cols\n","        self.mu = {}\n","        self.sigma = {}\n","    def fit(self, df: pd.DataFrame):\n","        for c in self.cols:\n","            if c in self.binary_cols:\n","                continue\n","            x = df[c].values.astype(float)\n","            if c in self.sentiment_cols:\n","                mask = x != 0.0\n","                if mask.any():\n","                    v = x[mask]\n","                    self.mu[c] = float(np.mean(v))\n","                    s = float(np.std(v, ddof=0))\n","                    self.sigma[c] = float(s if s > 1e-12 else 1.0)\n","                else:\n","                    self.mu[c] = 0.0\n","                    self.sigma[c] = 1.0\n","            else:\n","                self.mu[c] = float(np.mean(x))\n","                s = float(np.std(x, ddof=0))\n","                self.sigma[c] = float(s if s > 1e-12 else 1.0)\n","        return self\n","    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n","        out = df.copy()\n","        for c in self.cols:\n","            if c in self.binary_cols:\n","                continue\n","            x = out[c].values.astype(float)\n","            if c in self.sentiment_cols:\n","                mask = x != 0.0\n","                if mask.any():\n","                    x_scaled = (x[mask] - self.mu[c]) / self.sigma[c]\n","                    x[mask] = x_scaled\n","                x[~mask] = 0.0\n","            else:\n","                x = (x - self.mu[c]) / self.sigma[c]\n","            out[c] = x\n","        return out\n","\n","def detect_binary_columns(df: pd.DataFrame, cols: List[str]) -> List[str]:\n","    bins = []\n","    for c in cols:\n","        u = pd.unique(df[c].dropna().astype(float))\n","        if len(u) <= 2 and set(u).issubset({0.0,1.0}):\n","            bins.append(c)\n","    return bins\n","\n","def detect_sentiment_columns(cols: List[str]) -> List[str]:\n","    return [c for c in cols if c.startswith(SENTIMENT_PREFIXES)]"],"metadata":{"id":"5T1qgHOUsWpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Definition\n","\n","This block defines the sequence dataset, sinusoidal positional encoding, Transformer encoder regressor, and a parameter counter. The model uses a last-timestep head for next-day prediction, matching the L=90 causal window."],"metadata":{"id":"94z4znGKsZTZ"}},{"cell_type":"code","source":["# ====================== Sequences ======================\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, df: pd.DataFrame, feat_cols: List[str], L: int):\n","        self.X = df[feat_cols].values.astype(np.float32)\n","        self.y = df[\"Target\"].values.astype(np.float32)\n","        self.dates = df[\"date\"].values\n","        self.L = L\n","        self.n = len(df)\n","        self.indices = [(i-L, i) for i in range(L, self.n)]\n","    def __len__(self):\n","        return len(self.indices)\n","    def __getitem__(self, idx):\n","        a,b = self.indices[idx]\n","        x = self.X[a:b]\n","        y = self.y[b-1]\n","        return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n","\n","# ====================== Model ======================\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, max_len: int = 1000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer(\"pe\", pe.unsqueeze(1))\n","    def forward(self, x):\n","        n, b, d = x.size()\n","        return x + self.pe[:n, :b, :d]\n","\n","class TransformerRegressor(nn.Module):\n","    def __init__(self, d_in: int, d_model: int, nhead: int, dim_ff: int, depth: int, dropout: float):\n","        super().__init__()\n","        self.input_proj = nn.Linear(d_in, d_model)\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n","            dropout=dropout, batch_first=False, activation=\"gelu\")\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=depth)\n","        self.posenc = PositionalEncoding(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.head = nn.Sequential(\n","            nn.Linear(d_model, d_model // 2), nn.ReLU(), nn.Dropout(dropout),\n","            nn.Linear(d_model // 2, 1)\n","        )\n","    def forward(self, x):\n","        x = self.input_proj(x)        # (B, L, d_model)\n","        x = x.transpose(0,1)          # (L, B, d_model)\n","        x = self.posenc(x)\n","        x = self.encoder(x)           # (L, B, d_model)\n","        x = x[-1]                     # (B, d_model)\n","        x = self.dropout(x)\n","        y = self.head(x).squeeze(-1)  # (B,)\n","        return y\n","\n","def parameter_count(model: nn.Module) -> int:\n","    return sum(p.numel() for p in model.parameters())"],"metadata":{"id":"vgK8PufFscIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","\n","This block builds deterministic data loaders and performs training with early stopping on Validation. It saves per-refit training curves. No behaviour changes are introduced."],"metadata":{"id":"mustt2Fwsd9c"}},{"cell_type":"code","source":["# ====================== Training utilities ======================\n","\n","def build_loaders(df_train, df_val, feat_cols, L, batch_size, device) -> Tuple[DataLoader, DataLoader]:\n","    ds_tr = SeqDataset(df_train, feat_cols, L)\n","    ds_va = SeqDataset(df_val,   feat_cols, L)\n","    pin = (device.type == \"cuda\")\n","    tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=pin, num_workers=0)\n","    va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=pin, num_workers=0)\n","    return tr, va\n","\n","def train_one(model, train_loader, val_loader, max_epochs, es_patience, lr, weight_decay, grad_clip, device, curves_path) -> int:\n","    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    crit = nn.MSELoss()\n","    best_loss = float(\"inf\"); best_state = None\n","    patience = es_patience\n","    tr_hist, va_hist = [], []\n","    epoch_ran = 0\n","    for epoch in range(1, max_epochs+1):\n","        model.train()\n","        tr_loss = 0.0; n_tr = 0\n","        for xb, yb in train_loader:\n","            xb = xb.to(device); yb = yb.to(device)\n","            opt.zero_grad()\n","            yhat = model(xb)\n","            loss = crit(yhat, yb)\n","            loss.backward()\n","            if grad_clip is not None:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","            opt.step()\n","            tr_loss += float(loss.detach().cpu()) * len(xb)\n","            n_tr += len(xb)\n","        tr_loss /= max(1, n_tr)\n","\n","        model.eval()\n","        va_loss = 0.0; n_va = 0\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb = xb.to(device); yb = yb.to(device)\n","                yhat = model(xb)\n","                loss = crit(yhat, yb)\n","                va_loss += float(loss.detach().cpu()) * len(xb)\n","                n_va += len(xb)\n","        va_loss /= max(1, n_va)\n","\n","        tr_hist.append(tr_loss); va_hist.append(va_loss)\n","        if va_loss + 1e-12 < best_loss:\n","            best_loss = va_loss\n","            best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n","            patience = es_patience\n","        else:\n","            patience -= 1\n","        epoch_ran = epoch\n","        if patience == 0:\n","            break\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    plt.figure()\n","    plt.plot(range(1, len(tr_hist)+1), tr_hist, label=\"train\")\n","    plt.plot(range(1, len(va_hist)+1), va_hist, label=\"val\")\n","    plt.xlabel(\"epoch\"); plt.ylabel(\"MSE\"); plt.legend(); plt.tight_layout()\n","    plt.savefig(curves_path); plt.close()\n","    return epoch_ran"],"metadata":{"id":"VC_dcvXYsfkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation\n","\n","This block computes Test metrics: RMSE, MAE, Theil’s U2 vs naïve last-close, DA with ε-gating and Coverage, and trading metrics using the pure-sign rule at 0 and 10 bps with Turnover."],"metadata":{"id":"zh8VvKyAsgRz"}},{"cell_type":"code","source":["# ====================== Metrics (pure-sign trading; DA ε-gated) ======================\n","\n","def naive_last_close(y_true: np.ndarray) -> np.ndarray:\n","    y_naive = np.empty_like(y_true)\n","    y_naive[0] = y_true[0]\n","    y_naive[1:] = y_true[:-1]\n","    return y_naive\n","\n","def theils_u2(y_true: np.ndarray, y_hat: np.ndarray, y_naive: np.ndarray) -> float:\n","    num = np.sqrt(np.mean((y_hat - y_true)**2))\n","    den = np.sqrt(np.mean((y_naive - y_true)**2))\n","    if den == 0.0: return 0.0\n","    return float(num / den)\n","\n","def classes_from_returns(r: np.ndarray, eps: float) -> np.ndarray:\n","    cls = np.zeros_like(r)\n","    cls[r >= eps] = 1.0\n","    cls[r <= -eps] = -1.0\n","    return cls\n","\n","def directional_accuracy_eps(y_true: np.ndarray, y_hat: np.ndarray, eps: float) -> Tuple[float, float, int]:\n","    # Predicted & realised returns for t->t+1 (uses y_t in denominator for both)\n","    r_true = (y_true[1:] / y_true[:-1]) - 1.0\n","    r_hat  = (y_hat[1:]  / y_true[:-1]) - 1.0\n","    cls_true = classes_from_returns(r_true, eps)\n","    cls_hat  = classes_from_returns(r_hat,  eps)\n","    mask = cls_true != 0.0\n","    n = int(mask.sum())\n","    if n == 0:\n","        return 0.0, 0.0, 0\n","    da = float(np.mean((cls_hat[mask] == cls_true[mask]).astype(float)))\n","    coverage = float(mask.mean())\n","    return da, coverage, n\n","\n","def trading_metrics_puresign(y_true: np.ndarray, y_hat: np.ndarray, cost: float) -> Tuple[float, float, int]:\n","    \"\"\"\n","    Pure-sign rule:\n","      position_t = sign(ŷ_{t+1} - y_t) with ties -> 0\n","      ret_{t+1}  = (y_{t+1}/y_t) - 1\n","      strategy   = position_t * ret_{t+1}\n","      costs only when position changes; turnover = count of changes\n","    \"\"\"\n","    r_true = (y_true[1:] / y_true[:-1]) - 1.0\n","    pos = np.sign(y_hat[1:] - y_true[:-1])\n","    pos[np.isnan(pos)] = 0.0\n","    ties = (y_hat[1:] == y_true[:-1])\n","    pos[ties] = 0.0\n","    prev = 0.0\n","    strat = np.empty_like(r_true)\n","    turnover = 0\n","    for i in range(len(r_true)):\n","        if pos[i] != prev:\n","            turnover += 1\n","            strat[i] = pos[i] * r_true[i] - cost\n","        else:\n","            strat[i] = pos[i] * r_true[i]\n","        prev = pos[i]\n","    mean = float(np.mean(strat)) if len(strat) else 0.0\n","    std  = float(np.std(strat, ddof=0)) if len(strat) else 0.0\n","    sharpe = float(mean / std) if std > 1e-12 else 0.0\n","    cum = np.cumsum(strat)\n","    peak = np.maximum.accumulate(cum)\n","    drawdown = peak - cum\n","    maxdd = float(np.max(drawdown)) if len(drawdown) else 0.0\n","    return sharpe, maxdd, int(turnover)\n","\n","def compute_metrics(y_true: np.ndarray, y_hat: np.ndarray) -> Dict[str, float]:\n","    y_naive = naive_last_close(y_true)\n","    rmse = float(np.sqrt(np.mean((y_hat - y_true)**2)))\n","    mae = float(np.mean(np.abs(y_hat - y_true)))\n","    u2 = theils_u2(y_true, y_hat, y_naive)\n","    da, cov, n = directional_accuracy_eps(y_true, y_hat, EPSILON_DA)\n","    sharpe0, mdd0, to0   = trading_metrics_puresign(y_true, y_hat, TRADING_COSTS[0])\n","    sharpe10, mdd10, _to = trading_metrics_puresign(y_true, y_hat, TRADING_COSTS[1])\n","    return {\n","        \"RMSE\": rmse,\n","        \"MAE\": mae,\n","        \"U2\": u2,\n","        \"DA_epsilon\": float(da),\n","        \"Coverage\": float(cov),\n","        \"n\": int(n),\n","        \"Sharpe_0bps\": float(sharpe0),\n","        \"MaxDD_0bps\": float(mdd0),\n","        \"Sharpe_10bps\": float(sharpe10),\n","        \"MaxDD_10bps\": float(mdd10),\n","        \"Turnover\": int(to0)\n","    }"],"metadata":{"id":"o8y5HOmNsjoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Outputs and Artefacts (walk-forward and packaging)\n","\n","This block implements daily expanding-origin inference with a monthly refit cadence, enforces Test window length n=146, writes per-ticker predictions, metrics, run_config, and a canonical training curves file; then writes run-root provenance files and packages a zip."],"metadata":{"id":"n9Vvc3nCskg_"}},{"cell_type":"code","source":["# ====================== Daily walk-forward with monthly refits ======================\n","\n","def first_trading_day_each_month(dates: pd.Series) -> List[pd.Timestamp]:\n","    df = pd.DataFrame({\"date\": pd.to_datetime(dates)}).copy()\n","    df[\"ym\"] = df[\"date\"].dt.to_period(\"M\")\n","    out = df.groupby(\"ym\")[\"date\"].min().tolist()\n","    return [pd.Timestamp(d) for d in out]\n","\n","def ensure_146(df_pred: pd.DataFrame):\n","    df_pred = df_pred[(df_pred[\"date\"] >= pd.Timestamp(TEST_START)) & (df_pred[\"date\"] <= pd.Timestamp(TEST_END))].copy()\n","    dates = pd.to_datetime(df_pred[\"date\"].unique())\n","    if len(dates) != 146 or dates.min() != pd.Timestamp(TEST_START) or dates.max() != pd.Timestamp(TEST_END):\n","        raise AssertionError(f\"Test window invalid. Expected 146 rows from {TEST_START} to {TEST_END}, got {len(dates)} rows from {dates.min()} to {dates.max()}.\")\n","    return df_pred\n","\n","def train_model_for_month(df_all_scaled: pd.DataFrame, refit_start: pd.Timestamp,\n","                          feat_cols: List[str], base_out_dir: Path, ticker: str) -> Tuple[Optional[nn.Module], int]:\n","    # Train/Val splits are fixed; history excludes current refit_start\n","    train_df = df_all_scaled[(df_all_scaled[\"date\"] >= TRAIN_START) &\n","                             (df_all_scaled[\"date\"] <= TRAIN_END) &\n","                             (df_all_scaled[\"date\"] < refit_start)].copy()\n","    val_df   = df_all_scaled[(df_all_scaled[\"date\"] >= VAL_START) &\n","                             (df_all_scaled[\"date\"] <= VAL_END) &\n","                             (df_all_scaled[\"date\"] < refit_start)].copy()\n","    if len(train_df) < L + 5 or len(val_df) < L + 5:\n","        return None, 0\n","    tr_loader, va_loader = build_loaders(train_df, val_df, feat_cols, L, BATCH_SIZE, DEVICE)\n","    model = TransformerRegressor(d_in=len(feat_cols), d_model=D_MODEL, nhead=NHEAD,\n","                                 dim_ff=DIM_FF, depth=DEPTH, dropout=DROPOUT).to(DEVICE)\n","    curves_path = base_out_dir / f\"training_curves_{ticker}_{refit_start.strftime('%Y-%m-%d')}.png\"\n","    epochs = train_one(model, tr_loader, va_loader, MAX_EPOCHS, ES_PATIENCE, LR, WEIGHT_DECAY, GRAD_CLIP, DEVICE, curves_path)\n","    return model, int(epochs)\n","\n","def daily_walk_forward(df_all_scaled: pd.DataFrame, feat_cols: List[str], L: int,\n","                       base_out_dir: Path, ticker: str) -> Tuple[pd.DataFrame, Dict[str,int], List[str]]:\n","    # Full Test date list, sorted\n","    test_df = df_all_scaled[(df_all_scaled[\"date\"] >= TEST_START) & (df_all_scaled[\"date\"] <= TEST_END)].copy()\n","    test_dates = pd.to_datetime(test_df[\"date\"]).drop_duplicates().sort_values().tolist()\n","    if len(test_dates) == 0:\n","        return pd.DataFrame(columns=[\"date\",\"y_true\",\"y_hat\"]), {}, []\n","    # Monthly refit anchors (first trading day in each Test month)\n","    refit_days = first_trading_day_each_month(pd.Series(test_dates))\n","    epochs_per_refit: Dict[str,int] = {}\n","    used_refits_iso: List[str] = []\n","    preds: List[Tuple[pd.Timestamp, float, float]] = []\n","\n","    for refit_start in refit_days:\n","        # Train model for this month on train-only scaled data\n","        model, epochs = train_model_for_month(df_all_scaled, refit_start, feat_cols, base_out_dir, ticker)\n","        if model is None:\n","            continue\n","        refit_iso = refit_start.strftime(\"%Y-%m-%d\")\n","        epochs_per_refit[refit_iso] = epochs\n","        used_refits_iso.append(refit_iso)\n","        model.eval()\n","\n","        # Predict each trading day d in this calendar month within the Test window\n","        month_end = (refit_start + pd.offsets.MonthEnd(0))\n","        month_dates = df_all_scaled.loc[\n","            (df_all_scaled[\"date\"] >= refit_start) &\n","            (df_all_scaled[\"date\"] <= month_end) &\n","            (df_all_scaled[\"date\"] >= TEST_START) &\n","            (df_all_scaled[\"date\"] <= TEST_END),\n","            \"date\"\n","        ].drop_duplicates().sort_values().tolist()\n","\n","        with torch.no_grad():\n","            for d in month_dates:\n","                window_df = df_all_scaled.loc[df_all_scaled[\"date\"] <= d].tail(L).copy()\n","                if len(window_df) < L:\n","                    continue\n","                x = torch.from_numpy(window_df[feat_cols].values.astype(np.float32)).unsqueeze(0)  # (1, L, F)\n","                y_hat = model(x.to(DEVICE)).cpu().numpy().ravel()[0]\n","                y_true = float(df_all_scaled.loc[df_all_scaled[\"date\"] == d, \"Target\"].iloc[0])\n","                preds.append((pd.Timestamp(d), y_true, float(y_hat)))\n","\n","    out = pd.DataFrame(preds, columns=[\"date\",\"y_true\",\"y_hat\"]).drop_duplicates(subset=[\"date\"]).sort_values(\"date\")\n","    return out, epochs_per_refit, used_refits_iso"],"metadata":{"id":"DCZYbXtasmSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Outputs and Artefacts (runner)\n","\n","This final block executes the per-ticker run: applies train-only scaling, performs monthly refits, writes artefacts with canonical filenames, records run_config with optimiser and learning-rate schedule fields, writes manifests, and packages the zip. Expanding-origin daily inference within each month is retained."],"metadata":{"id":"5W1W90Wlso-0"}},{"cell_type":"code","source":["# ====================== Runner (finalisation-ready) ======================\n","import os, sys, json, time, shutil, zipfile, hashlib, platform\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","# --------- paths ---------\n","# Bundle root holds provenance; model folder holds per-ticker outputs\n","BUNDLE_ROOT = Path(\"TRANSFORMER_FINAL\")              # <— top-level bundle directory\n","RUN_ROOT    = BUNDLE_ROOT / \"TRANSFORMER\"            # <— model directory with per-ticker subfolders\n","RUN_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","# --------- provenance helpers ---------\n","def sha256_file(p: Path, chunk_size: int = 1024 * 1024) -> str:\n","    h = hashlib.sha256()\n","    with p.open(\"rb\") as f:\n","        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","def write_env_manifest(root: Path) -> Path:\n","    lines = []\n","    lines.append(f\"timestamp_utc: {time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())}\")\n","    lines.append(f\"os: {platform.platform()}\")\n","    lines.append(f\"python: {sys.version.split()[0]}\")\n","    # Best-effort library versions (present if installed)\n","    def ver(modname):\n","        try:\n","            mod = __import__(modname)\n","            return getattr(mod, \"__version__\", \"unknown\")\n","        except Exception:\n","            return \"not_installed\"\n","    lines.append(f\"numpy: {ver('numpy')}\")\n","    lines.append(f\"pandas: {ver('pandas')}\")\n","    lines.append(f\"torch: {ver('torch')}\")\n","    lines.append(f\"statsmodels: {ver('statsmodels')}\")\n","    out = root / \"env_manifest.txt\"\n","    out.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n","    return out\n","\n","def write_file_hashes(root: Path) -> Path:\n","    records = []\n","    for p in sorted(root.rglob(\"*\")):\n","        if p.is_file():\n","            rel = p.relative_to(root).as_posix()\n","            # Skip file_hashes.json itself while computing hashes\n","            if rel == \"file_hashes.json\":\n","                continue\n","            records.append({\"path\": rel, \"sha256\": sha256_file(p)})\n","    out = root / \"file_hashes.json\"\n","    out.write_text(json.dumps({\"files\": records}, indent=2), encoding=\"utf-8\")\n","    return out\n","\n","def package_zip(root: Path) -> Path:\n","    # Zip the BUNDLE_ROOT so env_manifest.txt and file_hashes.json sit at archive top level\n","    zip_path = root.with_suffix(\"\")  # remove extension if any\n","    zip_path = zip_path.parent / f\"{root.name}_bundle.zip\"\n","    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as z:\n","        for p in sorted(root.rglob(\"*\")):\n","            if p.is_file():\n","                z.write(p, p.relative_to(root))\n","    return zip_path\n","\n","# --------- your existing dependencies assumed to be defined elsewhere ---------\n","# read_ticker_df, feature_columns, detect_binary_columns, detect_sentiment_columns,\n","# ZeroPreservingScaler, daily_walk_forward, ensure_146, compute_metrics,\n","# TransformerRegressor, parameter_count, and constants:\n","# TICKERS, TRAIN_START, TRAIN_END, TEST_START, TEST_END,\n","# L, D_MODEL, NHEAD, DIM_FF, DEPTH, DROPOUT, BATCH_SIZE, MAX_EPOCHS,\n","# ES_PATIENCE, LR, WEIGHT_DECAY, GRAD_CLIP, RANDOM_SEED\n","\n","def run_for_ticker(ticker: str):\n","    out_dir = RUN_ROOT / ticker\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    df = read_ticker_df(ticker)\n","    feat_cols = feature_columns(df)\n","\n","    # ----- Train-only scaling fit (invariant) -----\n","    df_train_only = df[(df[\"date\"] >= TRAIN_START) & (df[\"date\"] <= TRAIN_END)].copy()\n","    bin_cols  = detect_binary_columns(df_train_only, feat_cols)\n","    sent_cols = detect_sentiment_columns(feat_cols)\n","    scaler_train = ZeroPreservingScaler(cols=feat_cols, sentiment_cols=sent_cols, binary_cols=bin_cols).fit(df_train_only)\n","\n","    # ----- Transform full cadence range using Train-only scaler -----\n","    df_all   = df[(df[\"date\"] >= TRAIN_START) & (df[\"date\"] <= TEST_END)].copy()\n","    df_all_s = scaler_train.transform(df_all)\n","\n","    preds_df, epochs_per_refit, used_refits_iso = daily_walk_forward(df_all_s, feat_cols, L, out_dir, ticker)\n","\n","    # ----- Enforce exact Test window and 146 rows -----\n","    preds_df = preds_df[(preds_df[\"date\"] >= pd.Timestamp(TEST_START)) & (preds_df[\"date\"] <= pd.Timestamp(TEST_END))].copy()\n","    if preds_df.empty:\n","        raise RuntimeError(f\"No predictions produced for {ticker}. Check input dates and feature/Target availability.\")\n","    preds_df = ensure_146(preds_df)\n","    preds_df[\"residual\"] = preds_df[\"y_true\"] - preds_df[\"y_hat\"]\n","    preds_df[\"in_sample_flag\"] = 0\n","\n","    # ----- Metrics -----\n","    y_true = preds_df[\"y_true\"].values.astype(float)\n","    y_hat  = preds_df[\"y_hat\"].values.astype(float)\n","    metrics = compute_metrics(y_true, y_hat)  # expected to include RMSE, MAE, U2, DA_epsilon, Coverage, n, Sharpe_0bps, Sharpe_10bps, MaxDD_0bps, MaxDD_10bps, Turnover\n","\n","    # ----- Write artefacts -----\n","    preds_path = out_dir / f\"predictions_TRANSFORMER_{ticker}.csv\"\n","    preds_df.to_csv(preds_path, index=False)\n","\n","    metrics_path = out_dir / f\"metrics_TRANSFORMER_{ticker}.json\"\n","    with open(metrics_path, \"w\") as f:\n","        json.dump({k: (float(v) if isinstance(v, (np.floating,)) else v) for k, v in metrics.items()}, f, indent=2)\n","\n","    # Canonical training-curve image: copy the last monthly curve to a fixed name\n","    if used_refits_iso:\n","        last_refit_png = out_dir / f\"training_curves_{ticker}_{used_refits_iso[-1]}.png\"\n","        canonical_png  = out_dir / f\"training_curves_TRANSFORMER_{ticker}.png\"\n","        if last_refit_png.exists():\n","            shutil.copy2(last_refit_png, canonical_png)\n","\n","    # ----- run_config -----\n","    cfg = {\n","        \"model_id\": \"TRANSFORMER\",\n","        \"ticker\": ticker,\n","        \"L\": int(L),\n","        \"features_used\": feat_cols,\n","        \"sentiment_zero_preserving\": True,\n","        \"hyperparameters\": {\n","            \"d_model\": int(D_MODEL),\n","            \"nhead\": int(NHEAD),\n","            \"dim_ff\": int(DIM_FF),\n","            \"depth\": int(DEPTH),\n","            \"dropout\": float(DROPOUT),\n","            \"batch_size\": int(BATCH_SIZE),\n","            \"max_epochs\": int(MAX_EPOCHS),\n","            \"early_stop_patience\": int(ES_PATIENCE),\n","            \"lr\": float(LR),\n","            \"weight_decay\": float(WEIGHT_DECAY),\n","            \"grad_clip\": float(GRAD_CLIP),\n","            \"optimiser\": \"Adam\",\n","            \"lr_schedule\": None,\n","        },\n","        \"random_seed\": int(RANDOM_SEED),\n","        \"cadence\": \"monthly_refit\",\n","        \"refit_dates\": used_refits_iso,\n","        \"epochs_per_refit\": epochs_per_refit,\n","        \"parameter_count\": int(parameter_count(TransformerRegressor(len(feat_cols), D_MODEL, NHEAD, DIM_FF, DEPTH, DROPOUT))),\n","        \"metrics_on_inverse_levels\": True  # explicit confirmation for dissertation reporting\n","    }\n","    cfg_path = out_dir / f\"run_config_TRANSFORMER_{ticker}.json\"\n","    with open(cfg_path, \"w\") as f:\n","        json.dump(cfg, f, indent=2)\n","\n","def main():\n","    # Per-ticker runs\n","    for t in TICKERS:\n","        run_for_ticker(t)\n","\n","    # Provenance at bundle root\n","    BUNDLE_ROOT.mkdir(parents=True, exist_ok=True)\n","    write_env_manifest(BUNDLE_ROOT)\n","    write_file_hashes(BUNDLE_ROOT)\n","\n","    # Zip the bundle root so provenance is at archive top level\n","    zip_path = package_zip(BUNDLE_ROOT)\n","\n","    # Optional Colab download\n","    try:\n","        from google.colab import files  # type: ignore\n","        files.download(str(zip_path))\n","    except Exception:\n","        pass\n","\n","    print(f\"Done.\\nBundle root: {BUNDLE_ROOT.resolve()}\\nZip: {zip_path.resolve()}\\nSHA256(zip): {sha256_file(zip_path)}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"hcKke-Afo3jN","executionInfo":{"status":"ok","timestamp":1759090812626,"user_tz":-60,"elapsed":5773467,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"3eb10473-6625-46a3-f467-3362b8454e2d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a2154501-a222-4b35-9e33-237c5f6e385e\", \"TRANSFORMER_FINAL_bundle.zip\", 1111826)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done.\n","Bundle root: /content/TRANSFORMER_FINAL\n","Zip: /content/TRANSFORMER_FINAL_bundle.zip\n","SHA256(zip): fb44311b7598e04b61840e314707065e3b78ae0aeefc0fca084582bc4b787d55\n"]}]}]}