{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwi7p4JzLPKlhRzFDwWWzv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HYBRID (LSTM -> Transformer)\n"],"metadata":{"id":"DWgggWVsd8jw"}},{"cell_type":"markdown","source":["##01. Imports and Configuration\n","\n","This block sets up libraries, deterministic behaviour and immutable hyperparameters.\n","Why: ensures run-to-run reproducibility and identical outputs.\n","Method choices: fixed seeds; TF32 disabled; America/New_York 16:00 close; constant hyperparameters."],"metadata":{"id":"6OlPK2mBd19_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"npwF-CVwbl9b"},"outputs":[],"source":["import os, sys, json, math, time, hashlib, zipfile, random, shutil\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","# ------------------------- Determinism prelude -------------------------\n","SEED = 42\n","os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","if hasattr(torch.backends, \"cuda\") and hasattr(torch.backends.cuda, \"matmul\"):\n","    torch.backends.cuda.matmul.allow_tf32 = False\n","if hasattr(torch.backends, \"cudnn\"):\n","    torch.backends.cudnn.allow_tf32 = False\n","try:\n","    torch.use_deterministic_algorithms(True)\n","except Exception:\n","    pass\n","\n","def set_seed(seed: int = SEED):\n","    import random as _r, numpy as _np, torch as _t\n","    _r.seed(seed); _np.random.seed(seed)\n","    _t.manual_seed(seed)\n","    if _t.cuda.is_available():\n","        _t.cuda.manual_seed(seed); _t.cuda.manual_seed_all(seed)\n","\n","# ----------------------------- Config -----------------------------\n","MODEL_ID = \"HYBRID\"\n","TICKERS = [\"AAPL\",\"AMZN\",\"MSFT\",\"TSLA\",\"AMD\"]\n","\n","TRAIN_START = \"2021-02-03\"\n","TRAIN_END   = \"2022-12-30\"\n","VAL_START   = \"2023-01-03\"\n","VAL_END     = \"2023-05-31\"\n","TEST_START  = \"2023-06-01\"\n","TEST_END    = \"2023-12-28\"\n","TEST_LEN    = 146\n","\n","LOOKBACK = 90\n","HORIZON  = 1\n","\n","BATCH_SIZE      = 64\n","EPOCHS_BASE     = 120\n","REFIT_MONTHLY   = True\n","REFIT_EPOCHS    = 6\n","LR              = 1e-3\n","WEIGHT_DECAY    = 1e-4\n","PATIENCE        = 15\n","GRAD_CLIP       = 1.0\n","\n","NUM_WORKERS         = 0\n","PIN_MEMORY          = torch.cuda.is_available()\n","PERSISTENT_WORKERS  = False\n","\n","DA_EPS = 0.0010\n","TIMEZONE = \"America/New_York\"\n","MARKET_CUTOFF = \"16:00\"\n","MEAN_BIAS_TAU = 10.0\n","\n","REFERENCE_FEATURES_JSON = \"transformer_outputs/TRANSFORMER/AAPL/run_config_TRANSFORMER_AAPL.json\"\n","\n","PRICE_COLS = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n","TECH_COLS  = [\"Return_1d\",\"LogRet_1d\",\"Vol_7\",\"Vol_21\",\"SMA_7\",\"SMA_21\",\"RSI_14\",\"MACD\",\"MACD_Signal\"]\n","\n","MODULE_ROOT = Path(\"outputs\")\n","MODEL_DIR   = MODULE_ROOT / MODEL_ID\n","TPL_DIR     = MODULE_ROOT / \"third_party_licenses\"\n","for d in [MODULE_ROOT, MODEL_DIR, TPL_DIR]:\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","set_seed(SEED)\n","\n","# --------------------- Sentiment detection (prefix-driven) ---------------------\n","SENTIMENT_PREFIXES = (\"Tw_\", \"Rd_\", \"Nw_SP500_\")\n","\n","def is_sentiment_feature(col: str) -> bool:\n","    return isinstance(col, str) and col.startswith(SENTIMENT_PREFIXES)\n","\n","def derive_sentiment_cols(features: List[str]) -> List[str]:\n","    return [c for c in features if is_sentiment_feature(c)]\n","\n","# --------------------- Feature-order -------------------------\n","def load_reference_features(path: str) -> Optional[List[str]]:\n","    p = Path(path)\n","    if not p.exists():\n","        return None\n","    try:\n","        ref = json.loads(p.read_text())\n","        if \"features_used\" in ref and isinstance(ref[\"features_used\"], list):\n","            feats = list(ref[\"features_used\"])\n","        else:\n","            feats = list(ref.get(\"features\", {}).get(\"list\", []))\n","        return feats\n","    except Exception:\n","        return None\n","\n","def features_sha256(features: List[str]) -> str:\n","    payload = json.dumps({\"features_used\": features}, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n","    h = hashlib.sha256(); h.update(payload); return h.hexdigest()\n","\n","def build_features(df: pd.DataFrame) -> List[str]:\n","    ref_feats = load_reference_features(REFERENCE_FEATURES_JSON)\n","\n","    def local_fallback() -> List[str]:\n","        core = PRICE_COLS + TECH_COLS\n","        def pick(pref): return sorted([c for c in df.columns if c.startswith(pref)])\n","        return [c for c in core if c in df.columns] + pick(\"Tw_\") + pick(\"Rd_\") + pick(\"Nw_SP500_\")\n","\n","    if ref_feats:\n","        feats = [c for c in ref_feats if c in df.columns]\n","        fb = local_fallback()\n","        # Strict list equality to guarantee identical order; include SHA guidance for audits.\n","        if feats != fb:\n","            raise AssertionError(\n","                \"Features parity violation: reference features_used differ in ORDER or MEMBERSHIP from local derivation.\\n\"\n","                f\"ref_len={len(feats)} fb_len={len(fb)}\\n\"\n","                f\"ref_sha={features_sha256(feats)}\\nfb_sha={features_sha256(fb)}\\n\"\n","                \"Regenerate the reference pack (LSTM(SE)/Transformer) or align upstream CSV schemas to restore identical features_used.\"\n","            )\n","    else:\n","        feats = local_fallback()\n","\n","    for rc in PRICE_COLS:\n","        assert rc in feats, f\"Required column missing: {rc}\"\n","    return feats"]},{"cell_type":"markdown","source":["## Data Loading\n","\n","This block defines file discovery and enforced temporal splits.\n","Why: exact reading of final CSVs and fixed Train/Validation/Test ranges.\n","Method choices: fixed splits 2021-02-03→2022-12-30 (Train), 2023-01-03→2023-05-31 (Validation), 2023-06-01→2023-12-28 (Test, n=146)."],"metadata":{"id":"t7G5nh5Ud-wM"}},{"cell_type":"code","source":["# ------------------------ I/O & Sanitisation -----------------------\n","def load_input_csv(ticker: str) -> pd.DataFrame:\n","    p = Path(f\"{ticker}_input.csv\")\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Expected {p.name} next to the script.\")\n","    df = pd.read_csv(p)\n","    need = [\"date\",\"ticker\",\"Target\",\"Close\"]\n","    miss = sorted(list(set(need) - set(df.columns)))\n","    if miss:\n","        raise AssertionError(f\"{p.name} missing columns: {miss}\")\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    df = df.sort_values(\"date\").reset_index(drop=True)\n","    return df\n","\n","def sanitise_frame(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    for c in df.columns:\n","        if c not in [\"date\",\"ticker\"]:\n","            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","    # sentiment zero-encoding (robust to missing *_count columns)\n","    for fam, cnt in [(\"Tw_\", \"Tw_count\"), (\"Rd_\", \"Rd_count\"), (\"Nw_SP500_\", \"Nw_SP500_count\")]:\n","        fam_cols = [c for c in df.columns if c.startswith(fam)]\n","        if fam_cols:\n","            df[fam_cols] = df[fam_cols].fillna(0.0)\n","            cnt_series = df[cnt] if cnt in df.columns else pd.Series(0, index=df.index, dtype=float)\n","            cnt_series = pd.to_numeric(cnt_series, errors=\"coerce\").fillna(0.0)\n","            mask0 = (cnt_series == 0.0)\n","            df.loc[mask0, fam_cols] = 0.0\n","    for cnt in [\"Tw_count\",\"Rd_count\",\"Nw_SP500_count\"]:\n","        if cnt in df: df[cnt] = pd.to_numeric(df[cnt], errors=\"coerce\").fillna(0.0)\n","\n","    # prices/technicals: causal ffill\n","    for c in PRICE_COLS + TECH_COLS:\n","        if c in df.columns:\n","            df[c] = df[c].ffill()\n","\n","    # Target AFTER features exist\n","    if \"Close\" not in df.columns:\n","        raise AssertionError(\"Close column missing for Target computation.\")\n","    df[\"Target\"] = df[\"Close\"].shift(-1)\n","\n","    # fill remaining NaNs (features only)\n","    feat_cols = [c for c in df.columns if c not in [\"date\",\"ticker\",\"Target\"]]\n","    df[feat_cols] = df[feat_cols].fillna(0.0)\n","    return df\n","\n","def split_data(df: pd.DataFrame):\n","    tr = df[(df[\"date\"] >= pd.to_datetime(TRAIN_START)) & (df[\"date\"] <= pd.to_datetime(TRAIN_END))].copy().reset_index(drop=True)\n","    va = df[(df[\"date\"] >= pd.to_datetime(VAL_START))   & (df[\"date\"] <= pd.to_datetime(VAL_END))].copy().reset_index(drop=True)\n","    te = df[(df[\"date\"] >= pd.to_datetime(TEST_START))  & (df[\"date\"] <= pd.to_datetime(TEST_END))].copy().reset_index(drop=True)\n","    if len(te) != TEST_LEN:\n","        raise AssertionError(f\"Test rows={len(te)} != {TEST_LEN}.\")\n","    for name, d in [(\"Train\",tr),(\"Val\",va),(\"Test\",te)]:\n","        assert d[\"date\"].is_monotonic_increasing, f\"{name} not sorted\"\n","        assert d[\"date\"].is_unique, f\"{name} duplicate dates\"\n","    return tr, va, te\n","\n","def soft_target_check(df: pd.DataFrame, name: str):\n","    delta = (df[\"Target\"] - df[\"Close\"].shift(-1)).abs()\n","    mism = int((delta > 1e-6).sum())\n","    if mism > 0:\n","        print(f\"[warn] {name}: {mism} rows where Target != Close.shift(-1) (tol 1e-6).\")"],"metadata":{"id":"7KYa4U5EeD9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Preprocessing\n","\n","This block applies train-only scaling and windowing.\n","Why: standardise features while preserving zeros for Tw_/Rd_/Nw_SP500_* and prepare sequential windows.\n","Method choices: Zero-preserving standardiser; no shuffling; Validation transformed using the history-fitted scaler during refits to avoid leakage."],"metadata":{"id":"r8CLpC8FeEwK"}},{"cell_type":"code","source":["# ------------------ Zero-preserving z-scores ----------------\n","class ZeroPreservingStandardScaler:\n","    def __init__(self, sentiment_cols: List[str], eps: float = 1e-8):\n","        self.sentiment = set([c for c in sentiment_cols if isinstance(c, str)])\n","        self.mu, self.sigma = {}, {}\n","        self.feature_order = None\n","        self.eps = float(eps)\n","    def fit(self, X_df: pd.DataFrame):\n","        X = X_df.astype(np.float32)\n","        self.feature_order = list(X.columns)\n","        for c in self.feature_order:\n","            col = X[c].to_numpy(np.float32)\n","            if c in self.sentiment:\n","                nz = col != 0.0\n","                vals = col[nz]\n","                mu = float(vals.mean()) if vals.size else 0.0\n","                sd = float(vals.std(ddof=0)) if vals.size else 1.0\n","                sd = max(sd, self.eps)\n","            else:\n","                mu = float(col.mean()); sd = max(float(col.std(ddof=0)), self.eps)\n","            self.mu[c], self.sigma[c] = mu, sd\n","        return self\n","    def transform(self, X_df: pd.DataFrame) -> pd.DataFrame:\n","        X = X_df.astype(np.float32).copy()\n","        for c in self.feature_order:\n","            if c in self.sentiment:\n","                nz = X[c].to_numpy(np.float32) != 0.0\n","                X.loc[nz, c] = (X.loc[nz, c] - self.mu[c]) / self.sigma[c]\n","                X.loc[~nz, c] = 0.0\n","            else:\n","                X[c] = (X[c] - self.mu[c]) / self.sigma[c]\n","        X = X.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n","        return X\n","\n","def audit_zero_preservation(df_original: pd.DataFrame, df_scaled: pd.DataFrame, sentiment_cols: List[str]):\n","    bad = []\n","    for c in sentiment_cols:\n","        if c not in df_scaled.columns or c not in df_original.columns:\n","            continue\n","        mask = df_original[c] == 0\n","        if mask.any() and not np.allclose(df_scaled.loc[mask, c].to_numpy(), 0.0):\n","            bad.append(c)\n","    if bad:\n","        raise AssertionError(f\"Zero preservation failed for: {bad}\")\n","\n","def zero_preservation_report(df_before: pd.DataFrame, df_after: pd.DataFrame, sentiment_cols: List[str]) -> pd.DataFrame:\n","    rows = []\n","    for c in sentiment_cols:\n","        if c not in df_before.columns or c not in df_after.columns:\n","            continue\n","        xin = df_before[c].to_numpy()\n","        xout = df_after[c].to_numpy()\n","        violations = int(((xin == 0) & (xout != 0)).sum())\n","        rows.append({\"feature\": c, \"violations\": violations, \"ok\": violations == 0})\n","    return pd.DataFrame(rows)\n","\n","# ------------------ Windowing (residual-anchored) -------------------\n","def make_windows_from_slice(df_slice: pd.DataFrame, feature_cols: List[str], lookback: int = LOOKBACK, horizon: int = HORIZON) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","    dfw = df_slice.sort_values(\"date\").reset_index(drop=True).copy()\n","    A = dfw[feature_cols].to_numpy(np.float32)\n","    C = dfw[\"Close\"].to_numpy(np.float32)\n","    X_list, y_list, c_list = [], [], []\n","    N = len(dfw)\n","    for i in range(lookback, N - horizon + 1):\n","        Xi = A[i-lookback:i, :]\n","        yi_idx = i - 1 + horizon\n","        if yi_idx < 0 or yi_idx >= len(C): continue\n","        yi = C[yi_idx]\n","        ci = C[i-1]\n","        if np.isfinite(Xi).all() and np.isfinite(yi) and np.isfinite(ci):\n","            X_list.append(Xi); y_list.append(yi); c_list.append(ci)\n","    if not X_list:\n","        return np.empty((0, lookback, len(feature_cols)), np.float32), np.empty((0,1), np.float32), np.empty((0,1), np.float32)\n","    X = np.stack(X_list).astype(np.float32)\n","    y = np.array(y_list, dtype=np.float32).reshape(-1,1)\n","    c = np.array(c_list, dtype=np.float32).reshape(-1,1)\n","    return X, y, c\n","\n","def make_val_windows_with_carryin(trXy: pd.DataFrame, vaXy: pd.DataFrame, feature_cols: List[str], lookback: int = LOOKBACK, horizon: int = HORIZON) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","    if lookback <= 0 or horizon <= 0:\n","        return np.empty((0,0,0), np.float32), np.empty((0,1), np.float32), np.empty((0,1), np.float32)\n","    ctx = trXy.tail(lookback).copy()\n","    cat = pd.concat([ctx, vaXy], ignore_index=True)\n","    A = cat[feature_cols].to_numpy(np.float32)\n","    C = cat[\"Close\"].to_numpy(np.float32)\n","    X_list, y_list, c_list = [], [], []\n","    n_val = len(vaXy); Ncat = len(cat)\n","    for j in range(n_val):\n","        k = lookback + j\n","        start = k - lookback; end = k\n","        label_idx = (k - 1) + horizon\n","        if start < 0 or label_idx >= Ncat: continue\n","        Xi = A[start:end, :]; yi = float(C[label_idx]); ci = float(C[end-1])\n","        if Xi.shape[0] == lookback and np.isfinite(Xi).all() and math.isfinite(yi) and math.isfinite(ci):\n","            X_list.append(Xi); y_list.append(yi); c_list.append(ci)\n","    if not X_list:\n","        return np.empty((0, lookback, A.shape[1] if A.ndim == 2 else 0), np.float32), np.empty((0,1), np.float32), np.empty((0,1), np.float32)\n","    X = np.stack(X_list).astype(np.float32)\n","    y = np.array(y_list, dtype=np.float32).reshape(-1,1)\n","    c = np.array(c_list, dtype=np.float32).reshape(-1,1)\n","    return X, y, c\n","\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, X, y, c):\n","        self.X = X.astype(np.float32); self.y = y.astype(np.float32); self.c = c.astype(np.float32)\n","    def __len__(self): return len(self.X)\n","    def __getitem__(self, i):\n","        return torch.from_numpy(self.X[i]), torch.from_numpy(self.y[i]), torch.from_numpy(self.c[i])\n","\n","def _worker_init_fn(worker_id):\n","    seed = SEED + worker_id + 1\n","    random.seed(seed); np.random.seed(seed)\n","\n","def make_loader(X, y, c, batch=BATCH_SIZE):\n","    # Note: shuffle=False for sequence continuity; no generator needed.\n","    return DataLoader(\n","        TimeSeriesDataset(X, y, c),\n","        batch_size=batch,\n","        shuffle=False,\n","        drop_last=False,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        persistent_workers=PERSISTENT_WORKERS if NUM_WORKERS > 0 else False,\n","        worker_init_fn=_worker_init_fn if NUM_WORKERS > 0 else None,\n","    )"],"metadata":{"id":"S7agGTZOeHNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model Definition\n","\n","This block defines the Hybrid LSTM → Transformer model and scheduler helper.\n","Why: exact architecture and forward pass for residual-anchored level forecasts.\n","Method choices: sinusoidal positional encoding; TransformerEncoder; MLP head produces delta then add last Close."],"metadata":{"id":"hUfod7ABeJhO"}},{"cell_type":"code","source":["# ----------------------------- Model -------------------------------\n","class SinusoidalPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=1000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer(\"pe\", pe.unsqueeze(0), persistent=False)\n","    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n","\n","class HybridLSTMTransformer(nn.Module):\n","    def __init__(self, input_dim, lstm_hidden=128, lstm_layers=1, d_model=128, nhead=4, enc_layers=2, dropout=0.2):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_dim, lstm_hidden, lstm_layers, batch_first=True, bidirectional=False)\n","        self.lstm_drop = nn.Dropout(dropout)\n","        self.proj_in = nn.Linear(lstm_hidden, d_model)\n","        self.posenc  = SinusoidalPositionalEncoding(d_model)\n","        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n","                                               dropout=dropout, batch_first=True, activation=\"relu\")\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=enc_layers)\n","        self.enc_norm = nn.LayerNorm(d_model)\n","        self.head = nn.Sequential(nn.Linear(lstm_hidden + d_model, 256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n","    def forward(self, x):\n","        x = torch.nan_to_num(x)\n","        lstm_out, _ = self.lstm(x)\n","        lstm_last = self.lstm_drop(lstm_out[:, -1, :])\n","        z = self.proj_in(lstm_out)\n","        z = self.posenc(z); z = self.encoder(z); z = self.enc_norm(z)\n","        enc_pool = z.mean(dim=1)\n","        delta = self.head(torch.cat([lstm_last, enc_pool], dim=1))\n","        return torch.nan_to_num(delta)\n","\n","def _make_plateau_scheduler(opt):\n","    try: return ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3, verbose=False)\n","    except TypeError: return ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)"],"metadata":{"id":"DrglV2UJeLY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Training\n","\n","This block trains the base model and supports ES with ReduceLROnPlateau.\n","Why: obtain a fitted model for the initial expanding-origin phase.\n","Method choices: accumulate losses on device; early stopping on Validation."],"metadata":{"id":"yhnNMeB4eRc2"}},{"cell_type":"code","source":["# --------------------------- Training loop -------------------------\n","def train_model(model, tr_loader, va_loader, epochs=EPOCHS_BASE, lr=LR, wd=WEIGHT_DECAY,\n","                patience=PATIENCE, grad_clip=GRAD_CLIP):\n","    model.to(DEVICE)\n","    loss_fn = nn.MSELoss()\n","    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","    sched = _make_plateau_scheduler(opt)\n","\n","    best = float(\"inf\"); stall = 0; best_state = None; best_epoch = 0\n","    tr_hist, va_hist = [], []\n","    ep_done = 0\n","\n","    for ep in range(epochs):\n","        ep_done = ep + 1\n","\n","        # Accumulate losses on device; convert once per epoch to avoid host syncs each batch.\n","        tr_loss_sum_t = torch.zeros((), device=DEVICE)\n","        tr_count = 0\n","\n","        model.train()\n","        for xb, yb, cb in tr_loader:\n","            xb, yb, cb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True), cb.to(DEVICE, non_blocking=True)\n","            opt.zero_grad(set_to_none=True)\n","            delta = model(xb)\n","            pred_level = cb + delta\n","            loss = loss_fn(pred_level, yb)\n","            if not torch.isfinite(loss):\n","                continue\n","            loss.backward()\n","            if grad_clip is not None:\n","                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","            opt.step()\n","            # accumulate weighted by batch size\n","            tr_loss_sum_t += loss.detach() * xb.size(0)\n","            tr_count += xb.size(0)\n","\n","        tr_loss_epoch = (tr_loss_sum_t / max(tr_count, 1)).item() if tr_count else float(\"inf\")\n","        tr_hist.append(tr_loss_epoch)\n","\n","        va_loss_sum_t = torch.zeros((), device=DEVICE)\n","        va_count = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for xb, yb, cb in va_loader:\n","                xb, yb, cb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True), cb.to(DEVICE, non_blocking=True)\n","                delta = model(xb)\n","                pred_level = cb + delta\n","                l = loss_fn(pred_level, yb)\n","                if not torch.isfinite(l):\n","                    continue\n","                va_loss_sum_t += l * xb.size(0)\n","                va_count += xb.size(0)\n","\n","        va_loss = (va_loss_sum_t / max(va_count, 1)).item() if va_count else float(\"inf\")\n","        va_hist.append(va_loss)\n","        sched.step(va_loss if np.isfinite(va_loss) else best)\n","\n","        if va_loss < best - 1e-9:\n","            best = va_loss; stall = 0; best_epoch = int(ep) + 1\n","            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n","        else:\n","            stall += 1\n","            if stall >= patience:\n","                break\n","\n","    if best_state is not None:\n","        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n","    return model, tr_hist, va_hist, best_epoch, ep_done"],"metadata":{"id":"CcOC8bJgeS0v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation\n","\n","This block computes deterministic metrics and the trading diagnostic.\n","Why: level accuracy plus rule-based long/short with ε-threshold sign.\n","Method choices: RMSE/MAE; Theil’s U2 vs naive last Validation close; Directional Accuracy ε=0.0010; Sharpe/MaxDD at 0 and 10 bps."],"metadata":{"id":"LqhxaewfeVcw"}},{"cell_type":"code","source":["# ------------------------------ Metrics ----------------------------\n","def rmse(a, b): a = np.asarray(a, float); b = np.asarray(b, float); return float(np.sqrt(np.mean((a-b)**2)))\n","def mae(a, b):  a = np.asarray(a, float); b = np.asarray(b, float); return float(np.mean(np.abs(a-b)))\n","def theils_u2(pred, actual, naive): return rmse(pred, actual) / max(rmse(naive, actual), 1e-12)\n","\n","def directional_accuracy_eps_from_levels(pred_levels, actual_levels, eps=DA_EPS, prev0=None):\n","    pred = np.asarray(pred_levels, float); act = np.asarray(actual_levels, float)\n","    base_prev = act[0] if prev0 is None else float(prev0)\n","    prev = np.concatenate([[base_prev], act[:-1]])\n","    ret_pred = (pred - prev) / np.where(prev == 0.0, 1.0, prev)\n","    ret_act  = (act  - prev) / np.where(prev == 0.0, 1.0, prev)\n","    mask = np.abs(ret_act) > eps\n","    n = int(mask.sum())\n","    if n == 0: return float(\"nan\"), 0.0, 0\n","    da  = float(np.mean(np.sign(ret_pred[mask]) == np.sign(ret_act[mask])))\n","    cov = float(n / len(ret_act))\n","    return da, cov, n\n","\n","def backtest_long_short_series(pred_levels, actual_levels, eps=DA_EPS, cost_bps=0.0, prev0=None):\n","    pred = np.asarray(pred_levels, float); act = np.asarray(actual_levels, float)\n","    base_prev = act[0] if prev0 is None else float(prev0)\n","    prev = np.concatenate([[base_prev], act[:-1]])\n","    ret_act  = (act  - prev) / np.where(prev == 0.0, 1.0, prev)\n","    ret_pred = (pred - prev) / np.where(prev == 0.0, 1.0, prev)\n","    pos = np.where(ret_pred >  eps,  1, np.where(ret_pred < -eps, -1, 0)).astype(int)\n","    pos_prev = np.concatenate([[0], pos[:-1]])\n","    cost = np.abs(pos - pos_prev) * (cost_bps/10000.0)\n","    strat = pos_prev * ret_act - cost\n","    eq = np.cumprod(1.0 + strat); peak = np.maximum.accumulate(eq)\n","    dd = 1.0 - eq/np.maximum(peak, 1e-12)\n","    sharpe = float((np.mean(strat) / (np.std(strat) + 1e-12)) * np.sqrt(252.0))\n","    return {\"sharpe\": sharpe, \"maxdd\": float(dd.max()), \"turnover\": int((pos != pos_prev).sum()),\n","            \"ret_pred_min\": float(ret_pred.min()), \"ret_pred_max\": float(ret_pred.max())}"],"metadata":{"id":"765CBmb_eYMr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Outputs and Artefacts\n","\n","This block handles provenance metadata, plots, run_config building, monthly refit evaluation, writing artefacts, packaging, and patch/verify.\n","Why: meet finalisation gate requirements and produce auditable deliverables.\n","Method choices: parameter count; zero-preservation spot CSV; environment manifest; file hashes; ZIP pack; parity stub."],"metadata":{"id":"tVwf3xcfeaqn"}},{"cell_type":"code","source":["# -------------- Provenance templates and param count ---------------\n","TEMPLATE_COMMITS = {\n","    \"transformer_template\": {\n","        \"repo_url\": \"https://github.com/oliverguhr/transformer-time-series-prediction\",\n","        \"license\": \"MIT\",\n","        \"commit_sha\": \"4e120f3f9b67482dd9a8ca88b49671881120d20f\"\n","    },\n","    \"lstm_template\": {\n","        \"repo_url\": \"https://github.com/jinglescode/time-series-forecasting-pytorch\",\n","        \"license\": \"Apache-2.0\",\n","        \"commit_sha\": \"749689e352290616e69d9b3e243af36936328964\"\n","    }\n","}\n","def hybrid_param_count(input_dim, lstm_hidden=128, lstm_layers=1, d_model=128, nhead=4, enc_layers=2):\n","    def lstm_layer_params(in_dim, hid): return 4*(in_dim*hid + hid*hid + hid)\n","    total = lstm_layer_params(input_dim, lstm_hidden)\n","    for _ in range(1, lstm_layers): total += lstm_layer_params(lstm_hidden, lstm_hidden)\n","    total += lstm_hidden*d_model + d_model\n","    D = d_model\n","    per_enc = (3*D*D + 3*D) + (D*D + D) + (4*D*D + 4*D) + (4*D*D + D) + (4*D)\n","    total += enc_layers * per_enc\n","    total += (lstm_hidden + d_model)*256 + 256 + 256 + 1\n","    return int(total)\n","\n","# ------------------------------ Plots ------------------------------\n","def plot_training_curves(out_dir: Path, ticker: str, tr_hist, va_hist):\n","    plt.figure(); plt.plot(tr_hist, label=\"train_loss\"); plt.plot(va_hist, label=\"val_loss\")\n","    plt.title(f\"{ticker}: training curves\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.tight_layout()\n","    p = out_dir / f\"training_curves_{MODEL_ID}_{ticker}.png\"; plt.savefig(p); plt.close(); return p\n","\n","def plot_actual_vs_pred(out_dir: Path, ticker: str, dates, actual, pred):\n","    fig, ax = plt.subplots(figsize=(9,4))\n","    ax.plot(dates, actual, label=\"Actual\"); ax.plot(dates, pred, label=\"Predicted\")\n","    ax.set_title(f\"{ticker}: Actual vs Predicted\"); ax.set_xlabel(\"date\"); ax.set_ylabel(\"price level\"); ax.legend()\n","    fig.tight_layout(); p = out_dir / f\"actual_vs_pred_{MODEL_ID}_{ticker}.png\"; fig.savefig(p); plt.close(fig); return p\n","\n","def plot_residuals_strip(out_dir: Path, ticker: str, dates, residual):\n","    fig, ax = plt.subplots(figsize=(9,3))\n","    ax.plot(dates, residual); ax.axhline(0, ls=\"--\")\n","    ax.set_title(f\"{ticker}: Residuals\"); ax.set_xlabel(\"date\"); ax.set_ylabel(\"actual - pred\")\n","    fig.tight_layout(); p = out_dir / f\"residuals_strip_{MODEL_ID}_{ticker}.png\"; fig.savefig(p); plt.close(fig); return p\n","\n","# ---------------------------- run_config ---------------------------\n","def build_run_config_spec(ticker, features, scaler_summary, refit_dates: List[str], param_count: int,\n","                          per_refit_seeds: List[int], base_best_epoch: int, refit_epochs_achieved: List[int]):\n","    env = {\"python\": sys.version.split()[0], \"numpy\": np.__version__, \"pandas\": pd.__version__,\n","           \"torch\": torch.__version__, \"device\": DEVICE, \"timezone\": TIMEZONE, \"price_timestamp\": MARKET_CUTOFF}\n","    spec = {\n","        \"pipeline\": \"hybrid_v3_residual_anchor\",\n","        \"model_id\": MODEL_ID,\n","        \"ticker\": ticker,\n","        \"splits\": {\"train\": [TRAIN_START, TRAIN_END], \"val\": [VAL_START, VAL_END], \"test\": [TEST_START, TEST_END]},\n","        \"split_test\": {\"start\": TEST_START, \"end\": TEST_END, \"n\": TEST_LEN},\n","        \"features\": {\"list\": features, \"order_fixed\": True, \"count\": len(features),\n","                     \"schema\": {\"target\": \"Target\", \"close_raw\": \"Close\"}, \"use_sentiment\": True},\n","        \"scaler\": {\"type\": \"ZeroPreservingStandardScaler\", \"zero_preserved_families\": list(SENTIMENT_PREFIXES),\n","                   \"summary\": scaler_summary, \"scope\": \"FINAL_REFIT\"},\n","        \"model\": {\"templates\": TEMPLATE_COMMITS,\n","                  \"fusion\": \"LSTM -> Linear -> PosEnc -> TransformerEncoder; concat(LSTM_last, mean_pool(Transformer)) -> MLP; output delta; add last Close\",\n","                  \"hparams\": {\"input_dim\": len(features), \"lstm_hidden\": 128, \"lstm_layers\": 1,\n","                              \"d_model\": 128, \"nhead\": 4, \"enc_layers\": 2, \"dropout\": 0.2},\n","                  \"parameter_count\": param_count},\n","        \"training\": {\"optimizer\": \"AdamW\", \"lr\": LR, \"weight_decay\": WEIGHT_DECAY,\n","                     \"scheduler\": {\"name\": \"ReduceLROnPlateau\", \"factor\": 0.5, \"patience\": 3},\n","                     \"loss\": \"MSE(level)\", \"early_stopping\": {\"on\":\"val_loss\",\"patience\": PATIENCE},\n","                     \"epochs_base\": EPOCHS_BASE, \"refit_monthly\": REFIT_MONTHLY, \"epochs_per_refit\": REFIT_EPOCHS,\n","                     \"epochs_per_refit_achieved\": refit_epochs_achieved, \"base_best_epoch\": base_best_epoch,\n","                     \"batch_size\": BATCH_SIZE, \"seed\": SEED, \"grad_clip\": GRAD_CLIP},\n","        \"dataloader\": {\"num_workers\": NUM_WORKERS, \"pin_memory\": PIN_MEMORY, \"persistent_workers\": PERSISTENT_WORKERS},\n","        \"cadence\": {\"lookback\": LOOKBACK, \"horizon\": HORIZON, \"mode\": \"monthly_refit\", \"refit_dates\": refit_dates},\n","        \"cadence_str\": \"monthly_refit\",\n","        \"window_L\": LOOKBACK,\n","        \"metrics_policy\": {\"theils_u2\": \"vs_naive_last_close\",\n","                           \"da_epsilon\": {\"eps\": DA_EPS, \"strict_gt\": True},\n","                           \"trading_rule\": \"eps-threshold sign\", \"cost_bps\": [0,10]},\n","        \"environment\": env,\n","        \"provenance\": {\n","            \"origin\": \"Adapted\",\n","            \"repos\": [\n","                {\"url\": TEMPLATE_COMMITS[\"transformer_template\"][\"repo_url\"],\n","                 \"commit_sha\": TEMPLATE_COMMITS[\"transformer_template\"][\"commit_sha\"],\n","                 \"license\": TEMPLATE_COMMITS[\"transformer_template\"][\"license\"]},\n","                {\"url\": TEMPLATE_COMMITS[\"lstm_template\"][\"repo_url\"],\n","                 \"commit_sha\": TEMPLATE_COMMITS[\"lstm_template\"][\"commit_sha\"],\n","                 \"license\": TEMPLATE_COMMITS[\"lstm_template\"][\"license\"]}\n","            ],\n","            \"attribution\": \"Adapted; training/evaluation wrappers, residual anchor, monthly refit, and artefact schema authored.\"\n","        },\n","        \"features_used\": features,\n","        \"seeds\": {\"global\": SEED, \"per_refit\": per_refit_seeds},\n","        \"parameter_count\": param_count,\n","        \"batch_size\": BATCH_SIZE,\n","        \"epochs_per_refit\": REFIT_EPOCHS,\n","        \"epochs_per_refit_achieved\": refit_epochs_achieved,\n","        \"refit_dates\": refit_dates,\n","        \"clock\": f\"{TIMEZONE}@{MARKET_CUTOFF}\",\n","        \"target_definition\": \"Close.shift(-1) after features\",\n","        \"y_scaled\": False\n","    }\n","    canonical = json.dumps(spec, sort_keys=True, separators=(\",\", \":\"))\n","    spec[\"protocol_hash\"] = hashlib.sha256(canonical.encode(\"utf-8\")).hexdigest()\n","    return spec\n","\n","# ------------- Train once, then monthly refit for Test --------------\n","def expanding_origin_predict_monthly(model_cfg: Dict,\n","                                     df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame,\n","                                     features: List[str]):\n","    sentiment_cols = derive_sentiment_cols(features)\n","\n","    train_scaler = ZeroPreservingStandardScaler(sentiment_cols).fit(df_train[features])\n","    trS = train_scaler.transform(df_train[features])\n","    vaS = train_scaler.transform(df_val[features])\n","    audit_zero_preservation(df_train, trS, sentiment_cols)\n","    audit_zero_preservation(df_val,   vaS, sentiment_cols)\n","\n","    def assemble(base_df, scaled_df):\n","        out = scaled_df.copy()\n","        out.insert(0, \"date\", base_df[\"date\"].values)\n","        out[\"Close\"]  = base_df[\"Close\"].values\n","        out[\"Target\"] = base_df[\"Target\"].values\n","        return out\n","\n","    trXy, vaXy = assemble(df_train, trS), assemble(df_val, vaS)\n","    Xtr, ytr, ctr = make_windows_from_slice(trXy, features, LOOKBACK, HORIZON)\n","    Xva, yva, cva = make_val_windows_with_carryin(trXy, vaXy, features, LOOKBACK, HORIZON)\n","    if len(Xtr)==0 or len(Xva)==0:\n","        raise AssertionError(f\"Empty Train/Val windows: Xtr={len(Xtr)}, Xva={len(Xva)}\")\n","\n","    set_seed(SEED)\n","    model = HybridLSTMTransformer(input_dim=len(features), **model_cfg).to(DEVICE)\n","    tr_loader = make_loader(Xtr, ytr, ctr, BATCH_SIZE)\n","    va_loader = make_loader(Xva, yva, cva, BATCH_SIZE)\n","    model, tr_hist, va_hist, best_epoch, _ep_done = train_model(model, tr_loader, va_loader, epochs=EPOCHS_BASE)\n","\n","    preds, actuals, dates = [], [], []\n","    refit_dates: List[str] = []\n","    per_refit_seeds: List[int] = []\n","    refit_epochs_achieved: List[int] = []\n","    base_hist = pd.concat([df_train, df_val], ignore_index=True)\n","\n","    current_scaler = train_scaler\n","    cur_month = None\n","    for i in range(len(df_test)):\n","        dt = df_test.iloc[i][\"date\"]\n","        ym = (int(dt.year), int(dt.month))\n","\n","        if REFIT_MONTHLY and (cur_month is None or ym != cur_month):\n","            cur_month = ym\n","            refit_dates.append(pd.Timestamp(dt).strftime(\"%Y-%m-%d\"))\n","            per_refit_seeds.append(SEED)\n","\n","            hist = pd.concat([base_hist, df_test.iloc[:i]], ignore_index=True)\n","\n","            # Fit scaler on history up to prediction date; then transform *Validation* with that same scaler.\n","            # This preserves causality and avoids leakage from future Test data into scaling statistics.\n","            current_scaler = ZeroPreservingStandardScaler(sentiment_cols).fit(hist[features])\n","            histS = current_scaler.transform(hist[features])\n","            audit_zero_preservation(hist, histS, sentiment_cols)\n","\n","            histXy = assemble(hist, histS)\n","            Xh, yh, ch = make_windows_from_slice(histXy, features, LOOKBACK, HORIZON)\n","            if len(Xh) > 0:\n","                set_seed(SEED)\n","                model = HybridLSTMTransformer(input_dim=len(features), **model_cfg).to(DEVICE)\n","                h_loader = make_loader(Xh, yh, ch, BATCH_SIZE)\n","\n","                vaS_refit = current_scaler.transform(df_val[features])\n","                audit_zero_preservation(df_val, vaS_refit, sentiment_cols)\n","                vaXy_refit = assemble(df_val, vaS_refit)\n","                Xva_refit, yva_refit, cva_refit = make_val_windows_with_carryin(histXy, vaXy_refit, features, LOOKBACK, HORIZON)\n","                va_loader_refit = make_loader(Xva_refit, yva_refit, cva_refit, BATCH_SIZE)\n","                model, _, _, _best_refit, ep_done_refit = train_model(model, h_loader, va_loader_refit, epochs=REFIT_EPOCHS, patience=2)\n","                refit_epochs_achieved.append(int(ep_done_refit))\n","            else:\n","                refit_epochs_achieved.append(0)\n","\n","        hist = pd.concat([base_hist, df_test.iloc[:i]], ignore_index=True)\n","        lastL = hist[features].tail(LOOKBACK)\n","        if lastL.shape[0] < LOOKBACK:\n","            raise AssertionError(\"Insufficient history for lookback window.\")\n","        lastL_scaled = current_scaler.transform(lastL).to_numpy(np.float32)\n","        last_close = float(hist[\"Close\"].iloc[-1])\n","        xb = torch.from_numpy(np.nan_to_num(lastL_scaled, nan=0.0, posinf=0.0, neginf=0.0)[None, ...]).to(DEVICE)\n","        with torch.no_grad():\n","            delta_hat = float(torch.nan_to_num(model(xb)).cpu().numpy().reshape(-1)[0])\n","        yhat = last_close + delta_hat\n","        preds.append(yhat)\n","        actuals.append(float(df_test.iloc[i][\"Close\"]))\n","        dates.append(pd.Timestamp(dt))\n","\n","    dfp = pd.DataFrame({\"date\": dates, \"y_true\": actuals, \"y_hat\": preds})\n","    assert len(dfp) == TEST_LEN and dfp[\"date\"].min().strftime(\"%Y-%m-%d\")==TEST_START and dfp[\"date\"].max().strftime(\"%Y-%m-%d\")==TEST_END\n","    return dfp, tr_hist, va_hist, best_epoch, refit_dates, per_refit_seeds, current_scaler, vaS, sentiment_cols, refit_epochs_achieved\n","\n","# ---------------- Zero-encoding audit artefact ---------------------\n","def write_zero_encoding_check(out_dir: Path, val_raw: pd.DataFrame, val_scaled: pd.DataFrame, sentiment_cols: List[str]):\n","    cols = [c for c in sentiment_cols if c in val_raw.columns and c in val_scaled.columns]\n","    if not cols:\n","        return\n","    dfm = val_raw[[\"date\"] + cols].merge(\n","        val_scaled[[\"date\"] + cols], on=\"date\", suffixes=(\"_raw\",\"_scaled\")\n","    )\n","    # Pick the first month present for a quick spot check; auditors can adjust window if needed.\n","    if not dfm.empty:\n","        first_month = (dfm[\"date\"].dt.to_period(\"M\").iloc[0])\n","        mask = dfm[\"date\"].dt.to_period(\"M\") == first_month\n","        df_sample = dfm[mask].head(8)\n","    else:\n","        df_sample = dfm.head(8)\n","    df_sample.to_csv((out_dir / \"zero_encoding_check_val_sample.csv\"), index=False)\n","\n","# --------------- Write per-ticker artefacts ------------------------\n","def write_scaler_summary_csv(out_dir: Path,\n","                             scaler: ZeroPreservingStandardScaler,\n","                             val_raw_for_check: pd.DataFrame,\n","                             val_scaled_for_check: pd.DataFrame,\n","                             sentiment_cols: List[str]):\n","    rows = []\n","    zrep = zero_preservation_report(val_raw_for_check, val_scaled_for_check, sentiment_cols)\n","    zmap = {r[\"feature\"]: (int(r[\"violations\"]), bool(r[\"ok\"])) for _, r in zrep.iterrows()}\n","    for c in scaler.mu.keys():\n","        vio, ok = zmap.get(c, (0, True)) if c in sentiment_cols else (0, True)\n","        rows.append({\n","            \"feature\": c,\n","            \"mu\": float(scaler.mu[c]),\n","            \"sigma\": float(scaler.sigma[c]),\n","            \"is_sentiment\": (c in scaler.sentiment),\n","            \"violations\": int(vio),\n","            \"ok\": bool(ok),\n","            \"scope\": \"FINAL_REFIT\"\n","        })\n","    pd.DataFrame(rows).to_csv(out_dir / \"scaler_summary_FINAL_REFIT.csv\", index=False)\n","\n","def write_ticker_artefacts(ticker: str,\n","                           dfpred: pd.DataFrame,\n","                           tr_hist, va_hist, best_epoch: int,\n","                           refit_dates: List[str], per_refit_seeds: List[int],\n","                           refit_epochs_achieved: List[int],\n","                           scaler: ZeroPreservingStandardScaler,\n","                           features_used: List[str],\n","                           full_df: pd.DataFrame,\n","                           valS: pd.DataFrame,\n","                           sentiment_cols: List[str]):\n","    out_dir = MODEL_DIR / ticker\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    dates  = pd.to_datetime(dfpred[\"date\"]).to_list()\n","    actual = dfpred[\"y_true\"].to_numpy()\n","    pred   = dfpred[\"y_hat\"].to_numpy()\n","    residual = actual - pred\n","\n","    mean_gap = float(np.abs(np.mean(pred) - np.mean(actual)))\n","    assert mean_gap < MEAN_BIAS_TAU, f\"Level sanity failed: |mean(y_hat) - mean(y_true)|={mean_gap:.3f} >= τ={MEAN_BIAS_TAU}.\"\n","\n","    pd.DataFrame({\n","        \"date\": dfpred[\"date\"],\n","        \"y_true\": actual,\n","        \"y_hat\": pred,\n","        \"residual\": residual,\n","        \"in_sample_flag\": 0\n","    }).to_csv(out_dir / f\"predictions_{MODEL_ID}_{ticker}.csv\", index=False)\n","\n","    _, va, _ = split_data(full_df)\n","    val_last_close = float(va[\"Close\"].iloc[-1])\n","    naive = np.concatenate([[val_last_close], actual[:-1]])\n","\n","    da_value, cov_da, n_da = directional_accuracy_eps_from_levels(pred, actual, eps=DA_EPS, prev0=val_last_close)\n","    u2 = theils_u2(pred, actual, naive)\n","    summ0  = backtest_long_short_series(pred, actual, eps=DA_EPS, cost_bps=0.0, prev0=val_last_close)\n","    summ10 = backtest_long_short_series(pred, actual, eps=DA_EPS, cost_bps=10.0, prev0=val_last_close)\n","\n","    diag = {\n","        \"eps\": DA_EPS,\n","        \"turnover\": summ0[\"turnover\"],\n","        \"pred_ret_min\": summ0[\"ret_pred_min\"],\n","        \"pred_ret_max\": summ0[\"ret_pred_max\"],\n","        \"note\": \"If turnover==0, Sharpe/MaxDD=0 is expected by construction.\"\n","    }\n","    (out_dir / f\"trading_diag_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(diag, indent=2), encoding=\"utf-8\")\n","\n","    def _zeroish(x): return abs(x) < 1e-12 or (isinstance(x, float) and (math.isnan(x) or math.isinf(x)))\n","    if summ0[\"turnover\"] > 0 and (_zeroish(summ0[\"sharpe\"]) and _zeroish(summ0[\"maxdd\"])):\n","        raise AssertionError(f\"{ticker}: zero trading metrics at 0 bps despite turnover>0.\")\n","    if summ10[\"turnover\"] > 0 and (_zeroish(summ10[\"sharpe\"]) and _zeroish(summ10[\"maxdd\"])):\n","        raise AssertionError(f\"{ticker}: zero trading metrics at 10 bps despite turnover>0.\")\n","\n","    metrics = {\n","        \"RMSE\": rmse(pred, actual),\n","        \"MAE\": mae(pred, actual),\n","        \"U2\": u2,\n","        \"DA_epsilon\": da_value,\n","        \"Coverage\": cov_da,\n","        \"n\": int(len(actual)),\n","        \"Sharpe_0bps\":  float(summ0[\"sharpe\"]),\n","        \"MaxDD_0bps\":   float(summ0[\"maxdd\"]),\n","        \"Sharpe_10bps\": float(summ10[\"sharpe\"]),\n","        \"MaxDD_10bps\":  float(summ10[\"maxdd\"]),\n","        \"best_epoch\": int(best_epoch)\n","    }\n","    (out_dir / f\"metrics_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n","\n","    plot_training_curves(out_dir, ticker, tr_hist, va_hist)\n","    plot_actual_vs_pred(out_dir, ticker, dates, actual, pred)\n","    plot_residuals_strip(out_dir, ticker, dates, residual)\n","\n","    # Build run_config with enriched clock/target plus scaler summary (including ok/violations)\n","    val_raw_slice = full_df[(full_df[\"date\"]>=VAL_START) & (full_df[\"date\"]<=VAL_END)][[\"date\"] + [c for c in features_used if c in full_df.columns]].reset_index(drop=True)\n","    val_scaled_for_check = pd.concat([val_raw_slice[[\"date\"]], valS[[c for c in features_used if c in valS.columns]].reset_index(drop=True)], axis=1)\n","    scaler_summary = {c: {\"mu\": float(scaler.mu[c]), \"sigma\": float(scaler.sigma[c]),\n","                          \"is_sentiment\": c in scaler.sentiment} for c in scaler.mu.keys()}\n","    zrep = zero_preservation_report(val_raw_slice, val_scaled_for_check, sentiment_cols)\n","    scaler_summary[\"zero_preservation_report\"] = zrep.to_dict(orient=\"records\")\n","\n","    param_count = hybrid_param_count(len(features_used))\n","    run_cfg = build_run_config_spec(\n","        ticker=ticker, features=features_used, scaler_summary=scaler_summary,\n","        refit_dates=refit_dates, param_count=param_count, per_refit_seeds=per_refit_seeds,\n","        base_best_epoch=int(metrics[\"best_epoch\"]), refit_epochs_achieved=[int(x) for x in refit_epochs_achieved]\n","    )\n","    (out_dir / f\"run_config_{MODEL_ID}_{ticker}.json\").write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")\n","\n","    cad_lines = [f\"{i+1:02d}: {d}  (first trading day of month segment)\" for i, d in enumerate(refit_dates)]\n","    (out_dir / f\"cadence_{ticker}.txt\").write_text(\"\\n\".join(cad_lines) + (\"\\n\" if cad_lines else \"\"), encoding=\"utf-8\")\n","\n","    # Zero-preservation compliance CSV with ok/violations\n","    write_zero_encoding_check(out_dir, val_raw_slice, val_scaled_for_check, sentiment_cols)\n","    write_scaler_summary_csv(out_dir, scaler, val_raw_slice, val_scaled_for_check, sentiment_cols)\n","\n","    print(f\"[{ticker}] RMSE={metrics['RMSE']:.4f}  U2={metrics['U2']:.4f}  \"\n","          f\"DA={metrics['DA_epsilon']:.4f} cov={metrics['Coverage']:.4f} \"\n","          f\"sh0={metrics['Sharpe_0bps']:.4f} dd0={metrics['MaxDD_0bps']:.4f} \"\n","          f\"sh10={metrics['Sharpe_10bps']:.4f} dd10={metrics['MaxDD_10bps']:.4f}\")\n","\n","    return {\n","        \"ticker\": ticker,\n","        \"RMSE\": metrics[\"RMSE\"], \"MAE\": metrics[\"MAE\"], \"U2\": metrics[\"U2\"],\n","        \"DA_epsilon\": metrics[\"DA_epsilon\"], \"Coverage\": metrics[\"Coverage\"], \"n\": metrics[\"n\"],\n","        \"Sharpe_0bps\": metrics[\"Sharpe_0bps\"], \"Sharpe_10bps\": metrics[\"Sharpe_10bps\"],\n","        \"MaxDD_0bps\": metrics[\"MaxDD_0bps\"], \"MaxDD_10bps\": metrics[\"MaxDD_10bps\"],\n","        \"parameter_count\": param_count\n","    }\n","\n","# ----------- Root provenance, licences, packaging, hashes ----------\n","MIT_TEXT = \"\"\"MIT License\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to do so, subject to the\n","following conditions:\n","The above copyright notice and this permission notice shall be included\n","in all copies or substantial portions of the Software.\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n","THE SOFTWARE.\n","\"\"\"\n","\n","APACHE2_TEXT = \"\"\"Apache License\n","Version 2.0, January 2004\n","https://www.apache.org/licenses/LICENSE-2.0.txt\n","\"\"\"\n","\n","def write_third_party_and_provenance():\n","    TPL_DIR.mkdir(parents=True, exist_ok=True)\n","    readme = [\n","        \"Third-party licences\",\n","        \"\",\n","        \"This folder mirrors licences for third-party repositories adapted or referenced by the HYBRID runner.\",\n","        f\"- Transformer template: {TEMPLATE_COMMITS['transformer_template']['repo_url']} ({TEMPLATE_COMMITS['transformer_template']['license']}), commit {TEMPLATE_COMMITS['transformer_template']['commit_sha']}\",\n","        f\"- LSTM template: {TEMPLATE_COMMITS['lstm_template']['repo_url']} ({TEMPLATE_COMMITS['lstm_template']['license']}), commit {TEMPLATE_COMMITS['lstm_template']['commit_sha']}\",\n","        \"\",\n","        \"Attribution: HYBRID adapts ideas from the above; date-split loader, zero-preserving scaler, residual anchor, and monthly refit evaluator are authored here.\"\n","    ]\n","    (TPL_DIR / \"README.md\").write_text(\"\\n\".join(readme), encoding=\"utf-8\")\n","    (TPL_DIR / \"LICENSE_MIT.txt\").write_text(MIT_TEXT, encoding=\"utf-8\")\n","    (TPL_DIR / \"LICENSE_Apache-2.0.txt\").write_text(APACHE2_TEXT, encoding=\"utf-8\")\n","\n","    prov_rows = []\n","    for name, obj in TEMPLATE_COMMITS.items():\n","        prov_rows.append({\n","            \"component\": name,\n","            \"repo_url\": obj[\"repo_url\"],\n","            \"license\": obj[\"license\"],\n","            \"commit_sha\": obj[\"commit_sha\"],\n","            \"where_used\": \"HYBRID model reference; encoder design\"\n","        })\n","    pd.DataFrame(prov_rows).to_csv(MODEL_DIR / \"code_provenance.csv\", index=False)\n","\n","def write_model_readme():\n","    readme = [\n","        f\"{MODEL_ID} outputs\",\n","        \"\",\n","        \"Splits\",\n","        f\"- Train: {TRAIN_START} -> {TRAIN_END}\",\n","        f\"- Val:   {VAL_START} -> {VAL_END}\",\n","        f\"- Test:  {TEST_START} -> {TEST_END} (n=146)\",\n","        \"\",\n","        f\"Timezone: {TIMEZONE}, price timestamp: {MARKET_CUTOFF} close.\",\n","        \"\",\n","        \"Metrics policy\",\n","        \"- Theil's U2 vs naive last close (last Validation close as carry-in).\",\n","        \"- Directional Accuracy on returns with eps=0.0010; 'coverage' = fraction of Test days with |actual return| > eps.\",\n","        \"- Trading rule: epsilon-threshold sign (positions are flat when |predicted next-day return| <= eps); turnover can be low in calm periods.\"\n","    ]\n","    (MODULE_ROOT / \"README.md\").write_text(\"\\n\".join(readme), encoding=\"utf-8\")\n","\n","def write_env_manifest():\n","    lines = [f\"Python {sys.version.split()[0]}\",\n","             f\"Pandas {pd.__version__}\",\n","             f\"NumPy {np.__version__}\",\n","             f\"Torch {torch.__version__}\",\n","             f\"Device {DEVICE}\",\n","             f\"NUM_WORKERS {NUM_WORKERS}\",\n","             f\"PIN_MEMORY {PIN_MEMORY}\",\n","             f\"PERSISTENT_WORKERS {PERSISTENT_WORKERS}\",\n","             f\"PYTHONHASHSEED {os.environ.get('PYTHONHASHSEED','')}\",\n","             f\"CUBLAS_WORKSPACE_CONFIG {os.environ.get('CUBLAS_WORKSPACE_CONFIG','')}\"]\n","    if torch.cuda.is_available():\n","        try:\n","            cap = torch.cuda.get_device_capability(); lines.append(f\"CUDA capability {cap}\")\n","        except Exception:\n","            pass\n","    (MODULE_ROOT / \"env_manifest.txt\").write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n","\n","def write_cross_ticker_table(rows):\n","    cols = [\"ticker\",\"RMSE\",\"MAE\",\"U2\",\"DA_epsilon\",\"Coverage\",\"n\",\n","            \"Sharpe_0bps\",\"Sharpe_10bps\",\"MaxDD_0bps\",\"MaxDD_10bps\",\"parameter_count\"]\n","    pd.DataFrame(rows)[cols].to_csv(MODULE_ROOT / \"cross_ticker_table_HYBRID.csv\", index=False)\n","\n","def write_features_parity_stub(any_features: List[str]):\n","    stub = {\n","        \"HYBRID\": features_sha256(any_features),\n","        \"note\": \"Compare this sha256 against LSTM_SE and TRANSFORMER packs for cross-model features parity.\"\n","    }\n","    (MODULE_ROOT / \"cross_model_features_parity_stub.json\").write_text(json.dumps(stub, indent=2), encoding=\"utf-8\")\n","\n","def write_file_hashes():\n","    def sha256_of(path: Path):\n","        h = hashlib.sha256()\n","        with open(path, \"rb\") as f:\n","            for chunk in iter(lambda: f.read(1<<20), b\"\"):\n","                h.update(chunk)\n","        return h.hexdigest()\n","    recs = []\n","    for p in MODULE_ROOT.rglob(\"*\"):\n","        if p.is_file() and not (p.parent == MODULE_ROOT / \"hybrid\" and p.name == \"hybrid.zip\"):\n","            recs.append({\"path\": str(p.relative_to(MODULE_ROOT)),\n","                         \"size\": p.stat().st_size,\n","                         \"sha256\": sha256_of(p),\n","                         \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(p.stat().st_mtime))})\n","    (MODULE_ROOT / \"file_hashes.json\").write_text(json.dumps(recs, indent=2), encoding=\"utf-8\")\n","\n","def package_zip():\n","    assert (MODULE_ROOT / \"env_manifest.txt\").exists(), \"Missing env_manifest.txt\"\n","    assert (MODULE_ROOT / \"file_hashes.json\").exists(), \"Missing file_hashes.json\"\n","    assert (MODULE_ROOT / \"cross_ticker_table_HYBRID.csv\").exists(), \"Missing cross_ticker_table_HYBRID.csv\"\n","    assert (TPL_DIR / \"README.md\").exists(), \"Missing third_party_licenses/README.md\"\n","    assert (TPL_DIR / \"LICENSE_MIT.txt\").exists(), \"Missing LICENSE_MIT.txt\"\n","    assert (TPL_DIR / \"LICENSE_Apache-2.0.txt\").exists(), \"Missing LICENSE_Apache-2.0.txt\"\n","\n","    outdir = MODULE_ROOT / \"hybrid\"\n","    outdir.mkdir(parents=True, exist_ok=True)\n","    zpath = outdir / \"hybrid.zip\"\n","    if zpath.exists(): zpath.unlink()\n","\n","    with zipfile.ZipFile(zpath, \"w\", zipfile.ZIP_DEFLATED) as z:\n","        z.write(MODULE_ROOT / \"env_manifest.txt\", arcname=\"env_manifest.txt\")\n","        z.write(MODULE_ROOT / \"file_hashes.json\", arcname=\"file_hashes.json\")\n","        z.write(MODULE_ROOT / \"cross_ticker_table_HYBRID.csv\", arcname=\"cross_ticker_table_HYBRID.csv\")\n","        for p in TPL_DIR.rglob(\"*\"):\n","            if p.is_file():\n","                z.write(p, arcname=str(p.relative_to(MODULE_ROOT)))\n","        for p in MODEL_DIR.rglob(\"*\"):\n","            if p.is_file():\n","                z.write(p, arcname=str(p.relative_to(MODULE_ROOT)))\n","        stub = MODULE_ROOT / \"cross_model_features_parity_stub.json\"\n","        if stub.exists(): z.write(stub, arcname=\"cross_model_features_parity_stub.json\")\n","        readme = MODULE_ROOT / \"README.md\"\n","        if readme.exists(): z.write(readme, arcname=\"README.md\")\n","    print(f\"[ZIP] wrote {zpath.resolve()}\")\n","    return zpath\n","\n","# --------------- Patch + Verify (idempotent) -----------------------\n","def patch_verify_refresh():\n","    rows = []\n","    for t in TICKERS:\n","        base = MODEL_DIR / t\n","        pred_path = base / f\"predictions_{MODEL_ID}_{t}.csv\"\n","        met_path  = base / f\"metrics_{MODEL_ID}_{t}.json\"\n","        rc_path   = base / f\"run_config_{MODEL_ID}_{t}.json\"\n","        assert pred_path.exists() and met_path.exists() and rc_path.exists(), f\"Missing artefacts for {t}\"\n","        dfp = pd.read_csv(pred_path, parse_dates=[\"date\"])\n","        full_df = load_input_csv(t)\n","        _, va_slice, _ = split_data(full_df)\n","        prev0 = float(va_slice[\"Close\"].iloc[-1])\n","\n","        y = dfp[\"y_true\"].to_numpy(float); p = dfp[\"y_hat\"].to_numpy(float)\n","        naive = np.concatenate([[prev0], y[:-1]])\n","        da, cov, _n_da = directional_accuracy_eps_from_levels(p, y, eps=DA_EPS, prev0=prev0)\n","        u2 = theils_u2(p, y, naive)\n","        m0 = backtest_long_short_series(p, y, eps=DA_EPS, cost_bps=0.0, prev0=prev0)\n","        m10 = backtest_long_short_series(p, y, eps=DA_EPS, cost_bps=10.0, prev0=prev0)\n","\n","        prev_metrics = {}\n","        try:\n","            prev_metrics = json.loads(met_path.read_text())\n","        except Exception:\n","            prev_metrics = {}\n","\n","        new_metrics = {\n","            \"RMSE\": rmse(p, y),\n","            \"MAE\": mae(p, y),\n","            \"U2\": u2,\n","            \"DA_epsilon\": da,\n","            \"Coverage\": cov,\n","            \"n\": int(len(y)),\n","            \"Sharpe_0bps\": float(m0[\"sharpe\"]), \"MaxDD_0bps\": float(m0[\"maxdd\"]),\n","            \"Sharpe_10bps\": float(m10[\"sharpe\"]), \"MaxDD_10bps\": float(m10[\"maxdd\"])\n","        }\n","        # Carry best_epoch forward for continuity (optional per checklist).\n","        if \"best_epoch\" in prev_metrics:\n","            new_metrics[\"best_epoch\"] = int(prev_metrics[\"best_epoch\"])\n","\n","        met_path.write_text(json.dumps(new_metrics, indent=2), encoding=\"utf-8\")\n","\n","        rc = json.loads(rc_path.read_text())\n","        rc.setdefault(\"window_L\", LOOKBACK)\n","        rc.setdefault(\"cadence_str\", \"monthly_refit\")\n","        rc.setdefault(\"split_test\", {\"start\": TEST_START, \"end\": TEST_END, \"n\": TEST_LEN})\n","        rc.setdefault(\"dataloader\", {\"num_workers\": NUM_WORKERS, \"pin_memory\": PIN_MEMORY, \"persistent_workers\": PERSISTENT_WORKERS})\n","        rc.setdefault(\"clock\", f\"{TIMEZONE}@{MARKET_CUTOFF}\")\n","        rc.setdefault(\"target_definition\", \"Close.shift(-1) after features\")\n","        rc.setdefault(\"y_scaled\", False)\n","        rc_path.write_text(json.dumps(rc, indent=2), encoding=\"utf-8\")\n","\n","        rows.append({\"ticker\": t, \"parameter_count\": rc.get(\"parameter_count\"), **new_metrics})\n","\n","        if m0[\"turnover\"] > 0 and abs(new_metrics.get(\"Sharpe_0bps\", 0.0)) < 1e-12 and abs(new_metrics.get(\"MaxDD_0bps\", 0.0)) < 1e-12:\n","            raise AssertionError(f\"[verify] {t} 0bps zero metrics despite turnover>0.\")\n","        if m10[\"turnover\"] > 0 and abs(new_metrics.get(\"Sharpe_10bps\", 0.0)) < 1e-12 and abs(new_metrics.get(\"MaxDD_10bps\", 0.0)) < 1e-12:\n","            raise AssertionError(f\"[verify] {t} 10bps zero metrics despite turnover>0.\")\n","\n","    write_cross_ticker_table(rows)\n","    write_file_hashes()\n","    print(\"[patch] metrics schema enforced, trading recomputed, cross-ticker + hashes refreshed.\")\n","\n","# -------------------------- Download helper ------------------------\n","def try_download(path: Path):\n","    try:\n","        from google.colab import files\n","        files.download(str(path))\n","    except Exception as e:\n","        print(f\"[download] Skipped ({e})\")\n","\n","# ------------------------------- Driver ----------------------------\n","def run_ticker(ticker: str):\n","    print(f\"\\n=== {ticker} ===\")\n","    df_raw = load_input_csv(ticker)\n","    df = sanitise_frame(df_raw)\n","    feat_cols = build_features(df)\n","    tr, va, te = split_data(df)\n","    soft_target_check(tr, f\"{ticker} Train\")\n","    soft_target_check(va, f\"{ticker} Val\")\n","    soft_target_check(te, f\"{ticker} Test\")\n","\n","    model_cfg = dict(lstm_hidden=128, lstm_layers=1, d_model=128, nhead=4, enc_layers=2, dropout=0.2)\n","    dfpred, tr_hist, va_hist, best_epoch, refit_dates, per_refit_seeds, final_scaler, vaS_scaled, sentiment_cols, refit_epochs_achieved = expanding_origin_predict_monthly(\n","        model_cfg=model_cfg, df_train=tr, df_val=va, df_test=te, features=feat_cols\n","    )\n","\n","    row = write_ticker_artefacts(\n","        ticker=ticker,\n","        dfpred=dfpred,\n","        tr_hist=tr_hist,\n","        va_hist=va_hist,\n","        best_epoch=best_epoch,\n","        refit_dates=refit_dates,\n","        per_refit_seeds=per_refit_seeds,\n","        refit_epochs_achieved=refit_epochs_achieved,\n","        scaler=final_scaler,\n","        features_used=feat_cols,\n","        full_df=df,\n","        valS=pd.concat([va[[\"date\"]].reset_index(drop=True),\n","                        final_scaler.transform(va[feat_cols]).reset_index(drop=True)], axis=1),\n","        sentiment_cols=sentiment_cols\n","    )\n","    return row, feat_cols\n","\n","def main():\n","    if (MODULE_ROOT / \"hybrid\").exists():\n","        shutil.rmtree(MODULE_ROOT / \"hybrid\", ignore_errors=True)\n","\n","    rows = []\n","    last_feats = None\n","    for t in TICKERS:\n","        row, feats = run_ticker(t)\n","        rows.append(row)\n","        last_feats = feats\n","\n","    write_third_party_and_provenance()\n","    write_model_readme()\n","    write_cross_ticker_table(rows)\n","    write_env_manifest()\n","\n","    if last_feats:\n","        write_features_parity_stub(last_feats)\n","\n","    patch_verify_refresh()\n","    write_file_hashes()\n","    zpath = package_zip()\n","    try_download(zpath)\n","    print(\"\\nAll done. Root:\", MODULE_ROOT)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"id":"hmHKfSIkecXk","executionInfo":{"status":"ok","timestamp":1759067611357,"user_tz":-60,"elapsed":2044197,"user":{"displayName":"Zahra Studies","userId":"15853855476748491235"}},"outputId":"b59e71bc-3798-4fd7-e755-0d8a2d1919a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== AAPL ===\n","[AAPL] RMSE=2.1435  U2=1.0066  DA=0.5328 cov=0.9384 sh0=-1.1925 dd0=0.1043 sh10=-1.2656 dd10=0.1061\n","\n","=== AMZN ===\n","[AMZN] RMSE=2.5214  U2=1.0051  DA=0.4627 cov=0.9178 sh0=-0.7389 dd0=0.1661 sh10=-0.8021 dd10=0.1728\n","\n","=== MSFT ===\n","[MSFT] RMSE=4.6477  U2=0.9988  DA=0.5775 cov=0.9726 sh0=0.0000 dd0=0.0000 sh10=0.0000 dd10=0.0000\n","\n","=== TSLA ===\n","[TSLA] RMSE=7.5103  U2=1.0026  DA=0.4789 cov=0.9726 sh0=-1.3263 dd0=0.3002 sh10=-1.3699 dd10=0.3030\n","\n","=== AMD ===\n","[AMD] RMSE=3.0082  U2=1.0021  DA=0.4786 cov=0.9589 sh0=0.0000 dd0=0.0000 sh10=0.0000 dd10=0.0000\n","[patch] metrics schema enforced, trading recomputed, cross-ticker + hashes refreshed.\n","[ZIP] wrote /content/outputs/hybrid/hybrid.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d2407b92-040c-40c3-8c30-f06b09cdaec8\", \"hybrid.zip\", 664942)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","All done. Root: outputs\n"]}]}]}